{
  "version": 3,
  "sources": ["../src/utils/model-loader.ts", "../src/core/types.ts", "../src/core/tensor.ts", "../src/core/scheduler.ts", "../src/core/memory.ts", "../src/core/runtime.ts", "../src/backends/webgpu.ts", "../src/backends/webnn.ts", "../src/backends/wasm.ts", "../src/backends/onnx.ts", "../src/backends/index.ts", "../src/utils/cache.ts", "../src/pipelines/base.ts", "../src/utils/tokenizer.ts", "../src/pipelines/text-classification.ts", "../src/pipelines/feature-extraction.ts", "../src/utils/preprocessor.ts", "../src/pipelines/image-classification.ts", "../src/pipelines/text-generation.ts", "../src/pipelines/object-detection.ts", "../src/pipelines/automatic-speech-recognition.ts", "../src/pipelines/zero-shot-classification.ts", "../src/pipelines/question-answering.ts", "../src/pipelines/index.ts", "../src/utils/index.ts", "../src/utils/hub.ts", "../src/tools/index.ts", "../src/index.ts"],
  "sourcesContent": ["/**\n * edgeFlow.js - Advanced Model Loader\n * \n * Features:\n * - Preloading: Background model loading\n * - Sharding: Split large files into chunks for download\n * - Resume Download: Continue download from where it left off\n * - Model Caching: IndexedDB storage for large models\n */\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * Download progress information\n */\nexport interface DownloadProgress {\n  /** Downloaded bytes */\n  loaded: number;\n  /** Total bytes (0 if unknown) */\n  total: number;\n  /** Progress percentage (0-100) */\n  percent: number;\n  /** Download speed in bytes/sec */\n  speed: number;\n  /** Estimated time remaining in ms */\n  eta: number;\n  /** Current chunk index (for sharded downloads) */\n  currentChunk?: number;\n  /** Total chunks (for sharded downloads) */\n  totalChunks?: number;\n}\n\n/**\n * Model loader options\n */\nexport interface ModelLoaderOptions {\n  /** Enable caching (default: true) */\n  cache?: boolean;\n  /** Cache name for IndexedDB (default: 'edgeflow-models') */\n  cacheName?: string;\n  /** Enable resume download (default: true) */\n  resumable?: boolean;\n  /** Chunk size for sharded downloads in bytes (default: 5MB) */\n  chunkSize?: number;\n  /** Progress callback */\n  onProgress?: (progress: DownloadProgress) => void;\n  /** Number of parallel download connections (default: 4) */\n  parallelConnections?: number;\n  /** Request timeout in ms (default: 30000) */\n  timeout?: number;\n  /** Force re-download even if cached */\n  forceDownload?: boolean;\n}\n\n/**\n * Preload options\n */\nexport interface PreloadOptions extends ModelLoaderOptions {\n  /** Priority (higher = more important, default: 0) */\n  priority?: number;\n}\n\n/**\n * Cached model metadata\n */\ninterface CachedModelMeta {\n  url: string;\n  size: number;\n  etag?: string;\n  lastModified?: string;\n  cachedAt: number;\n  chunks?: number;\n  complete: boolean;\n}\n\n/**\n * Download state for resume support\n */\ninterface DownloadState {\n  url: string;\n  totalSize: number;\n  downloadedSize: number;\n  chunks: ChunkState[];\n  startedAt: number;\n}\n\n/**\n * Chunk state\n */\ninterface ChunkState {\n  index: number;\n  start: number;\n  end: number;\n  downloaded: boolean;\n}\n\n// ============================================================================\n// IndexedDB Model Cache\n// ============================================================================\n\nconst DB_NAME = 'edgeflow-model-cache';\nconst DB_VERSION = 1;\nconst STORE_META = 'meta';\nconst STORE_CHUNKS = 'chunks';\nconst STORE_STATE = 'download-state';\n\n/**\n * IndexedDB-based model cache for large files\n */\nclass ModelCache {\n  private db: IDBDatabase | null = null;\n  private dbPromise: Promise<IDBDatabase> | null = null;\n\n  /**\n   * Open the database\n   */\n  private async openDB(): Promise<IDBDatabase> {\n    if (this.db) return this.db;\n    if (this.dbPromise) return this.dbPromise;\n\n    this.dbPromise = new Promise((resolve, reject) => {\n      const request = indexedDB.open(DB_NAME, DB_VERSION);\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        // Model metadata store\n        if (!db.objectStoreNames.contains(STORE_META)) {\n          db.createObjectStore(STORE_META, { keyPath: 'url' });\n        }\n        \n        // Chunk data store\n        if (!db.objectStoreNames.contains(STORE_CHUNKS)) {\n          const chunkStore = db.createObjectStore(STORE_CHUNKS, { keyPath: ['url', 'index'] });\n          chunkStore.createIndex('url', 'url', { unique: false });\n        }\n        \n        // Download state store (for resume)\n        if (!db.objectStoreNames.contains(STORE_STATE)) {\n          db.createObjectStore(STORE_STATE, { keyPath: 'url' });\n        }\n      };\n\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve(this.db);\n      };\n\n      request.onerror = () => reject(request.error);\n    });\n\n    return this.dbPromise;\n  }\n\n  /**\n   * Get cached model metadata\n   */\n  async getMeta(url: string): Promise<CachedModelMeta | null> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_META, 'readonly');\n      const store = tx.objectStore(STORE_META);\n      const request = store.get(url);\n      request.onsuccess = () => resolve(request.result ?? null);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  /**\n   * Save model metadata\n   */\n  async saveMeta(meta: CachedModelMeta): Promise<void> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_META, 'readwrite');\n      const store = tx.objectStore(STORE_META);\n      store.put(meta);\n      tx.oncomplete = () => resolve();\n      tx.onerror = () => reject(tx.error);\n    });\n  }\n\n  /**\n   * Save a chunk\n   */\n  async saveChunk(url: string, index: number, data: ArrayBuffer): Promise<void> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_CHUNKS, 'readwrite');\n      const store = tx.objectStore(STORE_CHUNKS);\n      store.put({ url, index, data });\n      tx.oncomplete = () => resolve();\n      tx.onerror = () => reject(tx.error);\n    });\n  }\n\n  /**\n   * Get all chunks for a URL\n   */\n  async getChunks(url: string): Promise<ArrayBuffer[]> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_CHUNKS, 'readonly');\n      const store = tx.objectStore(STORE_CHUNKS);\n      const index = store.index('url');\n      const request = index.getAll(url);\n      \n      request.onsuccess = () => {\n        const results = request.result as Array<{ url: string; index: number; data: ArrayBuffer }>;\n        // Sort by index and extract data\n        results.sort((a, b) => a.index - b.index);\n        resolve(results.map(r => r.data));\n      };\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  /**\n   * Get complete model data (merged chunks)\n   */\n  async getModel(url: string): Promise<ArrayBuffer | null> {\n    const meta = await this.getMeta(url);\n    if (!meta || !meta.complete) return null;\n\n    const chunks = await this.getChunks(url);\n    if (chunks.length === 0) return null;\n\n    // Merge chunks\n    const totalSize = chunks.reduce((sum, chunk) => sum + chunk.byteLength, 0);\n    const result = new Uint8Array(totalSize);\n    let offset = 0;\n    \n    for (const chunk of chunks) {\n      result.set(new Uint8Array(chunk), offset);\n      offset += chunk.byteLength;\n    }\n\n    return result.buffer;\n  }\n\n  /**\n   * Save download state (for resume)\n   */\n  async saveDownloadState(state: DownloadState): Promise<void> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_STATE, 'readwrite');\n      const store = tx.objectStore(STORE_STATE);\n      store.put(state);\n      tx.oncomplete = () => resolve();\n      tx.onerror = () => reject(tx.error);\n    });\n  }\n\n  /**\n   * Get download state\n   */\n  async getDownloadState(url: string): Promise<DownloadState | null> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_STATE, 'readonly');\n      const store = tx.objectStore(STORE_STATE);\n      const request = store.get(url);\n      request.onsuccess = () => resolve(request.result ?? null);\n      request.onerror = () => reject(request.error);\n    });\n  }\n\n  /**\n   * Delete download state\n   */\n  async deleteDownloadState(url: string): Promise<void> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_STATE, 'readwrite');\n      const store = tx.objectStore(STORE_STATE);\n      store.delete(url);\n      tx.oncomplete = () => resolve();\n      tx.onerror = () => reject(tx.error);\n    });\n  }\n\n  /**\n   * Delete cached model\n   */\n  async deleteModel(url: string): Promise<void> {\n    const db = await this.openDB();\n    \n    // Delete metadata\n    await new Promise<void>((resolve, reject) => {\n      const tx = db.transaction(STORE_META, 'readwrite');\n      const store = tx.objectStore(STORE_META);\n      store.delete(url);\n      tx.oncomplete = () => resolve();\n      tx.onerror = () => reject(tx.error);\n    });\n\n    // Delete chunks\n    const chunks = await this.getChunks(url);\n    if (chunks.length > 0) {\n      await new Promise<void>((resolve, reject) => {\n        const tx = db.transaction(STORE_CHUNKS, 'readwrite');\n        const store = tx.objectStore(STORE_CHUNKS);\n        const index = store.index('url');\n        const request = index.openCursor(IDBKeyRange.only(url));\n        \n        request.onsuccess = (event) => {\n          const cursor = (event.target as IDBRequest<IDBCursorWithValue>).result;\n          if (cursor) {\n            cursor.delete();\n            cursor.continue();\n          }\n        };\n        \n        tx.oncomplete = () => resolve();\n        tx.onerror = () => reject(tx.error);\n      });\n    }\n\n    // Delete download state\n    await this.deleteDownloadState(url);\n  }\n\n  /**\n   * Clear all cached models\n   */\n  async clear(): Promise<void> {\n    const db = await this.openDB();\n    \n    const stores = [STORE_META, STORE_CHUNKS, STORE_STATE];\n    for (const storeName of stores) {\n      await new Promise<void>((resolve, reject) => {\n        const tx = db.transaction(storeName, 'readwrite');\n        const store = tx.objectStore(storeName);\n        store.clear();\n        tx.oncomplete = () => resolve();\n        tx.onerror = () => reject(tx.error);\n      });\n    }\n  }\n\n  /**\n   * Get cache statistics\n   */\n  async getStats(): Promise<{ models: number; totalSize: number }> {\n    const db = await this.openDB();\n    return new Promise((resolve, reject) => {\n      const tx = db.transaction(STORE_META, 'readonly');\n      const store = tx.objectStore(STORE_META);\n      const request = store.getAll();\n      \n      request.onsuccess = () => {\n        const metas = request.result as CachedModelMeta[];\n        resolve({\n          models: metas.filter(m => m.complete).length,\n          totalSize: metas.reduce((sum, m) => sum + (m.complete ? m.size : 0), 0),\n        });\n      };\n      request.onerror = () => reject(request.error);\n    });\n  }\n}\n\n// Global cache instance\nconst modelCache = new ModelCache();\n\n// ============================================================================\n// Advanced Model Loader\n// ============================================================================\n\n/**\n * Check if server supports Range requests\n */\nasync function supportsRangeRequests(url: string): Promise<{ supports: boolean; size: number; etag?: string }> {\n  try {\n    const response = await fetch(url, { method: 'HEAD' });\n    const acceptRanges = response.headers.get('Accept-Ranges');\n    const contentLength = response.headers.get('Content-Length');\n    const etag = response.headers.get('ETag') ?? undefined;\n    \n    return {\n      supports: acceptRanges === 'bytes',\n      size: contentLength ? parseInt(contentLength, 10) : 0,\n      etag,\n    };\n  } catch {\n    return { supports: false, size: 0 };\n  }\n}\n\n/**\n * Download a single chunk using Range request\n */\nasync function downloadChunk(\n  url: string,\n  start: number,\n  end: number,\n  timeout: number\n): Promise<ArrayBuffer> {\n  const controller = new AbortController();\n  const timeoutId = setTimeout(() => controller.abort(), timeout);\n\n  try {\n    const response = await fetch(url, {\n      headers: { Range: `bytes=${start}-${end}` },\n      signal: controller.signal,\n    });\n\n    if (response.status !== 206 && response.status !== 200) {\n      throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n    }\n\n    return await response.arrayBuffer();\n  } finally {\n    clearTimeout(timeoutId);\n  }\n}\n\n/**\n * Download model with sharding and resume support\n */\nasync function downloadWithResume(\n  url: string,\n  options: ModelLoaderOptions\n): Promise<ArrayBuffer> {\n  const {\n    chunkSize = 5 * 1024 * 1024, // 5MB\n    parallelConnections = 4,\n    timeout = 30000,\n    onProgress,\n  } = options;\n\n  // Check server capabilities\n  const { supports: supportsRange, size: totalSize, etag } = await supportsRangeRequests(url);\n\n  // If no Range support or small file, download normally\n  if (!supportsRange || totalSize < chunkSize * 2) {\n    return downloadSimple(url, timeout, onProgress);\n  }\n\n  // Check for existing download state\n  let state = await modelCache.getDownloadState(url);\n  \n  // Initialize or reset state if needed\n  if (!state || (etag && state.totalSize !== totalSize)) {\n    const numChunks = Math.ceil(totalSize / chunkSize);\n    const chunks: ChunkState[] = [];\n    \n    for (let i = 0; i < numChunks; i++) {\n      const start = i * chunkSize;\n      const end = Math.min(start + chunkSize - 1, totalSize - 1);\n      chunks.push({ index: i, start, end, downloaded: false });\n    }\n    \n    state = {\n      url,\n      totalSize,\n      downloadedSize: 0,\n      chunks,\n      startedAt: Date.now(),\n    };\n    \n    // Clear any existing chunks\n    await modelCache.deleteModel(url);\n  }\n\n  // Download remaining chunks\n  const pendingChunks = state.chunks.filter(c => !c.downloaded);\n  let downloadedSize = state.downloadedSize;\n  const startTime = Date.now();\n  let lastProgressTime = startTime;\n  let lastDownloadedSize = downloadedSize;\n\n  // Progress tracking\n  const reportProgress = () => {\n    if (!onProgress) return;\n    \n    const now = Date.now();\n    const elapsed = (now - lastProgressTime) / 1000;\n    const bytesDownloaded = downloadedSize - lastDownloadedSize;\n    const speed = elapsed > 0 ? bytesDownloaded / elapsed : 0;\n    const remaining = totalSize - downloadedSize;\n    const eta = speed > 0 ? (remaining / speed) * 1000 : 0;\n\n    onProgress({\n      loaded: downloadedSize,\n      total: totalSize,\n      percent: (downloadedSize / totalSize) * 100,\n      speed,\n      eta,\n      currentChunk: state!.chunks.filter(c => c.downloaded).length,\n      totalChunks: state!.chunks.length,\n    });\n\n    lastProgressTime = now;\n    lastDownloadedSize = downloadedSize;\n  };\n\n  // Download chunks in parallel\n  const downloadQueue = [...pendingChunks];\n  const inProgress = new Map<number, Promise<void>>();\n\n  while (downloadQueue.length > 0 || inProgress.size > 0) {\n    // Start new downloads up to parallelConnections limit\n    while (downloadQueue.length > 0 && inProgress.size < parallelConnections) {\n      const chunk = downloadQueue.shift()!;\n      \n      const downloadPromise = (async () => {\n        try {\n          const data = await downloadChunk(url, chunk.start, chunk.end, timeout);\n          await modelCache.saveChunk(url, chunk.index, data);\n          \n          chunk.downloaded = true;\n          downloadedSize += data.byteLength;\n          \n          // Update state periodically\n          state!.downloadedSize = downloadedSize;\n          await modelCache.saveDownloadState(state!);\n          \n          reportProgress();\n        } finally {\n          inProgress.delete(chunk.index);\n        }\n      })();\n      \n      inProgress.set(chunk.index, downloadPromise);\n    }\n\n    // Wait for at least one to complete\n    if (inProgress.size > 0) {\n      await Promise.race(inProgress.values());\n    }\n  }\n\n  // All chunks downloaded, merge them\n  const chunks = await modelCache.getChunks(url);\n  const result = new Uint8Array(totalSize);\n  let offset = 0;\n  \n  for (const chunk of chunks) {\n    result.set(new Uint8Array(chunk), offset);\n    offset += chunk.byteLength;\n  }\n\n  // Save metadata and cleanup state\n  await modelCache.saveMeta({\n    url,\n    size: totalSize,\n    etag,\n    cachedAt: Date.now(),\n    chunks: chunks.length,\n    complete: true,\n  });\n  await modelCache.deleteDownloadState(url);\n\n  return result.buffer;\n}\n\n/**\n * Simple download without sharding\n */\nasync function downloadSimple(\n  url: string,\n  timeout: number,\n  onProgress?: (progress: DownloadProgress) => void\n): Promise<ArrayBuffer> {\n  const controller = new AbortController();\n  const timeoutId = setTimeout(() => controller.abort(), timeout);\n\n  try {\n    const response = await fetch(url, { signal: controller.signal });\n    \n    if (!response.ok) {\n      throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n    }\n\n    const contentLength = response.headers.get('Content-Length');\n    const total = contentLength ? parseInt(contentLength, 10) : 0;\n\n    if (!response.body || !onProgress || total === 0) {\n      return await response.arrayBuffer();\n    }\n\n    // Stream with progress\n    const reader = response.body.getReader();\n    const chunks: Uint8Array[] = [];\n    let loaded = 0;\n    const startTime = Date.now();\n\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n\n      chunks.push(value);\n      loaded += value.length;\n\n      const elapsed = (Date.now() - startTime) / 1000;\n      const speed = elapsed > 0 ? loaded / elapsed : 0;\n      const remaining = total - loaded;\n      const eta = speed > 0 ? (remaining / speed) * 1000 : 0;\n\n      onProgress({\n        loaded,\n        total,\n        percent: (loaded / total) * 100,\n        speed,\n        eta,\n      });\n    }\n\n    // Merge chunks\n    const result = new Uint8Array(loaded);\n    let offset = 0;\n    for (const chunk of chunks) {\n      result.set(chunk, offset);\n      offset += chunk.length;\n    }\n\n    return result.buffer;\n  } finally {\n    clearTimeout(timeoutId);\n  }\n}\n\n// ============================================================================\n// Preload Manager\n// ============================================================================\n\ninterface PreloadTask {\n  url: string;\n  priority: number;\n  options: ModelLoaderOptions;\n  promise: Promise<ArrayBuffer>;\n  resolve: (data: ArrayBuffer) => void;\n  reject: (error: Error) => void;\n  status: 'pending' | 'loading' | 'complete' | 'error';\n}\n\n/**\n * Preload manager for background model loading\n */\nclass PreloadManager {\n  private tasks: Map<string, PreloadTask> = new Map();\n  private queue: string[] = [];\n  private maxConcurrent = 2;\n  private activeCount = 0;\n\n  /**\n   * Preload a model in the background\n   */\n  preload(url: string, options: PreloadOptions = {}): Promise<ArrayBuffer> {\n    // Check if already preloading\n    const existing = this.tasks.get(url);\n    if (existing) {\n      return existing.promise;\n    }\n\n    // Create task\n    let resolve!: (data: ArrayBuffer) => void;\n    let reject!: (error: Error) => void;\n    \n    const promise = new Promise<ArrayBuffer>((res, rej) => {\n      resolve = res;\n      reject = rej;\n    });\n\n    const task: PreloadTask = {\n      url,\n      priority: options.priority ?? 0,\n      options,\n      promise,\n      resolve,\n      reject,\n      status: 'pending',\n    };\n\n    this.tasks.set(url, task);\n    \n    // Insert into queue based on priority\n    const insertIndex = this.queue.findIndex(u => {\n      const t = this.tasks.get(u);\n      return t && t.priority < task.priority;\n    });\n    \n    if (insertIndex === -1) {\n      this.queue.push(url);\n    } else {\n      this.queue.splice(insertIndex, 0, url);\n    }\n\n    // Process queue\n    this.processQueue();\n\n    return promise;\n  }\n\n  /**\n   * Process the preload queue\n   */\n  private async processQueue(): Promise<void> {\n    while (this.queue.length > 0 && this.activeCount < this.maxConcurrent) {\n      const url = this.queue.shift();\n      if (!url) break;\n\n      const task = this.tasks.get(url);\n      if (!task || task.status !== 'pending') continue;\n\n      this.activeCount++;\n      task.status = 'loading';\n\n      this.downloadTask(task).finally(() => {\n        this.activeCount--;\n        this.processQueue();\n      });\n    }\n  }\n\n  /**\n   * Download a preload task\n   */\n  private async downloadTask(task: PreloadTask): Promise<void> {\n    try {\n      const data = await loadModelData(task.url, task.options);\n      task.status = 'complete';\n      task.resolve(data);\n    } catch (error) {\n      task.status = 'error';\n      task.reject(error instanceof Error ? error : new Error(String(error)));\n    }\n  }\n\n  /**\n   * Check if a model is preloaded\n   */\n  isPreloaded(url: string): boolean {\n    const task = this.tasks.get(url);\n    return task?.status === 'complete';\n  }\n\n  /**\n   * Get preload status\n   */\n  getStatus(url: string): 'pending' | 'loading' | 'complete' | 'error' | 'not_found' {\n    const task = this.tasks.get(url);\n    return task?.status ?? 'not_found';\n  }\n\n  /**\n   * Get preloaded model data\n   */\n  async get(url: string): Promise<ArrayBuffer | null> {\n    const task = this.tasks.get(url);\n    if (!task) return null;\n    \n    if (task.status === 'complete' || task.status === 'loading') {\n      return task.promise;\n    }\n    \n    return null;\n  }\n\n  /**\n   * Cancel preload\n   */\n  cancel(url: string): void {\n    const task = this.tasks.get(url);\n    if (task && task.status === 'pending') {\n      this.tasks.delete(url);\n      this.queue = this.queue.filter(u => u !== url);\n      task.reject(new Error('Preload cancelled'));\n    }\n  }\n\n  /**\n   * Clear all preloads\n   */\n  clear(): void {\n    for (const [, task] of this.tasks) {\n      if (task.status === 'pending') {\n        task.reject(new Error('Preload cleared'));\n      }\n    }\n    this.tasks.clear();\n    this.queue = [];\n  }\n}\n\n// Global preload manager\nconst preloadManager = new PreloadManager();\n\n// ============================================================================\n// Public API\n// ============================================================================\n\n/**\n * Load model data with caching, sharding, and resume support\n */\nexport async function loadModelData(\n  url: string,\n  options: ModelLoaderOptions = {}\n): Promise<ArrayBuffer> {\n  const {\n    cache = true,\n    forceDownload = false,\n    resumable = true,\n  } = options;\n\n  // Check cache first\n  if (cache && !forceDownload) {\n    const cached = await modelCache.getModel(url);\n    if (cached) {\n      console.log(`\u2713 Model loaded from cache: ${url}`);\n      options.onProgress?.({\n        loaded: cached.byteLength,\n        total: cached.byteLength,\n        percent: 100,\n        speed: 0,\n        eta: 0,\n      });\n      return cached;\n    }\n  }\n\n  // Download with resume support\n  let data: ArrayBuffer;\n  \n  if (resumable) {\n    data = await downloadWithResume(url, options);\n  } else {\n    data = await downloadSimple(url, options.timeout ?? 30000, options.onProgress);\n  }\n\n  // Cache the result\n  if (cache) {\n    // For simple downloads, save as single chunk\n    if (!resumable) {\n      await modelCache.saveChunk(url, 0, data);\n      await modelCache.saveMeta({\n        url,\n        size: data.byteLength,\n        cachedAt: Date.now(),\n        chunks: 1,\n        complete: true,\n      });\n    }\n  }\n\n  return data;\n}\n\n/**\n * Preload a model in the background\n */\nexport function preloadModel(url: string, options: PreloadOptions = {}): Promise<ArrayBuffer> {\n  return preloadManager.preload(url, options);\n}\n\n/**\n * Preload multiple models\n */\nexport function preloadModels(\n  urls: Array<{ url: string; priority?: number }>,\n  options: Omit<PreloadOptions, 'priority'> = {}\n): Promise<ArrayBuffer[]> {\n  return Promise.all(\n    urls.map(({ url, priority }) => preloadManager.preload(url, { ...options, priority }))\n  );\n}\n\n/**\n * Check if a model is cached\n */\nexport async function isModelCached(url: string): Promise<boolean> {\n  const meta = await modelCache.getMeta(url);\n  return meta?.complete ?? false;\n}\n\n/**\n * Get cached model data\n */\nexport async function getCachedModel(url: string): Promise<ArrayBuffer | null> {\n  return modelCache.getModel(url);\n}\n\n/**\n * Delete a cached model\n */\nexport async function deleteCachedModel(url: string): Promise<void> {\n  return modelCache.deleteModel(url);\n}\n\n/**\n * Clear all cached models\n */\nexport async function clearModelCache(): Promise<void> {\n  return modelCache.clear();\n}\n\n/**\n * Get model cache statistics\n */\nexport async function getModelCacheStats(): Promise<{ models: number; totalSize: number }> {\n  return modelCache.getStats();\n}\n\n/**\n * Get preload status\n */\nexport function getPreloadStatus(url: string): 'pending' | 'loading' | 'complete' | 'error' | 'not_found' {\n  return preloadManager.getStatus(url);\n}\n\n/**\n * Cancel a preload\n */\nexport function cancelPreload(url: string): void {\n  preloadManager.cancel(url);\n}\n\n/**\n * Get preloaded model (or wait for preload to complete)\n */\nexport async function getPreloadedModel(url: string): Promise<ArrayBuffer | null> {\n  return preloadManager.get(url);\n}\n", "/**\n * edgeFlow.js - Core Type Definitions\n * \n * This file contains all the core types used throughout the framework.\n */\n\n// ============================================================================\n// Tensor Types\n// ============================================================================\n\n/**\n * Supported data types for tensors\n */\nexport type DataType = \n  | 'float32' \n  | 'float16' \n  | 'int32' \n  | 'int64' \n  | 'uint8' \n  | 'int8' \n  | 'bool';\n\n/**\n * TypedArray types used for tensor data\n */\nexport type TypedArray = \n  | Float32Array \n  | Float64Array \n  | Int32Array \n  | BigInt64Array \n  | Uint8Array \n  | Int8Array;\n\n/**\n * Tensor shape definition\n */\nexport type Shape = readonly number[];\n\n/**\n * Tensor interface\n */\nexport interface Tensor {\n  /** Unique identifier for the tensor */\n  readonly id: string;\n  /** Data type of the tensor */\n  readonly dtype: DataType;\n  /** Shape of the tensor */\n  readonly shape: Shape;\n  /** Total number of elements */\n  readonly size: number;\n  /** Underlying data */\n  readonly data: TypedArray;\n  /** Get data as Float32Array */\n  toFloat32Array(): Float32Array;\n  /** Get data as array */\n  toArray(): number[];\n  /** Clone the tensor */\n  clone(): Tensor;\n  /** Dispose the tensor and free memory */\n  dispose(): void;\n  /** Check if tensor has been disposed */\n  readonly isDisposed: boolean;\n}\n\n// ============================================================================\n// Runtime Types\n// ============================================================================\n\n/**\n * Supported runtime backends\n */\nexport type RuntimeType = 'webgpu' | 'webnn' | 'wasm' | 'auto';\n\n/**\n * Runtime capability flags\n */\nexport interface RuntimeCapabilities {\n  /** Supports concurrent execution */\n  concurrency: boolean;\n  /** Supports quantized models */\n  quantization: boolean;\n  /** Supports float16 */\n  float16: boolean;\n  /** Supports dynamic shapes */\n  dynamicShapes: boolean;\n  /** Maximum batch size */\n  maxBatchSize: number;\n  /** Available memory in bytes */\n  availableMemory: number;\n}\n\n/**\n * Runtime interface that all backends must implement\n */\nexport interface Runtime {\n  /** Runtime name */\n  readonly name: RuntimeType;\n  /** Runtime capabilities */\n  readonly capabilities: RuntimeCapabilities;\n  /** Initialize the runtime */\n  initialize(): Promise<void>;\n  /** Check if runtime is available in current environment */\n  isAvailable(): Promise<boolean>;\n  /** Load a model from ArrayBuffer */\n  loadModel(modelData: ArrayBuffer, options?: ModelLoadOptions): Promise<LoadedModel>;\n  /** Run inference */\n  run(model: LoadedModel, inputs: Tensor[]): Promise<Tensor[]>;\n  /** Dispose the runtime and free resources */\n  dispose(): void;\n}\n\n// ============================================================================\n// Model Types\n// ============================================================================\n\n/**\n * Model format types\n */\nexport type ModelFormat = 'onnx' | 'edgeflow' | 'safetensors';\n\n/**\n * Model quantization types\n */\nexport type QuantizationType = 'float32' | 'float16' | 'int8' | 'uint8' | 'int4';\n\n/**\n * Model metadata\n */\nexport interface ModelMetadata {\n  /** Model name/identifier */\n  name: string;\n  /** Model version */\n  version?: string;\n  /** Model description */\n  description?: string;\n  /** Model author */\n  author?: string;\n  /** Model license */\n  license?: string;\n  /** Model tags */\n  tags?: string[];\n  /** Input specifications */\n  inputs: ModelIOSpec[];\n  /** Output specifications */\n  outputs: ModelIOSpec[];\n  /** Model size in bytes */\n  sizeBytes: number;\n  /** Quantization type */\n  quantization: QuantizationType;\n  /** Model format */\n  format: ModelFormat;\n}\n\n/**\n * Model input/output specification\n */\nexport interface ModelIOSpec {\n  /** Name of the input/output */\n  name: string;\n  /** Data type */\n  dtype: DataType;\n  /** Shape (use -1 for dynamic dimensions) */\n  shape: number[];\n  /** Optional description */\n  description?: string;\n}\n\n/**\n * Options for loading a model\n */\nexport interface ModelLoadOptions {\n  /** Target quantization (convert during load) */\n  quantization?: QuantizationType;\n  /** Custom metadata */\n  metadata?: Partial<ModelMetadata>;\n  /** Enable caching */\n  cache?: boolean;\n  /** Progress callback */\n  onProgress?: (progress: number) => void;\n}\n\n/**\n * Loaded model instance\n */\nexport interface LoadedModel {\n  /** Unique model instance ID */\n  readonly id: string;\n  /** Model metadata */\n  readonly metadata: ModelMetadata;\n  /** Check if model is loaded */\n  readonly isLoaded: boolean;\n  /** Runtime this model is loaded on */\n  readonly runtime: RuntimeType;\n  /** Dispose the model and free resources */\n  dispose(): void;\n}\n\n// ============================================================================\n// Scheduler Types\n// ============================================================================\n\n/**\n * Task priority levels\n */\nexport type TaskPriority = 'low' | 'normal' | 'high' | 'critical';\n\n/**\n * Task status\n */\nexport type TaskStatus = 'pending' | 'running' | 'completed' | 'failed' | 'cancelled';\n\n/**\n * Inference task definition\n */\nexport interface InferenceTask<T = unknown> {\n  /** Unique task ID */\n  readonly id: string;\n  /** Model ID this task is for */\n  readonly modelId: string;\n  /** Task priority */\n  readonly priority: TaskPriority;\n  /** Task status */\n  readonly status: TaskStatus;\n  /** Creation timestamp */\n  readonly createdAt: number;\n  /** Start timestamp (when running) */\n  readonly startedAt?: number;\n  /** Completion timestamp */\n  readonly completedAt?: number;\n  /** Task result (when completed) */\n  readonly result?: T;\n  /** Task error (when failed) */\n  readonly error?: Error;\n  /** Cancel the task */\n  cancel(): void;\n  /** Wait for task completion */\n  wait(): Promise<T>;\n}\n\n/**\n * Scheduler options\n */\nexport interface SchedulerOptions {\n  /** Maximum concurrent tasks across all models */\n  maxConcurrentTasks?: number;\n  /** Maximum concurrent tasks per model */\n  maxConcurrentPerModel?: number;\n  /** Default task timeout in milliseconds */\n  defaultTimeout?: number;\n  /** Enable task batching */\n  enableBatching?: boolean;\n  /** Maximum batch size */\n  maxBatchSize?: number;\n  /** Batch timeout in milliseconds */\n  batchTimeout?: number;\n}\n\n// ============================================================================\n// Memory Types\n// ============================================================================\n\n/**\n * Memory statistics\n */\nexport interface MemoryStats {\n  /** Total allocated memory in bytes */\n  allocated: number;\n  /** Currently used memory in bytes */\n  used: number;\n  /** Peak memory usage in bytes */\n  peak: number;\n  /** Number of active tensors */\n  tensorCount: number;\n  /** Number of loaded models */\n  modelCount: number;\n}\n\n/**\n * Memory pool configuration\n */\nexport interface MemoryPoolConfig {\n  /** Initial pool size in bytes */\n  initialSize?: number;\n  /** Maximum pool size in bytes */\n  maxSize?: number;\n  /** Growth factor when expanding */\n  growthFactor?: number;\n  /** Enable automatic garbage collection */\n  autoGC?: boolean;\n  /** GC threshold (percentage of max size) */\n  gcThreshold?: number;\n}\n\n// ============================================================================\n// Pipeline Types\n// ============================================================================\n\n/**\n * Supported pipeline tasks\n */\nexport type PipelineTask = \n  | 'text-classification'\n  | 'token-classification'\n  | 'question-answering'\n  | 'fill-mask'\n  | 'text-generation'\n  | 'text2text-generation'\n  | 'summarization'\n  | 'translation'\n  | 'feature-extraction'\n  | 'sentiment-analysis'\n  | 'zero-shot-classification'\n  | 'image-classification'\n  | 'object-detection'\n  | 'image-segmentation'\n  | 'depth-estimation'\n  | 'image-to-text'\n  | 'audio-classification'\n  | 'automatic-speech-recognition'\n  | 'text-to-speech';\n\n/**\n * Pipeline configuration\n */\nexport interface PipelineConfig {\n  /** Task type */\n  task: PipelineTask;\n  /** Model ID or path */\n  model: string;\n  /** Runtime to use */\n  runtime?: RuntimeType;\n  /** Enable caching */\n  cache?: boolean;\n  /** Quantization type */\n  quantization?: QuantizationType;\n  /** Device to use */\n  device?: 'cpu' | 'gpu';\n  /** Custom tokenizer config */\n  tokenizer?: TokenizerConfig;\n}\n\n/**\n * Pipeline options passed during inference\n */\nexport interface PipelineOptions {\n  /** Batch size */\n  batchSize?: number;\n  /** Top K results */\n  topK?: number;\n  /** Temperature for generation */\n  temperature?: number;\n  /** Maximum length for generation */\n  maxLength?: number;\n  /** Task timeout in milliseconds */\n  timeout?: number;\n}\n\n// ============================================================================\n// Tokenizer Types\n// ============================================================================\n\n/**\n * Tokenizer configuration\n */\nexport interface TokenizerConfig {\n  /** Vocabulary size */\n  vocabSize: number;\n  /** Maximum sequence length */\n  maxLength: number;\n  /** Padding token ID */\n  padTokenId: number;\n  /** Unknown token ID */\n  unkTokenId: number;\n  /** Start of sequence token ID */\n  bosTokenId?: number;\n  /** End of sequence token ID */\n  eosTokenId?: number;\n  /** Separator token ID */\n  sepTokenId?: number;\n  /** CLS token ID */\n  clsTokenId?: number;\n  /** Mask token ID */\n  maskTokenId?: number;\n}\n\n/**\n * Tokenized output\n */\nexport interface TokenizedOutput {\n  /** Input IDs */\n  inputIds: number[];\n  /** Attention mask */\n  attentionMask: number[];\n  /** Token type IDs (for segment embeddings) */\n  tokenTypeIds?: number[];\n  /** Special tokens mask */\n  specialTokensMask?: number[];\n  /** Offset mapping (for token-level tasks) */\n  offsetMapping?: [number, number][];\n}\n\n// ============================================================================\n// Error Types\n// ============================================================================\n\n/**\n * Base error class for edgeFlow errors\n */\nexport class EdgeFlowError extends Error {\n  constructor(\n    message: string,\n    public readonly code: string,\n    public readonly details?: Record<string, unknown>\n  ) {\n    super(message);\n    this.name = 'EdgeFlowError';\n  }\n}\n\n/**\n * Error codes\n */\nexport const ErrorCodes = {\n  // Runtime errors\n  RUNTIME_NOT_AVAILABLE: 'RUNTIME_NOT_AVAILABLE',\n  RUNTIME_INIT_FAILED: 'RUNTIME_INIT_FAILED',\n  RUNTIME_NOT_INITIALIZED: 'RUNTIME_NOT_INITIALIZED',\n  \n  // Model errors\n  MODEL_NOT_FOUND: 'MODEL_NOT_FOUND',\n  MODEL_LOAD_FAILED: 'MODEL_LOAD_FAILED',\n  MODEL_INVALID_FORMAT: 'MODEL_INVALID_FORMAT',\n  MODEL_NOT_LOADED: 'MODEL_NOT_LOADED',\n  \n  // Inference errors\n  INFERENCE_FAILED: 'INFERENCE_FAILED',\n  INFERENCE_TIMEOUT: 'INFERENCE_TIMEOUT',\n  INFERENCE_CANCELLED: 'INFERENCE_CANCELLED',\n  \n  // Memory errors\n  OUT_OF_MEMORY: 'OUT_OF_MEMORY',\n  MEMORY_LEAK_DETECTED: 'MEMORY_LEAK_DETECTED',\n  \n  // Tensor errors\n  TENSOR_SHAPE_MISMATCH: 'TENSOR_SHAPE_MISMATCH',\n  TENSOR_DTYPE_MISMATCH: 'TENSOR_DTYPE_MISMATCH',\n  TENSOR_DISPOSED: 'TENSOR_DISPOSED',\n  \n  // Pipeline errors\n  PIPELINE_NOT_SUPPORTED: 'PIPELINE_NOT_SUPPORTED',\n  PIPELINE_INPUT_INVALID: 'PIPELINE_INPUT_INVALID',\n  \n  // General errors\n  INVALID_ARGUMENT: 'INVALID_ARGUMENT',\n  NOT_IMPLEMENTED: 'NOT_IMPLEMENTED',\n  UNKNOWN_ERROR: 'UNKNOWN_ERROR',\n} as const;\n\nexport type ErrorCode = typeof ErrorCodes[keyof typeof ErrorCodes];\n\n// ============================================================================\n// Event Types\n// ============================================================================\n\n/**\n * Event types emitted by edgeFlow\n */\nexport type EventType = \n  | 'model:loading'\n  | 'model:loaded'\n  | 'model:unloaded'\n  | 'inference:start'\n  | 'inference:complete'\n  | 'inference:error'\n  | 'memory:warning'\n  | 'memory:gc'\n  | 'runtime:ready'\n  | 'runtime:error';\n\n/**\n * Event payload interface\n */\nexport interface EdgeFlowEvent<T = unknown> {\n  type: EventType;\n  timestamp: number;\n  data: T;\n}\n\n/**\n * Event listener function type\n */\nexport type EventListener<T = unknown> = (event: EdgeFlowEvent<T>) => void;\n", "/**\n * edgeFlow.js - Tensor Implementation\n * \n * Lightweight tensor implementation with efficient memory management.\n */\n\nimport { \n  Tensor, \n  DataType, \n  Shape, \n  TypedArray,\n  EdgeFlowError,\n  ErrorCodes \n} from './types.js';\n\n// Counter for generating unique tensor IDs\nlet tensorIdCounter = 0;\n\n/**\n * Generate a unique tensor ID\n */\nfunction generateTensorId(): string {\n  return `tensor_${++tensorIdCounter}_${Date.now().toString(36)}`;\n}\n\n/**\n * Get the typed array constructor for a data type\n */\nfunction getTypedArrayConstructor(dtype: DataType): new (length: number) => TypedArray {\n  switch (dtype) {\n    case 'float32':\n      return Float32Array;\n    case 'float16':\n      // Float16 not natively supported, use Float32Array\n      return Float32Array;\n    case 'int32':\n      return Int32Array;\n    case 'int64':\n      return BigInt64Array as unknown as new (length: number) => TypedArray;\n    case 'uint8':\n    case 'bool':\n      return Uint8Array;\n    case 'int8':\n      return Int8Array;\n    default:\n      throw new EdgeFlowError(\n        `Unsupported data type: ${dtype}`,\n        ErrorCodes.INVALID_ARGUMENT,\n        { dtype }\n      );\n  }\n}\n\n/**\n * Calculate the total number of elements from shape\n */\nfunction calculateSize(shape: Shape): number {\n  if (shape.length === 0) return 1; // Scalar\n  return shape.reduce((acc, dim) => acc * dim, 1);\n}\n\n/**\n * Validate tensor shape\n */\nfunction validateShape(shape: Shape): void {\n  for (let i = 0; i < shape.length; i++) {\n    const dim = shape[i];\n    if (dim === undefined || !Number.isInteger(dim) || dim < 0) {\n      throw new EdgeFlowError(\n        `Invalid shape dimension at index ${i}: ${dim}`,\n        ErrorCodes.INVALID_ARGUMENT,\n        { shape, index: i, dimension: dim }\n      );\n    }\n  }\n}\n\n/**\n * EdgeFlowTensor - Core tensor implementation\n */\nexport class EdgeFlowTensor implements Tensor {\n  readonly id: string;\n  readonly dtype: DataType;\n  readonly shape: Shape;\n  readonly size: number;\n  private _data: TypedArray;\n  private _isDisposed: boolean = false;\n\n  constructor(\n    data: TypedArray | number[],\n    shape: Shape,\n    dtype: DataType = 'float32'\n  ) {\n    validateShape(shape);\n    \n    this.id = generateTensorId();\n    this.dtype = dtype;\n    this.shape = Object.freeze([...shape]) as Shape;\n    this.size = calculateSize(this.shape);\n\n    // Validate data size matches shape\n    const expectedSize = this.size;\n    if (data.length !== expectedSize) {\n      throw new EdgeFlowError(\n        `Data length (${data.length}) does not match shape ${JSON.stringify(shape)} (expected ${expectedSize})`,\n        ErrorCodes.TENSOR_SHAPE_MISMATCH,\n        { dataLength: data.length, expectedSize, shape }\n      );\n    }\n\n    // Convert to appropriate typed array\n    if (data instanceof Array) {\n      const TypedArrayCtor = getTypedArrayConstructor(dtype);\n      this._data = new TypedArrayCtor(data.length);\n      \n      if (dtype === 'int64') {\n        // BigInt64Array requires BigInt values\n        const bigIntData = this._data as unknown as BigInt64Array;\n        for (let i = 0; i < data.length; i++) {\n          bigIntData[i] = BigInt(Math.round(data[i] ?? 0));\n        }\n      } else {\n        for (let i = 0; i < data.length; i++) {\n          (this._data as Float32Array)[i] = data[i] ?? 0;\n        }\n      }\n    } else {\n      this._data = data;\n    }\n  }\n\n  get data(): TypedArray {\n    this.checkDisposed();\n    return this._data;\n  }\n\n  get isDisposed(): boolean {\n    return this._isDisposed;\n  }\n\n  /**\n   * Check if tensor has been disposed\n   */\n  private checkDisposed(): void {\n    if (this._isDisposed) {\n      throw new EdgeFlowError(\n        'Cannot access disposed tensor',\n        ErrorCodes.TENSOR_DISPOSED,\n        { tensorId: this.id }\n      );\n    }\n  }\n\n  /**\n   * Convert to Float32Array\n   */\n  toFloat32Array(): Float32Array {\n    this.checkDisposed();\n    \n    if (this._data instanceof Float32Array) {\n      return this._data;\n    }\n    \n    const result = new Float32Array(this.size);\n    for (let i = 0; i < this.size; i++) {\n      result[i] = Number(this._data[i] ?? 0);\n    }\n    return result;\n  }\n\n  /**\n   * Convert to regular array\n   */\n  toArray(): number[] {\n    this.checkDisposed();\n    if (this.dtype === 'int64') {\n      // BigInt64Array needs special handling\n      const bigIntData = this._data as unknown as BigInt64Array;\n      const result: number[] = [];\n      for (let i = 0; i < bigIntData.length; i++) {\n        result.push(Number(bigIntData[i]));\n      }\n      return result;\n    }\n    return Array.from(this._data as Float32Array);\n  }\n\n  /**\n   * Clone the tensor\n   */\n  clone(): EdgeFlowTensor {\n    this.checkDisposed();\n    \n    const TypedArrayCtor = this._data.constructor as new (data: TypedArray) => TypedArray;\n    const clonedData = new TypedArrayCtor(this._data);\n    return new EdgeFlowTensor(clonedData, this.shape, this.dtype);\n  }\n\n  /**\n   * Dispose the tensor and free memory\n   */\n  dispose(): void {\n    if (!this._isDisposed) {\n      this._isDisposed = true;\n      // Help garbage collection - use Object.assign to avoid type issues\n      Object.assign(this, { _data: null });\n    }\n  }\n\n  /**\n   * Get value at specific indices\n   */\n  get(...indices: number[]): number {\n    this.checkDisposed();\n    \n    if (indices.length !== this.shape.length) {\n      throw new EdgeFlowError(\n        `Expected ${this.shape.length} indices, got ${indices.length}`,\n        ErrorCodes.INVALID_ARGUMENT,\n        { expectedIndices: this.shape.length, gotIndices: indices.length }\n      );\n    }\n\n    let flatIndex = 0;\n    let stride = 1;\n    \n    for (let i = this.shape.length - 1; i >= 0; i--) {\n      const idx = indices[i] ?? 0;\n      const dim = this.shape[i] ?? 1;\n      \n      if (idx < 0 || idx >= dim) {\n        throw new EdgeFlowError(\n          `Index ${idx} out of bounds for dimension ${i} with size ${dim}`,\n          ErrorCodes.INVALID_ARGUMENT,\n          { index: idx, dimension: i, size: dim }\n        );\n      }\n      \n      flatIndex += idx * stride;\n      stride *= dim;\n    }\n\n    return Number(this._data[flatIndex] ?? 0);\n  }\n\n  /**\n   * Set value at specific indices\n   */\n  set(value: number, ...indices: number[]): void {\n    this.checkDisposed();\n    \n    if (indices.length !== this.shape.length) {\n      throw new EdgeFlowError(\n        `Expected ${this.shape.length} indices, got ${indices.length}`,\n        ErrorCodes.INVALID_ARGUMENT,\n        { expectedIndices: this.shape.length, gotIndices: indices.length }\n      );\n    }\n\n    let flatIndex = 0;\n    let stride = 1;\n    \n    for (let i = this.shape.length - 1; i >= 0; i--) {\n      const idx = indices[i] ?? 0;\n      const dim = this.shape[i] ?? 1;\n      \n      if (idx < 0 || idx >= dim) {\n        throw new EdgeFlowError(\n          `Index ${idx} out of bounds for dimension ${i} with size ${dim}`,\n          ErrorCodes.INVALID_ARGUMENT,\n          { index: idx, dimension: i, size: dim }\n        );\n      }\n      \n      flatIndex += idx * stride;\n      stride *= dim;\n    }\n\n    (this._data as Float32Array)[flatIndex] = value;\n  }\n\n  /**\n   * Reshape the tensor (returns new tensor)\n   */\n  reshape(newShape: Shape): EdgeFlowTensor {\n    this.checkDisposed();\n    \n    const newSize = calculateSize(newShape);\n    if (newSize !== this.size) {\n      throw new EdgeFlowError(\n        `Cannot reshape tensor of size ${this.size} to shape ${JSON.stringify(newShape)} (size ${newSize})`,\n        ErrorCodes.TENSOR_SHAPE_MISMATCH,\n        { currentSize: this.size, newSize, newShape }\n      );\n    }\n\n    const TypedArrayCtor = this._data.constructor as new (data: TypedArray) => TypedArray;\n    const clonedData = new TypedArrayCtor(this._data);\n    return new EdgeFlowTensor(clonedData, newShape, this.dtype);\n  }\n\n  /**\n   * Transpose the tensor (2D only for now)\n   */\n  transpose(): EdgeFlowTensor {\n    this.checkDisposed();\n    \n    if (this.shape.length !== 2) {\n      throw new EdgeFlowError(\n        'Transpose is currently only supported for 2D tensors',\n        ErrorCodes.NOT_IMPLEMENTED,\n        { shape: this.shape }\n      );\n    }\n\n    const [rows, cols] = this.shape as [number, number];\n    const result = new Float32Array(this.size);\n    \n    for (let i = 0; i < rows; i++) {\n      for (let j = 0; j < cols; j++) {\n        result[j * rows + i] = Number(this._data[i * cols + j] ?? 0);\n      }\n    }\n\n    return new EdgeFlowTensor(result, [cols, rows], this.dtype);\n  }\n\n  /**\n   * Create string representation\n   */\n  toString(): string {\n    return `Tensor(shape=[${this.shape.join(', ')}], dtype=${this.dtype})`;\n  }\n}\n\n// ============================================================================\n// Tensor Factory Functions\n// ============================================================================\n\n/**\n * Create a tensor from data\n */\nexport function tensor(\n  data: TypedArray | number[] | number[][],\n  shape?: Shape,\n  dtype: DataType = 'float32'\n): EdgeFlowTensor {\n  // Handle nested arrays\n  if (Array.isArray(data) && data.length > 0 && Array.isArray(data[0])) {\n    const rows = data.length;\n    const cols = (data[0] as number[]).length;\n    const flatData: number[] = [];\n    \n    for (const row of data as number[][]) {\n      if (row.length !== cols) {\n        throw new EdgeFlowError(\n          'Nested arrays must have consistent dimensions',\n          ErrorCodes.INVALID_ARGUMENT\n        );\n      }\n      flatData.push(...row);\n    }\n    \n    return new EdgeFlowTensor(flatData, shape ?? [rows, cols], dtype);\n  }\n\n  // Infer shape if not provided\n  const inferredShape = shape ?? [data.length];\n  return new EdgeFlowTensor(data as TypedArray | number[], inferredShape, dtype);\n}\n\n/**\n * Create a tensor filled with zeros\n */\nexport function zeros(shape: Shape, dtype: DataType = 'float32'): EdgeFlowTensor {\n  const size = calculateSize(shape);\n  const TypedArrayCtor = getTypedArrayConstructor(dtype);\n  const data = new TypedArrayCtor(size);\n  return new EdgeFlowTensor(data, shape, dtype);\n}\n\n/**\n * Create a tensor filled with ones\n */\nexport function ones(shape: Shape, dtype: DataType = 'float32'): EdgeFlowTensor {\n  const size = calculateSize(shape);\n  const TypedArrayCtor = getTypedArrayConstructor(dtype);\n  const data = new TypedArrayCtor(size);\n  data.fill(1 as never);\n  return new EdgeFlowTensor(data, shape, dtype);\n}\n\n/**\n * Create a tensor filled with a specific value\n */\nexport function full(\n  shape: Shape, \n  value: number, \n  dtype: DataType = 'float32'\n): EdgeFlowTensor {\n  const size = calculateSize(shape);\n  const TypedArrayCtor = getTypedArrayConstructor(dtype);\n  const data = new TypedArrayCtor(size);\n  data.fill(value as never);\n  return new EdgeFlowTensor(data, shape, dtype);\n}\n\n/**\n * Create a tensor with random values between 0 and 1\n */\nexport function random(shape: Shape, dtype: DataType = 'float32'): EdgeFlowTensor {\n  const size = calculateSize(shape);\n  const data = new Float32Array(size);\n  for (let i = 0; i < size; i++) {\n    data[i] = Math.random();\n  }\n  return new EdgeFlowTensor(data, shape, dtype);\n}\n\n/**\n * Create a tensor with random values from normal distribution\n */\nexport function randn(shape: Shape, dtype: DataType = 'float32'): EdgeFlowTensor {\n  const size = calculateSize(shape);\n  const data = new Float32Array(size);\n  \n  // Box-Muller transform for normal distribution\n  for (let i = 0; i < size; i += 2) {\n    const u1 = Math.random();\n    const u2 = Math.random();\n    const r = Math.sqrt(-2 * Math.log(u1));\n    const theta = 2 * Math.PI * u2;\n    \n    data[i] = r * Math.cos(theta);\n    if (i + 1 < size) {\n      data[i + 1] = r * Math.sin(theta);\n    }\n  }\n  \n  return new EdgeFlowTensor(data, shape, dtype);\n}\n\n/**\n * Create a 1D tensor with evenly spaced values\n */\nexport function arange(\n  start: number, \n  stop?: number, \n  step: number = 1, \n  dtype: DataType = 'float32'\n): EdgeFlowTensor {\n  if (stop === undefined) {\n    stop = start;\n    start = 0;\n  }\n  \n  const size = Math.ceil((stop - start) / step);\n  const data = new Float32Array(size);\n  \n  for (let i = 0; i < size; i++) {\n    data[i] = start + i * step;\n  }\n  \n  return new EdgeFlowTensor(data, [size], dtype);\n}\n\n/**\n * Create a 1D tensor with evenly spaced values (specify number of points)\n */\nexport function linspace(\n  start: number, \n  stop: number, \n  num: number = 50, \n  dtype: DataType = 'float32'\n): EdgeFlowTensor {\n  const data = new Float32Array(num);\n  const step = (stop - start) / (num - 1);\n  \n  for (let i = 0; i < num; i++) {\n    data[i] = start + i * step;\n  }\n  \n  return new EdgeFlowTensor(data, [num], dtype);\n}\n\n/**\n * Create an identity matrix\n */\nexport function eye(n: number, dtype: DataType = 'float32'): EdgeFlowTensor {\n  const data = new Float32Array(n * n);\n  \n  for (let i = 0; i < n; i++) {\n    data[i * n + i] = 1;\n  }\n  \n  return new EdgeFlowTensor(data, [n, n], dtype);\n}\n\n// ============================================================================\n// Tensor Operations\n// ============================================================================\n\n/**\n * Element-wise addition\n */\nexport function add(a: EdgeFlowTensor, b: EdgeFlowTensor | number): EdgeFlowTensor {\n  if (typeof b === 'number') {\n    const result = new Float32Array(a.size);\n    const aData = a.toFloat32Array();\n    for (let i = 0; i < a.size; i++) {\n      result[i] = (aData[i] ?? 0) + b;\n    }\n    return new EdgeFlowTensor(result, a.shape, a.dtype);\n  }\n\n  if (a.size !== b.size) {\n    throw new EdgeFlowError(\n      'Tensor sizes must match for element-wise operations',\n      ErrorCodes.TENSOR_SHAPE_MISMATCH,\n      { aShape: a.shape, bShape: b.shape }\n    );\n  }\n\n  const result = new Float32Array(a.size);\n  const aData = a.toFloat32Array();\n  const bData = b.toFloat32Array();\n  \n  for (let i = 0; i < a.size; i++) {\n    result[i] = (aData[i] ?? 0) + (bData[i] ?? 0);\n  }\n  \n  return new EdgeFlowTensor(result, a.shape, a.dtype);\n}\n\n/**\n * Element-wise subtraction\n */\nexport function sub(a: EdgeFlowTensor, b: EdgeFlowTensor | number): EdgeFlowTensor {\n  if (typeof b === 'number') {\n    const result = new Float32Array(a.size);\n    const aData = a.toFloat32Array();\n    for (let i = 0; i < a.size; i++) {\n      result[i] = (aData[i] ?? 0) - b;\n    }\n    return new EdgeFlowTensor(result, a.shape, a.dtype);\n  }\n\n  if (a.size !== b.size) {\n    throw new EdgeFlowError(\n      'Tensor sizes must match for element-wise operations',\n      ErrorCodes.TENSOR_SHAPE_MISMATCH,\n      { aShape: a.shape, bShape: b.shape }\n    );\n  }\n\n  const result = new Float32Array(a.size);\n  const aData = a.toFloat32Array();\n  const bData = b.toFloat32Array();\n  \n  for (let i = 0; i < a.size; i++) {\n    result[i] = (aData[i] ?? 0) - (bData[i] ?? 0);\n  }\n  \n  return new EdgeFlowTensor(result, a.shape, a.dtype);\n}\n\n/**\n * Element-wise multiplication\n */\nexport function mul(a: EdgeFlowTensor, b: EdgeFlowTensor | number): EdgeFlowTensor {\n  if (typeof b === 'number') {\n    const result = new Float32Array(a.size);\n    const aData = a.toFloat32Array();\n    for (let i = 0; i < a.size; i++) {\n      result[i] = (aData[i] ?? 0) * b;\n    }\n    return new EdgeFlowTensor(result, a.shape, a.dtype);\n  }\n\n  if (a.size !== b.size) {\n    throw new EdgeFlowError(\n      'Tensor sizes must match for element-wise operations',\n      ErrorCodes.TENSOR_SHAPE_MISMATCH,\n      { aShape: a.shape, bShape: b.shape }\n    );\n  }\n\n  const result = new Float32Array(a.size);\n  const aData = a.toFloat32Array();\n  const bData = b.toFloat32Array();\n  \n  for (let i = 0; i < a.size; i++) {\n    result[i] = (aData[i] ?? 0) * (bData[i] ?? 0);\n  }\n  \n  return new EdgeFlowTensor(result, a.shape, a.dtype);\n}\n\n/**\n * Element-wise division\n */\nexport function div(a: EdgeFlowTensor, b: EdgeFlowTensor | number): EdgeFlowTensor {\n  if (typeof b === 'number') {\n    const result = new Float32Array(a.size);\n    const aData = a.toFloat32Array();\n    for (let i = 0; i < a.size; i++) {\n      result[i] = (aData[i] ?? 0) / b;\n    }\n    return new EdgeFlowTensor(result, a.shape, a.dtype);\n  }\n\n  if (a.size !== b.size) {\n    throw new EdgeFlowError(\n      'Tensor sizes must match for element-wise operations',\n      ErrorCodes.TENSOR_SHAPE_MISMATCH,\n      { aShape: a.shape, bShape: b.shape }\n    );\n  }\n\n  const result = new Float32Array(a.size);\n  const aData = a.toFloat32Array();\n  const bData = b.toFloat32Array();\n  \n  for (let i = 0; i < a.size; i++) {\n    result[i] = (aData[i] ?? 0) / (bData[i] ?? 0);\n  }\n  \n  return new EdgeFlowTensor(result, a.shape, a.dtype);\n}\n\n/**\n * Matrix multiplication (2D tensors)\n */\nexport function matmul(a: EdgeFlowTensor, b: EdgeFlowTensor): EdgeFlowTensor {\n  if (a.shape.length !== 2 || b.shape.length !== 2) {\n    throw new EdgeFlowError(\n      'matmul requires 2D tensors',\n      ErrorCodes.INVALID_ARGUMENT,\n      { aShape: a.shape, bShape: b.shape }\n    );\n  }\n\n  const [m, k1] = a.shape as [number, number];\n  const [k2, n] = b.shape as [number, number];\n\n  if (k1 !== k2) {\n    throw new EdgeFlowError(\n      `Matrix dimensions incompatible for multiplication: (${m}x${k1}) @ (${k2}x${n})`,\n      ErrorCodes.TENSOR_SHAPE_MISMATCH,\n      { aShape: a.shape, bShape: b.shape }\n    );\n  }\n\n  const result = new Float32Array(m * n);\n  const aData = a.toFloat32Array();\n  const bData = b.toFloat32Array();\n\n  for (let i = 0; i < m; i++) {\n    for (let j = 0; j < n; j++) {\n      let sum = 0;\n      for (let k = 0; k < k1; k++) {\n        sum += (aData[i * k1 + k] ?? 0) * (bData[k * n + j] ?? 0);\n      }\n      result[i * n + j] = sum;\n    }\n  }\n\n  return new EdgeFlowTensor(result, [m, n], a.dtype);\n}\n\n/**\n * Softmax activation\n */\nexport function softmax(t: EdgeFlowTensor, axis: number = -1): EdgeFlowTensor {\n  const data = t.toFloat32Array();\n  const result = new Float32Array(t.size);\n  \n  // Handle negative axis\n  const actualAxis = axis < 0 ? t.shape.length + axis : axis;\n  \n  if (actualAxis < 0 || actualAxis >= t.shape.length) {\n    throw new EdgeFlowError(\n      `Invalid axis ${axis} for tensor with ${t.shape.length} dimensions`,\n      ErrorCodes.INVALID_ARGUMENT,\n      { axis, shape: t.shape }\n    );\n  }\n\n  // For 1D tensors\n  if (t.shape.length === 1) {\n    let max = -Infinity;\n    for (let i = 0; i < t.size; i++) {\n      if ((data[i] ?? 0) > max) max = data[i] ?? 0;\n    }\n    \n    let sum = 0;\n    for (let i = 0; i < t.size; i++) {\n      result[i] = Math.exp((data[i] ?? 0) - max);\n      sum += result[i] ?? 0;\n    }\n    \n    for (let i = 0; i < t.size; i++) {\n      result[i] = (result[i] ?? 0) / sum;\n    }\n    \n    return new EdgeFlowTensor(result, t.shape, t.dtype);\n  }\n\n  // For 2D tensors along last axis\n  if (t.shape.length === 2 && actualAxis === 1) {\n    const [rows, cols] = t.shape as [number, number];\n    \n    for (let i = 0; i < rows; i++) {\n      let max = -Infinity;\n      for (let j = 0; j < cols; j++) {\n        if ((data[i * cols + j] ?? 0) > max) max = data[i * cols + j] ?? 0;\n      }\n      \n      let sum = 0;\n      for (let j = 0; j < cols; j++) {\n        result[i * cols + j] = Math.exp((data[i * cols + j] ?? 0) - max);\n        sum += result[i * cols + j] ?? 0;\n      }\n      \n      for (let j = 0; j < cols; j++) {\n        result[i * cols + j] = (result[i * cols + j] ?? 0) / sum;\n      }\n    }\n    \n    return new EdgeFlowTensor(result, t.shape, t.dtype);\n  }\n\n  throw new EdgeFlowError(\n    'Softmax currently only supports 1D tensors or 2D tensors along the last axis',\n    ErrorCodes.NOT_IMPLEMENTED,\n    { shape: t.shape, axis }\n  );\n}\n\n/**\n * ReLU activation\n */\nexport function relu(t: EdgeFlowTensor): EdgeFlowTensor {\n  const data = t.toFloat32Array();\n  const result = new Float32Array(t.size);\n  \n  for (let i = 0; i < t.size; i++) {\n    result[i] = Math.max(0, data[i] ?? 0);\n  }\n  \n  return new EdgeFlowTensor(result, t.shape, t.dtype);\n}\n\n/**\n * Sigmoid activation\n */\nexport function sigmoid(t: EdgeFlowTensor): EdgeFlowTensor {\n  const data = t.toFloat32Array();\n  const result = new Float32Array(t.size);\n  \n  for (let i = 0; i < t.size; i++) {\n    result[i] = 1 / (1 + Math.exp(-(data[i] ?? 0)));\n  }\n  \n  return new EdgeFlowTensor(result, t.shape, t.dtype);\n}\n\n/**\n * Tanh activation\n */\nexport function tanh(t: EdgeFlowTensor): EdgeFlowTensor {\n  const data = t.toFloat32Array();\n  const result = new Float32Array(t.size);\n  \n  for (let i = 0; i < t.size; i++) {\n    result[i] = Math.tanh(data[i] ?? 0);\n  }\n  \n  return new EdgeFlowTensor(result, t.shape, t.dtype);\n}\n\n/**\n * Sum all elements or along an axis\n */\nexport function sum(t: EdgeFlowTensor, axis?: number): EdgeFlowTensor | number {\n  const data = t.toFloat32Array();\n  \n  if (axis === undefined) {\n    let total = 0;\n    for (let i = 0; i < t.size; i++) {\n      total += data[i] ?? 0;\n    }\n    return total;\n  }\n\n  // Handle negative axis\n  const actualAxis = axis < 0 ? t.shape.length + axis : axis;\n  \n  if (actualAxis < 0 || actualAxis >= t.shape.length) {\n    throw new EdgeFlowError(\n      `Invalid axis ${axis} for tensor with ${t.shape.length} dimensions`,\n      ErrorCodes.INVALID_ARGUMENT,\n      { axis, shape: t.shape }\n    );\n  }\n\n  // Calculate new shape\n  const newShape = [...t.shape];\n  newShape.splice(actualAxis, 1);\n  \n  if (newShape.length === 0) {\n    let total = 0;\n    for (let i = 0; i < t.size; i++) {\n      total += data[i] ?? 0;\n    }\n    return total;\n  }\n\n  // For 2D sum along axis\n  if (t.shape.length === 2) {\n    const [rows, cols] = t.shape as [number, number];\n    \n    if (actualAxis === 0) {\n      const result = new Float32Array(cols);\n      for (let j = 0; j < cols; j++) {\n        for (let i = 0; i < rows; i++) {\n          result[j] = (result[j] ?? 0) + (data[i * cols + j] ?? 0);\n        }\n      }\n      return new EdgeFlowTensor(result, [cols], t.dtype);\n    } else {\n      const result = new Float32Array(rows);\n      for (let i = 0; i < rows; i++) {\n        for (let j = 0; j < cols; j++) {\n          result[i] = (result[i] ?? 0) + (data[i * cols + j] ?? 0);\n        }\n      }\n      return new EdgeFlowTensor(result, [rows], t.dtype);\n    }\n  }\n\n  throw new EdgeFlowError(\n    'Sum along axis currently only supports up to 2D tensors',\n    ErrorCodes.NOT_IMPLEMENTED,\n    { shape: t.shape, axis }\n  );\n}\n\n/**\n * Mean of all elements or along an axis\n */\nexport function mean(t: EdgeFlowTensor, axis?: number): EdgeFlowTensor | number {\n  if (axis === undefined) {\n    return (sum(t) as number) / t.size;\n  }\n\n  const result = sum(t, axis);\n  if (typeof result === 'number') {\n    return result / (t.shape[axis] ?? 1);\n  }\n\n  const axisSize = t.shape[axis] ?? 1;\n  return div(result, axisSize);\n}\n\n/**\n * Argmax - return index of maximum value\n */\nexport function argmax(t: EdgeFlowTensor, axis?: number): number | EdgeFlowTensor {\n  const data = t.toFloat32Array();\n  \n  if (axis === undefined) {\n    let maxIdx = 0;\n    let maxVal = data[0] ?? -Infinity;\n    \n    for (let i = 1; i < t.size; i++) {\n      if ((data[i] ?? -Infinity) > maxVal) {\n        maxVal = data[i] ?? -Infinity;\n        maxIdx = i;\n      }\n    }\n    return maxIdx;\n  }\n\n  // Handle negative axis\n  const actualAxis = axis < 0 ? t.shape.length + axis : axis;\n  \n  // For 2D along last axis\n  if (t.shape.length === 2 && actualAxis === 1) {\n    const [rows, cols] = t.shape as [number, number];\n    const result = new Float32Array(rows);\n    \n    for (let i = 0; i < rows; i++) {\n      let maxIdx = 0;\n      let maxVal = data[i * cols] ?? -Infinity;\n      \n      for (let j = 1; j < cols; j++) {\n        if ((data[i * cols + j] ?? -Infinity) > maxVal) {\n          maxVal = data[i * cols + j] ?? -Infinity;\n          maxIdx = j;\n        }\n      }\n      result[i] = maxIdx;\n    }\n    \n    return new EdgeFlowTensor(result, [rows], 'int32');\n  }\n\n  throw new EdgeFlowError(\n    'Argmax along axis currently only supports 2D tensors along the last axis',\n    ErrorCodes.NOT_IMPLEMENTED,\n    { shape: t.shape, axis }\n  );\n}\n\n/**\n * Concatenate tensors along an axis\n */\nexport function concat(tensors: EdgeFlowTensor[], axis: number = 0): EdgeFlowTensor {\n  if (tensors.length === 0) {\n    throw new EdgeFlowError(\n      'Cannot concatenate empty array of tensors',\n      ErrorCodes.INVALID_ARGUMENT\n    );\n  }\n\n  if (tensors.length === 1) {\n    return tensors[0]?.clone() ?? zeros([0]);\n  }\n\n  const first = tensors[0];\n  if (!first) {\n    throw new EdgeFlowError('First tensor is undefined', ErrorCodes.INVALID_ARGUMENT);\n  }\n\n  // Handle negative axis\n  const actualAxis = axis < 0 ? first.shape.length + axis : axis;\n\n  // Validate shapes\n  for (let i = 1; i < tensors.length; i++) {\n    const t = tensors[i];\n    if (!t) continue;\n    \n    if (t.shape.length !== first.shape.length) {\n      throw new EdgeFlowError(\n        'All tensors must have the same number of dimensions',\n        ErrorCodes.TENSOR_SHAPE_MISMATCH\n      );\n    }\n    \n    for (let j = 0; j < first.shape.length; j++) {\n      if (j !== actualAxis && first.shape[j] !== t.shape[j]) {\n        throw new EdgeFlowError(\n          `Shape mismatch at dimension ${j}`,\n          ErrorCodes.TENSOR_SHAPE_MISMATCH\n        );\n      }\n    }\n  }\n\n  // Calculate new shape\n  const newShape = [...first.shape];\n  let totalAxisSize = 0;\n  for (const t of tensors) {\n    if (t) totalAxisSize += t.shape[actualAxis] ?? 0;\n  }\n  newShape[actualAxis] = totalAxisSize;\n\n  // For 1D concatenation\n  if (first.shape.length === 1) {\n    const result = new Float32Array(totalAxisSize);\n    let offset = 0;\n    \n    for (const t of tensors) {\n      if (!t) continue;\n      result.set(t.toFloat32Array(), offset);\n      offset += t.size;\n    }\n    \n    return new EdgeFlowTensor(result, newShape, first.dtype);\n  }\n\n  throw new EdgeFlowError(\n    'Concatenation currently only supports 1D tensors',\n    ErrorCodes.NOT_IMPLEMENTED\n  );\n}\n", "/**\n * edgeFlow.js - Inference Scheduler\n * \n * Task scheduler for managing concurrent inference execution.\n * Supports priority queues, model-level isolation, and batch processing.\n */\n\nimport {\n  InferenceTask,\n  TaskPriority,\n  TaskStatus,\n  SchedulerOptions,\n  EdgeFlowError,\n  ErrorCodes,\n  EventType,\n  EventListener,\n  EdgeFlowEvent,\n} from './types.js';\n\n// ============================================================================\n// Task Implementation\n// ============================================================================\n\n/**\n * Internal task implementation\n */\nclass Task<T = unknown> implements InferenceTask<T> {\n  readonly id: string;\n  readonly modelId: string;\n  readonly priority: TaskPriority;\n  readonly createdAt: number;\n  \n  private _status: TaskStatus = 'pending';\n  private _startedAt?: number;\n  private _completedAt?: number;\n  private _result?: T;\n  private _error?: Error;\n  private _executor: () => Promise<T>;\n  private _resolvers: Array<{\n    resolve: (value: T) => void;\n    reject: (error: Error) => void;\n  }> = [];\n  private _cancelled = false;\n\n  constructor(\n    id: string,\n    modelId: string,\n    priority: TaskPriority,\n    executor: () => Promise<T>\n  ) {\n    this.id = id;\n    this.modelId = modelId;\n    this.priority = priority;\n    this.createdAt = Date.now();\n    this._executor = executor;\n  }\n\n  get status(): TaskStatus {\n    return this._status;\n  }\n\n  get startedAt(): number | undefined {\n    return this._startedAt;\n  }\n\n  get completedAt(): number | undefined {\n    return this._completedAt;\n  }\n\n  get result(): T | undefined {\n    return this._result;\n  }\n\n  get error(): Error | undefined {\n    return this._error;\n  }\n\n  /**\n   * Cancel the task\n   */\n  cancel(): void {\n    if (this._status === 'pending') {\n      this._cancelled = true;\n      this._status = 'cancelled';\n      this._completedAt = Date.now();\n      \n      const cancelError = new EdgeFlowError(\n        'Task was cancelled',\n        ErrorCodes.INFERENCE_CANCELLED,\n        { taskId: this.id }\n      );\n      \n      for (const { reject } of this._resolvers) {\n        reject(cancelError);\n      }\n      this._resolvers = [];\n    }\n  }\n\n  /**\n   * Wait for task completion\n   */\n  wait(): Promise<T> {\n    if (this._status === 'completed') {\n      return Promise.resolve(this._result as T);\n    }\n    \n    if (this._status === 'failed') {\n      return Promise.reject(this._error);\n    }\n    \n    if (this._status === 'cancelled') {\n      return Promise.reject(new EdgeFlowError(\n        'Task was cancelled',\n        ErrorCodes.INFERENCE_CANCELLED,\n        { taskId: this.id }\n      ));\n    }\n\n    return new Promise<T>((resolve, reject) => {\n      this._resolvers.push({ resolve, reject });\n    });\n  }\n\n  /**\n   * Execute the task\n   */\n  async execute(): Promise<void> {\n    if (this._cancelled) {\n      return;\n    }\n\n    this._status = 'running';\n    this._startedAt = Date.now();\n\n    try {\n      this._result = await this._executor();\n      this._status = 'completed';\n      this._completedAt = Date.now();\n      \n      for (const { resolve } of this._resolvers) {\n        resolve(this._result);\n      }\n    } catch (err) {\n      this._error = err instanceof Error ? err : new Error(String(err));\n      this._status = 'failed';\n      this._completedAt = Date.now();\n      \n      for (const { reject } of this._resolvers) {\n        reject(this._error);\n      }\n    }\n    \n    this._resolvers = [];\n  }\n}\n\n// ============================================================================\n// Priority Queue Implementation\n// ============================================================================\n\n/**\n * Priority mapping for comparison\n */\nconst PRIORITY_ORDER: Record<TaskPriority, number> = {\n  critical: 0,\n  high: 1,\n  normal: 2,\n  low: 3,\n};\n\n/**\n * Priority queue for tasks\n */\nclass PriorityQueue<T extends Task> {\n  private items: T[] = [];\n\n  get length(): number {\n    return this.items.length;\n  }\n\n  isEmpty(): boolean {\n    return this.items.length === 0;\n  }\n\n  /**\n   * Add item to queue with priority ordering\n   */\n  enqueue(item: T): void {\n    let inserted = false;\n    \n    for (let i = 0; i < this.items.length; i++) {\n      const currentItem = this.items[i];\n      if (currentItem && PRIORITY_ORDER[item.priority] < PRIORITY_ORDER[currentItem.priority]) {\n        this.items.splice(i, 0, item);\n        inserted = true;\n        break;\n      }\n    }\n    \n    if (!inserted) {\n      this.items.push(item);\n    }\n  }\n\n  /**\n   * Remove and return highest priority item\n   */\n  dequeue(): T | undefined {\n    return this.items.shift();\n  }\n\n  /**\n   * Peek at highest priority item without removing\n   */\n  peek(): T | undefined {\n    return this.items[0];\n  }\n\n  /**\n   * Remove a specific item by ID\n   */\n  remove(id: string): T | undefined {\n    const index = this.items.findIndex(item => item.id === id);\n    if (index !== -1) {\n      const [removed] = this.items.splice(index, 1);\n      return removed;\n    }\n    return undefined;\n  }\n\n  /**\n   * Get all items\n   */\n  getAll(): T[] {\n    return [...this.items];\n  }\n\n  /**\n   * Clear the queue\n   */\n  clear(): void {\n    this.items = [];\n  }\n}\n\n// ============================================================================\n// Batch Collector\n// ============================================================================\n\n/**\n * Collects tasks for batch processing\n */\nclass BatchCollector<T> {\n  private tasks: Task<T>[] = [];\n  private timer: ReturnType<typeof setTimeout> | null = null;\n  private readonly maxSize: number;\n  private readonly timeout: number;\n  private readonly onBatch: (tasks: Task<T>[]) => void;\n\n  constructor(\n    maxSize: number,\n    timeout: number,\n    onBatch: (tasks: Task<T>[]) => void\n  ) {\n    this.maxSize = maxSize;\n    this.timeout = timeout;\n    this.onBatch = onBatch;\n  }\n\n  add(task: Task<T>): void {\n    this.tasks.push(task);\n\n    if (this.tasks.length >= this.maxSize) {\n      this.flush();\n    } else if (!this.timer) {\n      this.timer = setTimeout(() => this.flush(), this.timeout);\n    }\n  }\n\n  flush(): void {\n    if (this.timer) {\n      clearTimeout(this.timer);\n      this.timer = null;\n    }\n\n    if (this.tasks.length > 0) {\n      const batch = this.tasks;\n      this.tasks = [];\n      this.onBatch(batch);\n    }\n  }\n\n  clear(): void {\n    if (this.timer) {\n      clearTimeout(this.timer);\n      this.timer = null;\n    }\n    this.tasks = [];\n  }\n}\n\n// ============================================================================\n// Inference Scheduler\n// ============================================================================\n\n// Counter for task IDs\nlet taskIdCounter = 0;\n\n/**\n * Generate unique task ID\n */\nfunction generateTaskId(): string {\n  return `task_${++taskIdCounter}_${Date.now().toString(36)}`;\n}\n\n/**\n * Default scheduler options\n */\nconst DEFAULT_OPTIONS: Required<SchedulerOptions> = {\n  maxConcurrentTasks: 4,\n  maxConcurrentPerModel: 1,\n  defaultTimeout: 30000,\n  enableBatching: false,\n  maxBatchSize: 32,\n  batchTimeout: 50,\n};\n\n/**\n * InferenceScheduler - Manages concurrent task execution\n * \n * Features:\n * - Priority-based task scheduling\n * - Model-level concurrency control\n * - Optional batch processing\n * - Task cancellation\n * - Event emission\n */\nexport class InferenceScheduler {\n  private readonly options: Required<SchedulerOptions>;\n  private readonly queues: Map<string, PriorityQueue<Task>> = new Map();\n  private readonly runningTasks: Map<string, Set<string>> = new Map();\n  private readonly allTasks: Map<string, Task> = new Map();\n  private readonly batchers: Map<string, BatchCollector<unknown>> = new Map();\n  private readonly listeners: Map<EventType, Set<EventListener>> = new Map();\n  private globalRunningCount = 0;\n  private isProcessing = false;\n  private disposed = false;\n\n  constructor(options: SchedulerOptions = {}) {\n    this.options = { ...DEFAULT_OPTIONS, ...options };\n  }\n\n  /**\n   * Get or create queue for a model\n   */\n  private getQueue(modelId: string): PriorityQueue<Task> {\n    let queue = this.queues.get(modelId);\n    if (!queue) {\n      queue = new PriorityQueue<Task>();\n      this.queues.set(modelId, queue);\n    }\n    return queue;\n  }\n\n  /**\n   * Get or create running set for a model\n   */\n  private getRunningSet(modelId: string): Set<string> {\n    let running = this.runningTasks.get(modelId);\n    if (!running) {\n      running = new Set<string>();\n      this.runningTasks.set(modelId, running);\n    }\n    return running;\n  }\n\n  /**\n   * Check if we can start a new task for a model\n   */\n  private canStartTask(modelId: string): boolean {\n    if (this.globalRunningCount >= this.options.maxConcurrentTasks) {\n      return false;\n    }\n\n    const running = this.runningTasks.get(modelId);\n    if (running && running.size >= this.options.maxConcurrentPerModel) {\n      return false;\n    }\n\n    return true;\n  }\n\n  /**\n   * Process pending tasks\n   */\n  private async processQueue(): Promise<void> {\n    if (this.isProcessing || this.disposed) {\n      return;\n    }\n\n    this.isProcessing = true;\n\n    try {\n      // Find tasks that can be started\n      const tasksToStart: Task[] = [];\n\n      for (const [modelId, queue] of this.queues) {\n        while (!queue.isEmpty() && this.canStartTask(modelId)) {\n          const task = queue.dequeue();\n          if (task && task.status === 'pending') {\n            tasksToStart.push(task);\n            \n            const running = this.getRunningSet(modelId);\n            running.add(task.id);\n            this.globalRunningCount++;\n          }\n        }\n      }\n\n      // Execute tasks concurrently\n      await Promise.all(\n        tasksToStart.map(async (task) => {\n          this.emit('inference:start', { taskId: task.id, modelId: task.modelId });\n\n          try {\n            await task.execute();\n            this.emit('inference:complete', {\n              taskId: task.id,\n              modelId: task.modelId,\n              duration: (task.completedAt ?? 0) - (task.startedAt ?? 0),\n            });\n          } catch (error) {\n            this.emit('inference:error', {\n              taskId: task.id,\n              modelId: task.modelId,\n              error,\n            });\n          } finally {\n            // Clean up\n            const running = this.runningTasks.get(task.modelId);\n            if (running) {\n              running.delete(task.id);\n            }\n            this.globalRunningCount--;\n          }\n        })\n      );\n    } finally {\n      this.isProcessing = false;\n    }\n\n    // Check if there are more tasks to process\n    let hasPending = false;\n    for (const queue of this.queues.values()) {\n      if (!queue.isEmpty()) {\n        hasPending = true;\n        break;\n      }\n    }\n\n    if (hasPending) {\n      // Use setImmediate-like behavior for next tick processing\n      setTimeout(() => this.processQueue(), 0);\n    }\n  }\n\n  /**\n   * Schedule a task for execution\n   */\n  schedule<T>(\n    modelId: string,\n    executor: () => Promise<T>,\n    priority: TaskPriority = 'normal'\n  ): InferenceTask<T> {\n    if (this.disposed) {\n      throw new EdgeFlowError(\n        'Scheduler has been disposed',\n        ErrorCodes.RUNTIME_NOT_INITIALIZED\n      );\n    }\n\n    const task = new Task<T>(\n      generateTaskId(),\n      modelId,\n      priority,\n      executor\n    );\n\n    this.allTasks.set(task.id, task as Task);\n\n    // Add to queue\n    const queue = this.getQueue(modelId);\n    queue.enqueue(task as Task);\n\n    // Trigger processing\n    this.processQueue();\n\n    return task;\n  }\n\n  /**\n   * Schedule with timeout\n   */\n  scheduleWithTimeout<T>(\n    modelId: string,\n    executor: () => Promise<T>,\n    timeout: number = this.options.defaultTimeout,\n    priority: TaskPriority = 'normal'\n  ): InferenceTask<T> {\n    const timeoutExecutor = (): Promise<T> => {\n      return new Promise<T>((resolve, reject) => {\n        const timer = setTimeout(() => {\n          reject(new EdgeFlowError(\n            `Task timed out after ${timeout}ms`,\n            ErrorCodes.INFERENCE_TIMEOUT,\n            { timeout }\n          ));\n        }, timeout);\n\n        executor()\n          .then(result => {\n            clearTimeout(timer);\n            resolve(result);\n          })\n          .catch(error => {\n            clearTimeout(timer);\n            reject(error);\n          });\n      });\n    };\n\n    return this.schedule(modelId, timeoutExecutor, priority);\n  }\n\n  /**\n   * Schedule multiple tasks and wait for all\n   */\n  async scheduleAll<T>(\n    tasks: Array<{\n      modelId: string;\n      executor: () => Promise<T>;\n      priority?: TaskPriority;\n    }>\n  ): Promise<T[]> {\n    const scheduledTasks = tasks.map(({ modelId, executor, priority }) =>\n      this.schedule<T>(modelId, executor, priority)\n    );\n\n    return Promise.all(scheduledTasks.map(task => task.wait()));\n  }\n\n  /**\n   * Get task by ID\n   */\n  getTask(taskId: string): InferenceTask | undefined {\n    return this.allTasks.get(taskId);\n  }\n\n  /**\n   * Cancel a task\n   */\n  cancelTask(taskId: string): boolean {\n    const task = this.allTasks.get(taskId);\n    if (task && task.status === 'pending') {\n      task.cancel();\n      \n      // Remove from queue\n      for (const queue of this.queues.values()) {\n        queue.remove(taskId);\n      }\n      \n      return true;\n    }\n    return false;\n  }\n\n  /**\n   * Cancel all tasks for a model\n   */\n  cancelAllForModel(modelId: string): number {\n    const queue = this.queues.get(modelId);\n    if (!queue) return 0;\n\n    let cancelled = 0;\n    for (const task of queue.getAll()) {\n      if (task.status === 'pending') {\n        task.cancel();\n        cancelled++;\n      }\n    }\n    queue.clear();\n    \n    return cancelled;\n  }\n\n  /**\n   * Get statistics\n   */\n  getStats(): {\n    totalTasks: number;\n    pendingTasks: number;\n    runningTasks: number;\n    completedTasks: number;\n    failedTasks: number;\n    cancelledTasks: number;\n    queuedByModel: Record<string, number>;\n  } {\n    const stats = {\n      totalTasks: this.allTasks.size,\n      pendingTasks: 0,\n      runningTasks: 0,\n      completedTasks: 0,\n      failedTasks: 0,\n      cancelledTasks: 0,\n      queuedByModel: {} as Record<string, number>,\n    };\n\n    for (const task of this.allTasks.values()) {\n      switch (task.status) {\n        case 'pending':\n          stats.pendingTasks++;\n          break;\n        case 'running':\n          stats.runningTasks++;\n          break;\n        case 'completed':\n          stats.completedTasks++;\n          break;\n        case 'failed':\n          stats.failedTasks++;\n          break;\n        case 'cancelled':\n          stats.cancelledTasks++;\n          break;\n      }\n    }\n\n    for (const [modelId, queue] of this.queues) {\n      stats.queuedByModel[modelId] = queue.length;\n    }\n\n    return stats;\n  }\n\n  /**\n   * Add event listener\n   */\n  on<T = unknown>(event: EventType, listener: EventListener<T>): void {\n    let listeners = this.listeners.get(event);\n    if (!listeners) {\n      listeners = new Set();\n      this.listeners.set(event, listeners);\n    }\n    listeners.add(listener as EventListener);\n  }\n\n  /**\n   * Remove event listener\n   */\n  off<T = unknown>(event: EventType, listener: EventListener<T>): void {\n    const listeners = this.listeners.get(event);\n    if (listeners) {\n      listeners.delete(listener as EventListener);\n    }\n  }\n\n  /**\n   * Emit event\n   */\n  private emit<T>(type: EventType, data: T): void {\n    const event: EdgeFlowEvent<T> = {\n      type,\n      timestamp: Date.now(),\n      data,\n    };\n\n    const listeners = this.listeners.get(type);\n    if (listeners) {\n      for (const listener of listeners) {\n        try {\n          listener(event);\n        } catch (error) {\n          console.error('Error in event listener:', error);\n        }\n      }\n    }\n  }\n\n  /**\n   * Clear completed/failed/cancelled tasks from history\n   */\n  clearHistory(): void {\n    for (const [taskId, task] of this.allTasks) {\n      if (\n        task.status === 'completed' ||\n        task.status === 'failed' ||\n        task.status === 'cancelled'\n      ) {\n        this.allTasks.delete(taskId);\n      }\n    }\n  }\n\n  /**\n   * Dispose the scheduler\n   */\n  dispose(): void {\n    this.disposed = true;\n\n    // Cancel all pending tasks\n    for (const queue of this.queues.values()) {\n      for (const task of queue.getAll()) {\n        task.cancel();\n      }\n      queue.clear();\n    }\n\n    // Clear batchers\n    for (const batcher of this.batchers.values()) {\n      batcher.clear();\n    }\n\n    this.queues.clear();\n    this.runningTasks.clear();\n    this.allTasks.clear();\n    this.batchers.clear();\n    this.listeners.clear();\n  }\n}\n\n// ============================================================================\n// Global Scheduler Instance\n// ============================================================================\n\nlet globalScheduler: InferenceScheduler | null = null;\n\n/**\n * Get the global scheduler instance\n */\nexport function getScheduler(): InferenceScheduler {\n  if (!globalScheduler) {\n    globalScheduler = new InferenceScheduler();\n  }\n  return globalScheduler;\n}\n\n/**\n * Set the global scheduler instance\n */\nexport function setScheduler(scheduler: InferenceScheduler): void {\n  if (globalScheduler) {\n    globalScheduler.dispose();\n  }\n  globalScheduler = scheduler;\n}\n\n/**\n * Configure the global scheduler\n */\nexport function configureScheduler(options: SchedulerOptions): void {\n  setScheduler(new InferenceScheduler(options));\n}\n", "/**\n * edgeFlow.js - Memory Management\n * \n * Efficient memory management for tensors and models.\n * Features:\n * - Memory pooling\n * - Automatic garbage collection\n * - Memory tracking and statistics\n * - Leak detection\n */\n\nimport {\n  Tensor,\n  LoadedModel,\n  MemoryStats,\n  MemoryPoolConfig,\n  EventType,\n  EventListener,\n  EdgeFlowEvent,\n} from './types.js';\n\n// ============================================================================\n// Memory Tracking\n// ============================================================================\n\n/**\n * Tracked resource info\n */\ninterface TrackedResource {\n  id: string;\n  type: 'tensor' | 'model';\n  size: number;\n  createdAt: number;\n  stackTrace?: string;\n}\n\n/**\n * Default memory pool configuration\n */\nconst DEFAULT_POOL_CONFIG: Required<MemoryPoolConfig> = {\n  initialSize: 64 * 1024 * 1024, // 64MB\n  maxSize: 512 * 1024 * 1024, // 512MB\n  growthFactor: 1.5,\n  autoGC: true,\n  gcThreshold: 0.8, // 80%\n};\n\n// ============================================================================\n// Memory Manager\n// ============================================================================\n\n/**\n * MemoryManager - Central memory management\n * \n * Provides:\n * - Resource tracking\n * - Memory statistics\n * - Garbage collection coordination\n * - Memory warning events\n */\nexport class MemoryManager {\n  private static instance: MemoryManager | null = null;\n  \n  private readonly config: Required<MemoryPoolConfig>;\n  private readonly resources: Map<string, TrackedResource> = new Map();\n  private readonly disposers: Map<string, () => void> = new Map();\n  private readonly listeners: Map<EventType, Set<EventListener>> = new Map();\n  \n  private allocated = 0;\n  private peak = 0;\n  private gcScheduled = false;\n  private disposed = false;\n\n  private constructor(config: MemoryPoolConfig = {}) {\n    this.config = { ...DEFAULT_POOL_CONFIG, ...config };\n  }\n\n  /**\n   * Get singleton instance\n   */\n  static getInstance(): MemoryManager {\n    if (!MemoryManager.instance) {\n      MemoryManager.instance = new MemoryManager();\n    }\n    return MemoryManager.instance;\n  }\n\n  /**\n   * Configure the memory manager\n   */\n  static configure(config: MemoryPoolConfig): void {\n    if (MemoryManager.instance) {\n      console.warn('MemoryManager already initialized, configuration may not apply');\n    }\n    MemoryManager.instance = new MemoryManager(config);\n  }\n\n  /**\n   * Track a tensor\n   */\n  track(tensor: Tensor, disposer?: () => void): void {\n    if (this.disposed) return;\n\n    const size = this.estimateTensorSize(tensor);\n    \n    this.resources.set(tensor.id, {\n      id: tensor.id,\n      type: 'tensor',\n      size,\n      createdAt: Date.now(),\n      stackTrace: this.captureStackTrace(),\n    });\n\n    if (disposer) {\n      this.disposers.set(tensor.id, disposer);\n    }\n\n    this.allocated += size;\n    this.peak = Math.max(this.peak, this.allocated);\n\n    this.checkMemoryThreshold();\n  }\n\n  /**\n   * Track a model\n   */\n  trackModel(model: LoadedModel, disposer?: () => void): void {\n    if (this.disposed) return;\n\n    const size = model.metadata.sizeBytes;\n    \n    this.resources.set(model.id, {\n      id: model.id,\n      type: 'model',\n      size,\n      createdAt: Date.now(),\n      stackTrace: this.captureStackTrace(),\n    });\n\n    if (disposer) {\n      this.disposers.set(model.id, disposer);\n    }\n\n    this.allocated += size;\n    this.peak = Math.max(this.peak, this.allocated);\n\n    this.checkMemoryThreshold();\n  }\n\n  /**\n   * Untrack a resource\n   */\n  untrack(id: string): void {\n    const resource = this.resources.get(id);\n    if (resource) {\n      this.allocated -= resource.size;\n      this.resources.delete(id);\n      this.disposers.delete(id);\n    }\n  }\n\n  /**\n   * Release a resource\n   */\n  release(resourceOrId: Tensor | LoadedModel | string): void {\n    const id = typeof resourceOrId === 'string' ? resourceOrId : resourceOrId.id;\n    \n    const disposer = this.disposers.get(id);\n    if (disposer) {\n      try {\n        disposer();\n      } catch (error) {\n        console.error('Error disposing resource:', error);\n      }\n    }\n\n    this.untrack(id);\n  }\n\n  /**\n   * Estimate tensor memory size\n   */\n  private estimateTensorSize(tensor: Tensor): number {\n    const bytesPerElement = this.getBytesPerElement(tensor.dtype);\n    return tensor.size * bytesPerElement;\n  }\n\n  /**\n   * Get bytes per element for a data type\n   */\n  private getBytesPerElement(dtype: string): number {\n    switch (dtype) {\n      case 'float32':\n        return 4;\n      case 'float16':\n        return 2;\n      case 'int32':\n        return 4;\n      case 'int64':\n        return 8;\n      case 'uint8':\n      case 'int8':\n      case 'bool':\n        return 1;\n      default:\n        return 4;\n    }\n  }\n\n  /**\n   * Capture stack trace for debugging\n   */\n  private captureStackTrace(): string | undefined {\n    if (typeof Error.captureStackTrace === 'function') {\n      const obj: { stack?: string } = {};\n      Error.captureStackTrace(obj, this.captureStackTrace);\n      return obj.stack;\n    }\n    return new Error().stack;\n  }\n\n  /**\n   * Check if memory threshold is exceeded\n   */\n  private checkMemoryThreshold(): void {\n    if (!this.config.autoGC) return;\n\n    const usage = this.allocated / this.config.maxSize;\n    \n    if (usage >= this.config.gcThreshold && !this.gcScheduled) {\n      this.gcScheduled = true;\n      this.emit('memory:warning', {\n        allocated: this.allocated,\n        maxSize: this.config.maxSize,\n        usage,\n      });\n\n      // Schedule GC on next tick\n      setTimeout(() => {\n        this.gc();\n        this.gcScheduled = false;\n      }, 0);\n    }\n  }\n\n  /**\n   * Garbage collection helper\n   */\n  gc(): void {\n    this.emit('memory:gc', { before: this.allocated });\n\n    // In browser environment, we can only suggest GC\n    // by releasing unused resources\n    \n    // Find old resources that might be unused\n    const now = Date.now();\n    const oldResources: string[] = [];\n    \n    for (const [id, resource] of this.resources) {\n      // Resources older than 5 minutes might be candidates for cleanup\n      if (now - resource.createdAt > 5 * 60 * 1000) {\n        oldResources.push(id);\n      }\n    }\n\n    // Note: We don't automatically release old resources\n    // This is just for reporting purposes\n    // Actual cleanup should be done by the user\n\n    this.emit('memory:gc', { \n      after: this.allocated,\n      potentialCleanup: oldResources.length,\n    });\n  }\n\n  /**\n   * Get memory statistics\n   */\n  getStats(): MemoryStats {\n    let tensorCount = 0;\n    let modelCount = 0;\n\n    for (const resource of this.resources.values()) {\n      if (resource.type === 'tensor') {\n        tensorCount++;\n      } else {\n        modelCount++;\n      }\n    }\n\n    return {\n      allocated: this.allocated,\n      used: this.allocated, // In JS, allocated = used\n      peak: this.peak,\n      tensorCount,\n      modelCount,\n    };\n  }\n\n  /**\n   * Get detailed resource list (for debugging)\n   */\n  getResourceDetails(): TrackedResource[] {\n    return Array.from(this.resources.values());\n  }\n\n  /**\n   * Check for potential memory leaks\n   */\n  detectLeaks(maxAge: number = 10 * 60 * 1000): TrackedResource[] {\n    const now = Date.now();\n    const potentialLeaks: TrackedResource[] = [];\n\n    for (const resource of this.resources.values()) {\n      if (now - resource.createdAt > maxAge) {\n        potentialLeaks.push(resource);\n      }\n    }\n\n    return potentialLeaks;\n  }\n\n  /**\n   * Add event listener\n   */\n  on<T = unknown>(event: EventType, listener: EventListener<T>): void {\n    let listeners = this.listeners.get(event);\n    if (!listeners) {\n      listeners = new Set();\n      this.listeners.set(event, listeners);\n    }\n    listeners.add(listener as EventListener);\n  }\n\n  /**\n   * Remove event listener\n   */\n  off<T = unknown>(event: EventType, listener: EventListener<T>): void {\n    const listeners = this.listeners.get(event);\n    if (listeners) {\n      listeners.delete(listener as EventListener);\n    }\n  }\n\n  /**\n   * Emit event\n   */\n  private emit<T>(type: EventType, data: T): void {\n    const event: EdgeFlowEvent<T> = {\n      type,\n      timestamp: Date.now(),\n      data,\n    };\n\n    const listeners = this.listeners.get(type);\n    if (listeners) {\n      for (const listener of listeners) {\n        try {\n          listener(event);\n        } catch (error) {\n          console.error('Error in event listener:', error);\n        }\n      }\n    }\n  }\n\n  /**\n   * Reset statistics\n   */\n  resetStats(): void {\n    this.peak = this.allocated;\n  }\n\n  /**\n   * Dispose all resources\n   */\n  disposeAll(): void {\n    for (const id of this.resources.keys()) {\n      this.release(id);\n    }\n  }\n\n  /**\n   * Dispose the manager\n   */\n  dispose(): void {\n    this.disposeAll();\n    this.disposed = true;\n    this.listeners.clear();\n    MemoryManager.instance = null;\n  }\n}\n\n// ============================================================================\n// Memory Scope (RAII-like pattern)\n// ============================================================================\n\n/**\n * Memory scope for automatic resource cleanup\n * \n * Usage:\n * ```typescript\n * const result = await withMemoryScope(async (scope) => {\n *   const tensor1 = scope.track(createTensor(...));\n *   const tensor2 = scope.track(createTensor(...));\n *   // Process tensors\n *   return computeResult(tensor1, tensor2);\n * });\n * // tensor1 and tensor2 are automatically disposed\n * ```\n */\nexport class MemoryScope {\n  private resources: Array<{ dispose: () => void }> = [];\n  private children: MemoryScope[] = [];\n  private parent: MemoryScope | null = null;\n\n  constructor(parent?: MemoryScope) {\n    if (parent) {\n      this.parent = parent;\n      parent.children.push(this);\n    }\n  }\n\n  /**\n   * Track a resource in this scope\n   */\n  track<T extends { dispose: () => void }>(resource: T): T {\n    this.resources.push(resource);\n    return resource;\n  }\n\n  /**\n   * Create a child scope\n   */\n  createChild(): MemoryScope {\n    return new MemoryScope(this);\n  }\n\n  /**\n   * Keep a resource (don't dispose it when scope ends)\n   */\n  keep<T extends { dispose: () => void }>(resource: T): T {\n    const index = this.resources.indexOf(resource);\n    if (index !== -1) {\n      this.resources.splice(index, 1);\n    }\n    return resource;\n  }\n\n  /**\n   * Dispose all resources in this scope\n   */\n  dispose(): void {\n    // Dispose children first\n    for (const child of this.children) {\n      child.dispose();\n    }\n    this.children = [];\n\n    // Dispose resources in reverse order\n    for (let i = this.resources.length - 1; i >= 0; i--) {\n      try {\n        this.resources[i]?.dispose();\n      } catch (error) {\n        console.error('Error disposing resource in scope:', error);\n      }\n    }\n    this.resources = [];\n\n    // Remove from parent\n    if (this.parent) {\n      const index = this.parent.children.indexOf(this);\n      if (index !== -1) {\n        this.parent.children.splice(index, 1);\n      }\n      this.parent = null;\n    }\n  }\n}\n\n/**\n * Execute a function with automatic memory cleanup\n */\nexport async function withMemoryScope<T>(\n  fn: (scope: MemoryScope) => Promise<T>\n): Promise<T> {\n  const scope = new MemoryScope();\n  try {\n    return await fn(scope);\n  } finally {\n    scope.dispose();\n  }\n}\n\n/**\n * Synchronous version of withMemoryScope\n */\nexport function withMemoryScopeSync<T>(\n  fn: (scope: MemoryScope) => T\n): T {\n  const scope = new MemoryScope();\n  try {\n    return fn(scope);\n  } finally {\n    scope.dispose();\n  }\n}\n\n// ============================================================================\n// LRU Cache for Models\n// ============================================================================\n\n/**\n * LRU Cache for loaded models\n */\nexport class ModelCache {\n  private readonly maxSize: number;\n  private readonly maxModels: number;\n  private readonly cache: Map<string, { model: LoadedModel; size: number; lastAccess: number }> = new Map();\n  private currentSize = 0;\n\n  constructor(options: { maxSize?: number; maxModels?: number } = {}) {\n    this.maxSize = options.maxSize ?? 256 * 1024 * 1024; // 256MB default\n    this.maxModels = options.maxModels ?? 5;\n  }\n\n  /**\n   * Get a model from cache\n   */\n  get(key: string): LoadedModel | undefined {\n    const entry = this.cache.get(key);\n    if (entry) {\n      entry.lastAccess = Date.now();\n      return entry.model;\n    }\n    return undefined;\n  }\n\n  /**\n   * Add a model to cache\n   */\n  set(key: string, model: LoadedModel): void {\n    const size = model.metadata.sizeBytes;\n\n    // Check if we need to evict\n    while (\n      (this.currentSize + size > this.maxSize || this.cache.size >= this.maxModels) &&\n      this.cache.size > 0\n    ) {\n      this.evictLRU();\n    }\n\n    // Add to cache\n    this.cache.set(key, {\n      model,\n      size,\n      lastAccess: Date.now(),\n    });\n    this.currentSize += size;\n  }\n\n  /**\n   * Remove a model from cache\n   */\n  delete(key: string): boolean {\n    const entry = this.cache.get(key);\n    if (entry) {\n      entry.model.dispose();\n      this.currentSize -= entry.size;\n      this.cache.delete(key);\n      return true;\n    }\n    return false;\n  }\n\n  /**\n   * Check if model is in cache\n   */\n  has(key: string): boolean {\n    return this.cache.has(key);\n  }\n\n  /**\n   * Evict least recently used model\n   */\n  private evictLRU(): void {\n    let oldestKey: string | null = null;\n    let oldestTime = Infinity;\n\n    for (const [key, entry] of this.cache) {\n      if (entry.lastAccess < oldestTime) {\n        oldestTime = entry.lastAccess;\n        oldestKey = key;\n      }\n    }\n\n    if (oldestKey) {\n      this.delete(oldestKey);\n    }\n  }\n\n  /**\n   * Clear the cache\n   */\n  clear(): void {\n    for (const entry of this.cache.values()) {\n      entry.model.dispose();\n    }\n    this.cache.clear();\n    this.currentSize = 0;\n  }\n\n  /**\n   * Get cache statistics\n   */\n  getStats(): { size: number; count: number; maxSize: number; maxModels: number } {\n    return {\n      size: this.currentSize,\n      count: this.cache.size,\n      maxSize: this.maxSize,\n      maxModels: this.maxModels,\n    };\n  }\n}\n\n// ============================================================================\n// Convenience Functions\n// ============================================================================\n\n/**\n * Get memory manager instance\n */\nexport function getMemoryManager(): MemoryManager {\n  return MemoryManager.getInstance();\n}\n\n/**\n * Get memory statistics\n */\nexport function getMemoryStats(): MemoryStats {\n  return MemoryManager.getInstance().getStats();\n}\n\n/**\n * Release a resource\n */\nexport function release(resource: Tensor | LoadedModel): void {\n  MemoryManager.getInstance().release(resource);\n}\n\n/**\n * Force garbage collection hint\n */\nexport function gc(): void {\n  MemoryManager.getInstance().gc();\n}\n", "/**\n * edgeFlow.js - Runtime Management\n * \n * Manages runtime backends and automatic selection.\n * Provides unified interface for different compute backends.\n */\n\nimport {\n  Runtime,\n  RuntimeType,\n  RuntimeCapabilities,\n  LoadedModel,\n  ModelLoadOptions,\n  ModelMetadata,\n  Tensor,\n  EdgeFlowError,\n  ErrorCodes,\n  EventType,\n  EventListener,\n  EdgeFlowEvent,\n} from './types.js';\nimport { getScheduler } from './scheduler.js';\nimport { getMemoryManager } from './memory.js';\n\n// ============================================================================\n// Runtime Registry\n// ============================================================================\n\n/**\n * Registered runtime factories\n */\nconst runtimeFactories: Map<RuntimeType, () => Runtime> = new Map();\n\n/**\n * Cached runtime instances\n */\nconst runtimeInstances: Map<RuntimeType, Runtime> = new Map();\n\n/**\n * Runtime priority order (higher priority first)\n */\nconst RUNTIME_PRIORITY: RuntimeType[] = ['webgpu', 'webnn', 'wasm'];\n\n// ============================================================================\n// Runtime Manager\n// ============================================================================\n\n/**\n * RuntimeManager - Manages runtime selection and lifecycle\n * \n * Features:\n * - Automatic best runtime selection\n * - Runtime registration\n * - Capability detection\n * - Fallback handling\n */\nexport class RuntimeManager {\n  private static instance: RuntimeManager | null = null;\n  \n  private readonly listeners: Map<EventType, Set<EventListener>> = new Map();\n  private defaultRuntime: RuntimeType = 'auto';\n\n  private constructor() {}\n\n  /**\n   * Get singleton instance\n   */\n  static getInstance(): RuntimeManager {\n    if (!RuntimeManager.instance) {\n      RuntimeManager.instance = new RuntimeManager();\n    }\n    return RuntimeManager.instance;\n  }\n\n  /**\n   * Register a runtime factory\n   */\n  register(type: RuntimeType, factory: () => Runtime): void {\n    runtimeFactories.set(type, factory);\n  }\n\n  /**\n   * Get a runtime instance\n   */\n  async getRuntime(type: RuntimeType = 'auto'): Promise<Runtime> {\n    if (type === 'auto') {\n      return this.getBestRuntime();\n    }\n\n    // Check if already instantiated\n    let runtime = runtimeInstances.get(type);\n    if (runtime) {\n      return runtime;\n    }\n\n    // Create new instance\n    const factory = runtimeFactories.get(type);\n    if (!factory) {\n      throw new EdgeFlowError(\n        `Runtime '${type}' is not registered`,\n        ErrorCodes.RUNTIME_NOT_AVAILABLE,\n        { runtime: type }\n      );\n    }\n\n    runtime = factory();\n    \n    // Check availability\n    const available = await runtime.isAvailable();\n    if (!available) {\n      throw new EdgeFlowError(\n        `Runtime '${type}' is not available in this environment`,\n        ErrorCodes.RUNTIME_NOT_AVAILABLE,\n        { runtime: type }\n      );\n    }\n\n    // Initialize\n    try {\n      await runtime.initialize();\n    } catch (error) {\n      throw new EdgeFlowError(\n        `Failed to initialize runtime '${type}': ${error instanceof Error ? error.message : String(error)}`,\n        ErrorCodes.RUNTIME_INIT_FAILED,\n        { runtime: type, error }\n      );\n    }\n\n    runtimeInstances.set(type, runtime);\n    this.emit('runtime:ready', { runtime: type });\n\n    return runtime;\n  }\n\n  /**\n   * Get the best available runtime\n   */\n  async getBestRuntime(): Promise<Runtime> {\n    for (const type of RUNTIME_PRIORITY) {\n      try {\n        // Check if already available\n        const existing = runtimeInstances.get(type);\n        if (existing) {\n          return existing;\n        }\n\n        // Try to create and initialize\n        const factory = runtimeFactories.get(type);\n        if (!factory) continue;\n\n        const runtime = factory();\n        const available = await runtime.isAvailable();\n        \n        if (available) {\n          await runtime.initialize();\n          runtimeInstances.set(type, runtime);\n          this.emit('runtime:ready', { runtime: type });\n          return runtime;\n        }\n      } catch {\n        // Try next runtime\n        continue;\n      }\n    }\n\n    throw new EdgeFlowError(\n      'No runtime available. Please ensure WebGPU, WebNN, or WASM is supported.',\n      ErrorCodes.RUNTIME_NOT_AVAILABLE,\n      { triedRuntimes: RUNTIME_PRIORITY }\n    );\n  }\n\n  /**\n   * Check which runtimes are available\n   */\n  async detectAvailableRuntimes(): Promise<Map<RuntimeType, boolean>> {\n    const results = new Map<RuntimeType, boolean>();\n\n    for (const type of RUNTIME_PRIORITY) {\n      const factory = runtimeFactories.get(type);\n      if (!factory) {\n        results.set(type, false);\n        continue;\n      }\n\n      try {\n        const runtime = factory();\n        results.set(type, await runtime.isAvailable());\n      } catch {\n        results.set(type, false);\n      }\n    }\n\n    return results;\n  }\n\n  /**\n   * Get capabilities of a runtime\n   */\n  async getCapabilities(type: RuntimeType): Promise<RuntimeCapabilities> {\n    const runtime = await this.getRuntime(type);\n    return runtime.capabilities;\n  }\n\n  /**\n   * Set default runtime\n   */\n  setDefaultRuntime(type: RuntimeType): void {\n    this.defaultRuntime = type;\n  }\n\n  /**\n   * Get default runtime type\n   */\n  getDefaultRuntimeType(): RuntimeType {\n    return this.defaultRuntime;\n  }\n\n  /**\n   * Dispose a specific runtime\n   */\n  disposeRuntime(type: RuntimeType): void {\n    const runtime = runtimeInstances.get(type);\n    if (runtime) {\n      runtime.dispose();\n      runtimeInstances.delete(type);\n    }\n  }\n\n  /**\n   * Dispose all runtimes\n   */\n  disposeAll(): void {\n    for (const [type, runtime] of runtimeInstances) {\n      runtime.dispose();\n      runtimeInstances.delete(type);\n    }\n  }\n\n  /**\n   * Add event listener\n   */\n  on<T = unknown>(event: EventType, listener: EventListener<T>): void {\n    let listeners = this.listeners.get(event);\n    if (!listeners) {\n      listeners = new Set();\n      this.listeners.set(event, listeners);\n    }\n    listeners.add(listener as EventListener);\n  }\n\n  /**\n   * Remove event listener\n   */\n  off<T = unknown>(event: EventType, listener: EventListener<T>): void {\n    const listeners = this.listeners.get(event);\n    if (listeners) {\n      listeners.delete(listener as EventListener);\n    }\n  }\n\n  /**\n   * Emit event\n   */\n  private emit<T>(type: EventType, data: T): void {\n    const event: EdgeFlowEvent<T> = {\n      type,\n      timestamp: Date.now(),\n      data,\n    };\n\n    const listeners = this.listeners.get(type);\n    if (listeners) {\n      for (const listener of listeners) {\n        try {\n          listener(event);\n        } catch (error) {\n          console.error('Error in event listener:', error);\n        }\n      }\n    }\n  }\n}\n\n// ============================================================================\n// Model Loader\n// ============================================================================\n\n/**\n * Model instance counter\n */\nlet modelIdCounter = 0;\n\n/**\n * Generate unique model ID\n */\nfunction generateModelId(): string {\n  return `model_${++modelIdCounter}_${Date.now().toString(36)}`;\n}\n\n/**\n * LoadedModelImpl - Implementation of LoadedModel interface\n */\nexport class LoadedModelImpl implements LoadedModel {\n  readonly id: string;\n  readonly metadata: ModelMetadata;\n  readonly runtime: RuntimeType;\n  \n  private _isLoaded = true;\n  private readonly _dispose: () => void;\n\n  constructor(\n    metadata: ModelMetadata,\n    runtime: RuntimeType,\n    dispose: () => void\n  ) {\n    this.id = generateModelId();\n    this.metadata = metadata;\n    this.runtime = runtime;\n    this._dispose = dispose;\n  }\n\n  get isLoaded(): boolean {\n    return this._isLoaded;\n  }\n\n  dispose(): void {\n    if (this._isLoaded) {\n      this._isLoaded = false;\n      this._dispose();\n      getMemoryManager().untrack(this.id);\n    }\n  }\n}\n\n// ============================================================================\n// Model Loading Functions\n// ============================================================================\n\n/**\n * Load model from URL with advanced loading support\n * (caching, sharding, resume download)\n */\nexport async function loadModel(\n  url: string,\n  options: ModelLoadOptions & { \n    runtime?: RuntimeType;\n    cache?: boolean;\n    resumable?: boolean;\n    chunkSize?: number;\n    forceDownload?: boolean;\n  } = {}\n): Promise<LoadedModel> {\n  const manager = RuntimeManager.getInstance();\n  const runtime = await manager.getRuntime(options.runtime ?? 'auto');\n\n  // Import model loader dynamically to avoid circular dependencies\n  const { loadModelData } = await import('../utils/model-loader.js');\n\n  // Use advanced model loader with caching and resume support\n  const modelData = await loadModelData(url, {\n    cache: options.cache ?? true,\n    resumable: options.resumable ?? true,\n    chunkSize: options.chunkSize,\n    forceDownload: options.forceDownload,\n    onProgress: options.onProgress ? (progress) => {\n      options.onProgress!(progress.percent / 100);\n    } : undefined,\n  });\n\n  // Load into runtime\n  const model = await runtime.loadModel(modelData, options);\n\n  return model;\n}\n\n/**\n * Load model from ArrayBuffer\n */\nexport async function loadModelFromBuffer(\n  data: ArrayBuffer,\n  options: ModelLoadOptions & { runtime?: RuntimeType } = {}\n): Promise<LoadedModel> {\n  const manager = RuntimeManager.getInstance();\n  const runtime = await manager.getRuntime(options.runtime ?? 'auto');\n  return runtime.loadModel(data, options);\n}\n\n// ============================================================================\n// Inference Functions\n// ============================================================================\n\n/**\n * Run inference on a model\n */\nexport async function runInference(\n  model: LoadedModel,\n  inputs: Tensor[]\n): Promise<Tensor[]> {\n  if (!model.isLoaded) {\n    throw new EdgeFlowError(\n      'Model has been disposed',\n      ErrorCodes.MODEL_NOT_LOADED,\n      { modelId: model.id }\n    );\n  }\n\n  const manager = RuntimeManager.getInstance();\n  const runtime = await manager.getRuntime(model.runtime);\n  \n  // Use scheduler for execution\n  const scheduler = getScheduler();\n  const task = scheduler.schedule(model.id, () => runtime.run(model, inputs));\n  \n  return task.wait();\n}\n\n/**\n * Run inference with batch processing\n */\nexport async function runBatchInference(\n  model: LoadedModel,\n  batches: Tensor[][]\n): Promise<Tensor[][]> {\n  const scheduler = getScheduler();\n  const manager = RuntimeManager.getInstance();\n  const runtime = await manager.getRuntime(model.runtime);\n\n  // Schedule all batches\n  const tasks = batches.map(inputs =>\n    scheduler.schedule(model.id, () => runtime.run(model, inputs))\n  );\n\n  // Wait for all to complete\n  return Promise.all(tasks.map(task => task.wait()));\n}\n\n// ============================================================================\n// Convenience Functions\n// ============================================================================\n\n/**\n * Get runtime manager instance\n */\nexport function getRuntimeManager(): RuntimeManager {\n  return RuntimeManager.getInstance();\n}\n\n/**\n * Register a runtime\n */\nexport function registerRuntime(type: RuntimeType, factory: () => Runtime): void {\n  RuntimeManager.getInstance().register(type, factory);\n}\n\n/**\n * Get the best available runtime\n */\nexport async function getBestRuntime(): Promise<Runtime> {\n  return RuntimeManager.getInstance().getBestRuntime();\n}\n\n/**\n * Check available runtimes\n */\nexport async function getAvailableRuntimes(): Promise<Map<RuntimeType, boolean>> {\n  return RuntimeManager.getInstance().detectAvailableRuntimes();\n}\n", "/**\n * edgeFlow.js - WebGPU Backend\n * \n * High-performance WebGPU runtime for GPU-accelerated inference.\n * Features:\n * - Native concurrency support\n * - Efficient memory management\n * - Compute shader execution\n */\n\nimport {\n  Runtime,\n  RuntimeType,\n  RuntimeCapabilities,\n  LoadedModel,\n  ModelLoadOptions,\n  ModelMetadata,\n  Tensor,\n  EdgeFlowError,\n  ErrorCodes,\n} from '../core/types.js';\nimport { LoadedModelImpl } from '../core/runtime.js';\nimport { EdgeFlowTensor } from '../core/tensor.js';\nimport { getMemoryManager } from '../core/memory.js';\n\n// ============================================================================\n// WebGPU Type Declarations\n// ============================================================================\n\n// Declare WebGPU types for environments without @webgpu/types\ndeclare global {\n  interface Navigator {\n    gpu?: GPU;\n  }\n  \n  interface GPU {\n    requestAdapter(options?: GPURequestAdapterOptions): Promise<GPUAdapter | null>;\n  }\n  \n  interface GPURequestAdapterOptions {\n    powerPreference?: 'low-power' | 'high-performance';\n  }\n  \n  interface GPUAdapter {\n    requestDevice(descriptor?: GPUDeviceDescriptor): Promise<GPUDevice>;\n  }\n  \n  interface GPUDeviceDescriptor {\n    requiredFeatures?: string[];\n    requiredLimits?: Record<string, number>;\n  }\n  \n  interface GPUDevice {\n    limits: GPULimits;\n    lost: Promise<GPUDeviceLostInfo>;\n    createBuffer(descriptor: GPUBufferDescriptor): GPUBuffer;\n    createShaderModule(descriptor: GPUShaderModuleDescriptor): GPUShaderModule;\n    createBindGroupLayout(descriptor: GPUBindGroupLayoutDescriptor): GPUBindGroupLayout;\n    createPipelineLayout(descriptor: GPUPipelineLayoutDescriptor): GPUPipelineLayout;\n    createComputePipeline(descriptor: GPUComputePipelineDescriptor): GPUComputePipeline;\n    destroy(): void;\n  }\n  \n  interface GPULimits {\n    maxBufferSize: number;\n  }\n  \n  interface GPUDeviceLostInfo {\n    message: string;\n    reason: string;\n  }\n  \n  interface GPUBuffer {\n    destroy(): void;\n  }\n  \n  interface GPUShaderModule {}\n  interface GPUBindGroupLayout {}\n  interface GPUPipelineLayout {}\n  interface GPUComputePipeline {}\n  \n  interface GPUBufferDescriptor {\n    size: number;\n    usage: number;\n  }\n  \n  interface GPUShaderModuleDescriptor {\n    code: string;\n  }\n  \n  interface GPUBindGroupLayoutDescriptor {\n    entries: GPUBindGroupLayoutEntry[];\n  }\n  \n  interface GPUBindGroupLayoutEntry {\n    binding: number;\n    visibility: number;\n    buffer?: { type: string };\n  }\n  \n  interface GPUPipelineLayoutDescriptor {\n    bindGroupLayouts: GPUBindGroupLayout[];\n  }\n  \n  interface GPUComputePipelineDescriptor {\n    layout: GPUPipelineLayout;\n    compute: {\n      module: GPUShaderModule;\n      entryPoint: string;\n    };\n  }\n}\n\n// WebGPU constants\nconst GPUBufferUsage = {\n  STORAGE: 0x0080,\n  COPY_SRC: 0x0004,\n  COPY_DST: 0x0008,\n  MAP_READ: 0x0001,\n};\n\nconst GPUShaderStage = {\n  COMPUTE: 0x0004,\n};\n\n// ============================================================================\n// WebGPU Types\n// ============================================================================\n\n/**\n * WebGPU model data structure\n */\ninterface WebGPUModelData {\n  /** Shader modules */\n  shaders: Map<string, GPUShaderModule>;\n  /** Compute pipelines */\n  pipelines: Map<string, GPUComputePipeline>;\n  /** Weight buffers */\n  weights: Map<string, GPUBuffer>;\n  /** Bind group layouts */\n  bindGroupLayouts: GPUBindGroupLayout[];\n  /** Model configuration */\n  config: ModelConfig;\n}\n\n/**\n * Model configuration from model file\n */\ninterface ModelConfig {\n  name: string;\n  version: string;\n  layers: LayerConfig[];\n  inputs: { name: string; shape: number[]; dtype: string }[];\n  outputs: { name: string; shape: number[]; dtype: string }[];\n}\n\n/**\n * Layer configuration\n */\ninterface LayerConfig {\n  name: string;\n  type: string;\n  inputs: string[];\n  outputs: string[];\n  params: Record<string, unknown>;\n}\n\n// ============================================================================\n// WebGPU Runtime Implementation\n// ============================================================================\n\n/**\n * WebGPURuntime - GPU-accelerated inference runtime\n */\nexport class WebGPURuntime implements Runtime {\n  readonly name: RuntimeType = 'webgpu';\n  \n  private adapter: GPUAdapter | null = null;\n  private device: GPUDevice | null = null;\n  private models: Map<string, WebGPUModelData> = new Map();\n  private initialized = false;\n\n  get capabilities(): RuntimeCapabilities {\n    return {\n      concurrency: true,\n      quantization: true,\n      float16: true,\n      dynamicShapes: false,\n      maxBatchSize: 64,\n      availableMemory: this.device?.limits.maxBufferSize ?? 256 * 1024 * 1024,\n    };\n  }\n\n  /**\n   * Check if WebGPU is available\n   */\n  async isAvailable(): Promise<boolean> {\n    if (typeof navigator === 'undefined') return false;\n    if (!navigator.gpu) return false;\n\n    try {\n      const adapter = await navigator.gpu.requestAdapter();\n      return adapter !== null;\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Initialize the WebGPU runtime\n   */\n  async initialize(): Promise<void> {\n    if (this.initialized) return;\n\n    if (!navigator.gpu) {\n      throw new EdgeFlowError(\n        'WebGPU is not supported in this browser',\n        ErrorCodes.RUNTIME_NOT_AVAILABLE\n      );\n    }\n\n    // Request adapter\n    this.adapter = await navigator.gpu.requestAdapter({\n      powerPreference: 'high-performance',\n    });\n\n    if (!this.adapter) {\n      throw new EdgeFlowError(\n        'Failed to get WebGPU adapter',\n        ErrorCodes.RUNTIME_INIT_FAILED\n      );\n    }\n\n    // Request device\n    this.device = await this.adapter.requestDevice({\n      requiredFeatures: [],\n      requiredLimits: {},\n    });\n\n    // Handle device loss\n    this.device.lost.then((info: GPUDeviceLostInfo) => {\n      console.error('WebGPU device was lost:', info.message);\n      this.initialized = false;\n      this.device = null;\n    });\n\n    this.initialized = true;\n  }\n\n  /**\n   * Load a model\n   */\n  async loadModel(\n    modelData: ArrayBuffer,\n    options: ModelLoadOptions = {}\n  ): Promise<LoadedModel> {\n    this.ensureInitialized();\n\n    // Parse model data\n    const config = this.parseModelData(modelData);\n\n    // Create shader modules and pipelines\n    const webgpuData: WebGPUModelData = {\n      shaders: new Map(),\n      pipelines: new Map(),\n      weights: new Map(),\n      bindGroupLayouts: [],\n      config,\n    };\n\n    // Extract and upload weights\n    await this.uploadWeights(modelData, webgpuData);\n\n    // Create compute pipelines for each layer\n    await this.createPipelines(webgpuData);\n\n    // Generate model ID\n    const modelId = `webgpu_${Date.now().toString(36)}`;\n    this.models.set(modelId, webgpuData);\n\n    // Create metadata\n    const metadata: ModelMetadata = {\n      name: config.name || options.metadata?.name || 'unknown',\n      version: config.version,\n      inputs: config.inputs.map(i => ({\n        name: i.name,\n        dtype: i.dtype as 'float32',\n        shape: i.shape,\n      })),\n      outputs: config.outputs.map(o => ({\n        name: o.name,\n        dtype: o.dtype as 'float32',\n        shape: o.shape,\n      })),\n      sizeBytes: modelData.byteLength,\n      quantization: options.quantization ?? 'float32',\n      format: 'edgeflow',\n    };\n\n    // Create model instance\n    const model = new LoadedModelImpl(\n      metadata,\n      'webgpu',\n      () => this.unloadModel(modelId)\n    );\n\n    // Track in memory manager\n    getMemoryManager().trackModel(model, () => model.dispose());\n\n    return model;\n  }\n\n  /**\n   * Run inference\n   */\n  async run(model: LoadedModel, inputs: Tensor[]): Promise<Tensor[]> {\n    this.ensureInitialized();\n\n    // For now, use a simple fallback implementation\n    // In a full implementation, this would execute the compute pipelines\n    return this.executeModel(inputs, model.metadata);\n  }\n\n  /**\n   * Execute model (simplified implementation)\n   */\n  private async executeModel(inputs: Tensor[], metadata: ModelMetadata): Promise<Tensor[]> {\n    // This is a simplified implementation\n    // A full implementation would:\n    // 1. Upload input tensors to GPU buffers\n    // 2. Execute compute pipelines in topological order\n    // 3. Read back output tensors\n\n    const device = this.device!;\n    const outputs: Tensor[] = [];\n\n    for (const outputSpec of metadata.outputs) {\n      // Create output buffer\n      const outputSize = outputSpec.shape.reduce((a, b) => a * b, 1);\n      const outputBuffer = device.createBuffer({\n        size: outputSize * 4, // float32\n        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,\n      });\n\n      // Create staging buffer for readback\n      const stagingBuffer = device.createBuffer({\n        size: outputSize * 4,\n        usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,\n      });\n\n      // For now, return zeros (placeholder)\n      // In production, execute actual compute pipelines\n      const outputData = new Float32Array(outputSize);\n      \n      // Simulate some computation based on inputs\n      if (inputs.length > 0 && inputs[0]) {\n        const inputData = inputs[0].toFloat32Array();\n        for (let i = 0; i < Math.min(outputSize, inputData.length); i++) {\n          outputData[i] = (inputData[i] ?? 0);\n        }\n      }\n\n      outputs.push(new EdgeFlowTensor(outputData, outputSpec.shape, 'float32'));\n\n      // Cleanup\n      outputBuffer.destroy();\n      stagingBuffer.destroy();\n    }\n\n    return outputs;\n  }\n\n  /**\n   * Parse model data\n   */\n  private parseModelData(data: ArrayBuffer): ModelConfig {\n    // Try to parse as JSON first (for our custom format)\n    try {\n      const decoder = new TextDecoder();\n      const text = decoder.decode(new Uint8Array(data, 0, Math.min(1024, data.byteLength)));\n      \n      // Check if it starts with JSON\n      if (text.trim().startsWith('{')) {\n        // Find the JSON header end\n        let jsonEnd = text.indexOf('\\n---\\n');\n        if (jsonEnd === -1) jsonEnd = data.byteLength;\n        \n        const jsonStr = decoder.decode(new Uint8Array(data, 0, jsonEnd));\n        return JSON.parse(jsonStr) as ModelConfig;\n      }\n    } catch {\n      // Not JSON format\n    }\n\n    // Return default config for unknown formats\n    return {\n      name: 'unknown',\n      version: '1.0.0',\n      layers: [],\n      inputs: [{ name: 'input', shape: [-1, 768], dtype: 'float32' }],\n      outputs: [{ name: 'output', shape: [-1, 768], dtype: 'float32' }],\n    };\n  }\n\n  /**\n   * Upload weights to GPU\n   */\n  private async uploadWeights(\n    _data: ArrayBuffer,\n    modelData: WebGPUModelData\n  ): Promise<void> {\n    const device = this.device!;\n\n    // In a full implementation, parse weight data from the model file\n    // and upload to GPU buffers\n    \n    // Placeholder: create empty weight buffer\n    const weightsBuffer = device.createBuffer({\n      size: 1024,\n      usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,\n    });\n\n    modelData.weights.set('default', weightsBuffer);\n  }\n\n  /**\n   * Create compute pipelines\n   */\n  private async createPipelines(modelData: WebGPUModelData): Promise<void> {\n    const device = this.device!;\n\n    // Create a general-purpose compute shader\n    const shaderCode = /* wgsl */ `\n      @group(0) @binding(0) var<storage, read> input: array<f32>;\n      @group(0) @binding(1) var<storage, read_write> output: array<f32>;\n      \n      @compute @workgroup_size(64)\n      fn main(@builtin(global_invocation_id) gid: vec3<u32>) {\n        let idx = gid.x;\n        if (idx < arrayLength(&input)) {\n          output[idx] = input[idx];\n        }\n      }\n    `;\n\n    const shaderModule = device.createShaderModule({\n      code: shaderCode,\n    });\n\n    modelData.shaders.set('default', shaderModule);\n\n    // Create bind group layout\n    const bindGroupLayout = device.createBindGroupLayout({\n      entries: [\n        {\n          binding: 0,\n          visibility: GPUShaderStage.COMPUTE,\n          buffer: { type: 'read-only-storage' },\n        },\n        {\n          binding: 1,\n          visibility: GPUShaderStage.COMPUTE,\n          buffer: { type: 'storage' },\n        },\n      ],\n    });\n\n    modelData.bindGroupLayouts.push(bindGroupLayout);\n\n    // Create pipeline layout\n    const pipelineLayout = device.createPipelineLayout({\n      bindGroupLayouts: [bindGroupLayout],\n    });\n\n    // Create compute pipeline\n    const pipeline = device.createComputePipeline({\n      layout: pipelineLayout,\n      compute: {\n        module: shaderModule,\n        entryPoint: 'main',\n      },\n    });\n\n    modelData.pipelines.set('default', pipeline);\n  }\n\n  /**\n   * Unload a model\n   */\n  private unloadModel(modelId: string): void {\n    const modelData = this.models.get(modelId);\n    if (modelData) {\n      // Destroy GPU buffers\n      for (const buffer of modelData.weights.values()) {\n        buffer.destroy();\n      }\n      this.models.delete(modelId);\n    }\n  }\n\n  /**\n   * Ensure runtime is initialized\n   */\n  private ensureInitialized(): void {\n    if (!this.initialized || !this.device) {\n      throw new EdgeFlowError(\n        'WebGPU runtime is not initialized',\n        ErrorCodes.RUNTIME_NOT_INITIALIZED\n      );\n    }\n  }\n\n  /**\n   * Dispose the runtime\n   */\n  dispose(): void {\n    // Unload all models\n    for (const modelId of this.models.keys()) {\n      this.unloadModel(modelId);\n    }\n\n    // Destroy device\n    if (this.device) {\n      this.device.destroy();\n      this.device = null;\n    }\n\n    this.adapter = null;\n    this.initialized = false;\n  }\n}\n\n/**\n * Create WebGPU runtime factory\n */\nexport function createWebGPURuntime(): Runtime {\n  return new WebGPURuntime();\n}\n", "/**\n * edgeFlow.js - WebNN Backend\n * \n * Browser-native neural network acceleration using the Web Neural Network API.\n * Features:\n * - Hardware-accelerated inference\n * - Native browser integration\n * - Fallback to CPU when GPU unavailable\n */\n\nimport {\n  Runtime,\n  RuntimeType,\n  RuntimeCapabilities,\n  LoadedModel,\n  ModelLoadOptions,\n  ModelMetadata,\n  Tensor,\n  EdgeFlowError,\n  ErrorCodes,\n} from '../core/types.js';\nimport { LoadedModelImpl } from '../core/runtime.js';\nimport { EdgeFlowTensor } from '../core/tensor.js';\nimport { getMemoryManager } from '../core/memory.js';\n\n// ============================================================================\n// WebNN Type Definitions (since WebNN types may not be globally available)\n// ============================================================================\n\n/**\n * WebNN context type\n */\ntype MLContextType = 'default' | 'gpu' | 'cpu' | 'npu';\n\n/**\n * WebNN operand descriptor\n */\ninterface MLOperandDescriptor {\n  dataType: 'float32' | 'float16' | 'int32' | 'uint32' | 'int8' | 'uint8';\n  dimensions: number[];\n}\n\n/**\n * WebNN context options\n */\ninterface MLContextOptions {\n  deviceType?: MLContextType;\n  powerPreference?: 'default' | 'high-performance' | 'low-power';\n}\n\n// Extend Navigator for WebNN\ndeclare global {\n  interface Navigator {\n    ml?: {\n      createContext(options?: MLContextOptions): Promise<MLContext>;\n    };\n  }\n  \n  interface MLContext {\n    compute(\n      graph: MLGraph,\n      inputs: Record<string, ArrayBufferView>,\n      outputs: Record<string, ArrayBufferView>\n    ): Promise<Record<string, ArrayBufferView>>;\n  }\n  \n  interface MLGraph {\n    // Graph interface\n  }\n  \n  interface MLGraphBuilder {\n    input(name: string, desc: MLOperandDescriptor): MLOperand;\n    constant(desc: MLOperandDescriptor, data: ArrayBufferView): MLOperand;\n    build(outputs: Record<string, MLOperand>): Promise<MLGraph>;\n    \n    // Operations\n    add(a: MLOperand, b: MLOperand): MLOperand;\n    sub(a: MLOperand, b: MLOperand): MLOperand;\n    mul(a: MLOperand, b: MLOperand): MLOperand;\n    div(a: MLOperand, b: MLOperand): MLOperand;\n    matmul(a: MLOperand, b: MLOperand): MLOperand;\n    relu(x: MLOperand): MLOperand;\n    sigmoid(x: MLOperand): MLOperand;\n    tanh(x: MLOperand): MLOperand;\n    softmax(x: MLOperand): MLOperand;\n    reshape(x: MLOperand, newShape: number[]): MLOperand;\n    transpose(x: MLOperand, permutation?: number[]): MLOperand;\n  }\n  \n  interface MLOperand {\n    // Operand interface\n  }\n}\n\n// ============================================================================\n// WebNN Model Data\n// ============================================================================\n\n/**\n * WebNN model data structure\n */\ninterface WebNNModelData {\n  /** Compiled graph */\n  graph: MLGraph;\n  /** Graph builder (for potential graph modifications) */\n  builder: MLGraphBuilder;\n  /** Input names and shapes */\n  inputNames: string[];\n  /** Output names and shapes */\n  outputNames: string[];\n  /** Model configuration */\n  config: WebNNModelConfig;\n}\n\n/**\n * Model configuration\n */\ninterface WebNNModelConfig {\n  name: string;\n  version: string;\n  inputs: { name: string; shape: number[]; dtype: string }[];\n  outputs: { name: string; shape: number[]; dtype: string }[];\n}\n\n// ============================================================================\n// WebNN Runtime Implementation\n// ============================================================================\n\n/**\n * WebNNRuntime - Browser-native neural network runtime\n */\nexport class WebNNRuntime implements Runtime {\n  readonly name: RuntimeType = 'webnn';\n  \n  private context: MLContext | null = null;\n  private models: Map<string, WebNNModelData> = new Map();\n  private initialized = false;\n  private deviceType: MLContextType = 'default';\n\n  get capabilities(): RuntimeCapabilities {\n    return {\n      concurrency: true,\n      quantization: true,\n      float16: true,\n      dynamicShapes: false,\n      maxBatchSize: 32,\n      availableMemory: 256 * 1024 * 1024, // Estimated\n    };\n  }\n\n  /**\n   * Check if WebNN is available\n   */\n  async isAvailable(): Promise<boolean> {\n    if (typeof navigator === 'undefined') return false;\n    if (!navigator.ml) return false;\n\n    try {\n      const context = await navigator.ml.createContext({ deviceType: 'default' });\n      return context !== null;\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Initialize the WebNN runtime\n   */\n  async initialize(): Promise<void> {\n    if (this.initialized) return;\n\n    if (!navigator.ml) {\n      throw new EdgeFlowError(\n        'WebNN is not supported in this browser',\n        ErrorCodes.RUNTIME_NOT_AVAILABLE\n      );\n    }\n\n    // Try to get GPU context first, fallback to CPU\n    try {\n      this.context = await navigator.ml.createContext({ \n        deviceType: 'gpu',\n        powerPreference: 'high-performance',\n      });\n      this.deviceType = 'gpu';\n    } catch {\n      try {\n        this.context = await navigator.ml.createContext({ deviceType: 'cpu' });\n        this.deviceType = 'cpu';\n      } catch (error) {\n        throw new EdgeFlowError(\n          `Failed to create WebNN context: ${error instanceof Error ? error.message : String(error)}`,\n          ErrorCodes.RUNTIME_INIT_FAILED\n        );\n      }\n    }\n\n    this.initialized = true;\n  }\n\n  /**\n   * Load a model\n   */\n  async loadModel(\n    modelData: ArrayBuffer,\n    options: ModelLoadOptions = {}\n  ): Promise<LoadedModel> {\n    this.ensureInitialized();\n\n    // Parse model configuration\n    const config = this.parseModelConfig(modelData);\n\n    // Note: Full WebNN implementation would build the graph here\n    // This is a placeholder that creates minimal metadata\n    \n    const modelId = `webnn_${Date.now().toString(36)}`;\n\n    // Create metadata\n    const metadata: ModelMetadata = {\n      name: config.name || options.metadata?.name || 'unknown',\n      version: config.version || '1.0.0',\n      inputs: config.inputs.map(i => ({\n        name: i.name,\n        dtype: i.dtype as 'float32',\n        shape: i.shape,\n      })),\n      outputs: config.outputs.map(o => ({\n        name: o.name,\n        dtype: o.dtype as 'float32',\n        shape: o.shape,\n      })),\n      sizeBytes: modelData.byteLength,\n      quantization: options.quantization ?? 'float32',\n      format: 'edgeflow',\n    };\n\n    // Create model instance\n    const model = new LoadedModelImpl(\n      metadata,\n      'webnn',\n      () => this.unloadModel(modelId)\n    );\n\n    // Track in memory manager\n    getMemoryManager().trackModel(model, () => model.dispose());\n\n    return model;\n  }\n\n  /**\n   * Run inference\n   */\n  async run(model: LoadedModel, inputs: Tensor[]): Promise<Tensor[]> {\n    this.ensureInitialized();\n\n    // Simplified implementation - in production, would use compiled graph\n    return this.executeModel(inputs, model.metadata);\n  }\n\n  /**\n   * Execute model (simplified implementation)\n   */\n  private async executeModel(inputs: Tensor[], metadata: ModelMetadata): Promise<Tensor[]> {\n    const outputs: Tensor[] = [];\n\n    // For each expected output\n    for (const outputSpec of metadata.outputs) {\n      const outputSize = outputSpec.shape.reduce((a, b) => a * b, 1);\n      const outputData = new Float32Array(outputSize);\n\n      // Simple passthrough for demo (real impl would use WebNN compute)\n      if (inputs.length > 0 && inputs[0]) {\n        const inputData = inputs[0].toFloat32Array();\n        for (let i = 0; i < Math.min(outputSize, inputData.length); i++) {\n          outputData[i] = inputData[i] ?? 0;\n        }\n      }\n\n      outputs.push(new EdgeFlowTensor(outputData, outputSpec.shape, 'float32'));\n    }\n\n    return outputs;\n  }\n\n  /**\n   * Parse model configuration\n   */\n  private parseModelConfig(data: ArrayBuffer): WebNNModelConfig {\n    try {\n      const decoder = new TextDecoder();\n      const text = decoder.decode(new Uint8Array(data, 0, Math.min(1024, data.byteLength)));\n      \n      if (text.trim().startsWith('{')) {\n        let jsonEnd = text.indexOf('\\n---\\n');\n        if (jsonEnd === -1) jsonEnd = data.byteLength;\n        \n        const jsonStr = decoder.decode(new Uint8Array(data, 0, jsonEnd));\n        return JSON.parse(jsonStr) as WebNNModelConfig;\n      }\n    } catch {\n      // Not JSON format\n    }\n\n    return {\n      name: 'unknown',\n      version: '1.0.0',\n      inputs: [{ name: 'input', shape: [-1, 768], dtype: 'float32' }],\n      outputs: [{ name: 'output', shape: [-1, 768], dtype: 'float32' }],\n    };\n  }\n\n  /**\n   * Unload a model\n   */\n  private unloadModel(modelId: string): void {\n    this.models.delete(modelId);\n  }\n\n  /**\n   * Ensure runtime is initialized\n   */\n  private ensureInitialized(): void {\n    if (!this.initialized || !this.context) {\n      throw new EdgeFlowError(\n        'WebNN runtime is not initialized',\n        ErrorCodes.RUNTIME_NOT_INITIALIZED\n      );\n    }\n  }\n\n  /**\n   * Get device type\n   */\n  getDeviceType(): MLContextType {\n    return this.deviceType;\n  }\n\n  /**\n   * Dispose the runtime\n   */\n  dispose(): void {\n    this.models.clear();\n    this.context = null;\n    this.initialized = false;\n  }\n}\n\n/**\n * Create WebNN runtime factory\n */\nexport function createWebNNRuntime(): Runtime {\n  return new WebNNRuntime();\n}\n", "/**\n * edgeFlow.js - WebAssembly Backend\n * \n * Pure WASM runtime for universal browser support.\n * Features:\n * - Universal compatibility\n * - SIMD acceleration when available\n * - Memory-efficient execution\n */\n\nimport {\n  Runtime,\n  RuntimeType,\n  RuntimeCapabilities,\n  LoadedModel,\n  ModelLoadOptions,\n  ModelMetadata,\n  Tensor,\n  EdgeFlowError,\n  ErrorCodes,\n} from '../core/types.js';\nimport { LoadedModelImpl } from '../core/runtime.js';\nimport { EdgeFlowTensor, softmax as tensorSoftmax, relu as tensorRelu, sigmoid as tensorSigmoid } from '../core/tensor.js';\nimport { getMemoryManager } from '../core/memory.js';\n\n// ============================================================================\n// WASM Types\n// ============================================================================\n\n/**\n * WASM module instance\n */\ninterface WASMModule {\n  memory: WebAssembly.Memory;\n  exports: WASMExports;\n}\n\n/**\n * WASM exported functions\n */\ninterface WASMExports {\n  // Memory management\n  malloc(size: number): number;\n  free(ptr: number): void;\n  \n  // Tensor operations\n  matmul_f32(\n    a: number, aRows: number, aCols: number,\n    b: number, bRows: number, bCols: number,\n    out: number\n  ): void;\n  \n  add_f32(a: number, b: number, out: number, size: number): void;\n  mul_f32(a: number, b: number, out: number, size: number): void;\n  relu_f32(input: number, output: number, size: number): void;\n  sigmoid_f32(input: number, output: number, size: number): void;\n  softmax_f32(input: number, output: number, size: number): void;\n  \n  // SIMD variants (when available)\n  matmul_f32_simd?(\n    a: number, aRows: number, aCols: number,\n    b: number, bRows: number, bCols: number,\n    out: number\n  ): void;\n}\n\n/**\n * WASM model data structure\n */\ninterface WASMModelData {\n  /** Weight buffers */\n  weights: Map<string, { ptr: number; size: number; data: Float32Array }>;\n  /** Model configuration */\n  config: WASMModelConfig;\n  /** Layer execution order */\n  executionOrder: string[];\n}\n\n/**\n * Model configuration\n */\ninterface WASMModelConfig {\n  name: string;\n  version: string;\n  layers: WASMLayerConfig[];\n  inputs: { name: string; shape: number[]; dtype: string }[];\n  outputs: { name: string; shape: number[]; dtype: string }[];\n}\n\n/**\n * Layer configuration\n */\ninterface WASMLayerConfig {\n  name: string;\n  type: string;\n  inputShape: number[];\n  outputShape: number[];\n  weights?: string[];\n  params?: Record<string, unknown>;\n}\n\n// ============================================================================\n// WASM Runtime Implementation\n// ============================================================================\n\n/**\n * WASMRuntime - Pure WebAssembly inference runtime\n */\nexport class WASMRuntime implements Runtime {\n  readonly name: RuntimeType = 'wasm';\n  \n  private module: WASMModule | null = null;\n  private simdSupported = false;\n  private models: Map<string, WASMModelData> = new Map();\n  private initialized = false;\n\n  get capabilities(): RuntimeCapabilities {\n    return {\n      concurrency: false, // WASM is single-threaded by default\n      quantization: true,\n      float16: false,\n      dynamicShapes: true,\n      maxBatchSize: 16,\n      availableMemory: 128 * 1024 * 1024, // 128MB default\n    };\n  }\n\n  /**\n   * Check if WASM is available\n   */\n  async isAvailable(): Promise<boolean> {\n    if (typeof WebAssembly === 'undefined') return false;\n\n    try {\n      // Check if we can instantiate a minimal WASM module\n      const bytes = new Uint8Array([\n        0x00, 0x61, 0x73, 0x6d, // Magic number\n        0x01, 0x00, 0x00, 0x00, // Version\n      ]);\n      await WebAssembly.instantiate(bytes);\n      return true;\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Initialize the WASM runtime\n   */\n  async initialize(): Promise<void> {\n    if (this.initialized) return;\n\n    // Check SIMD support\n    this.simdSupported = await this.checkSIMDSupport();\n\n    // Create memory pool\n    const memory = new WebAssembly.Memory({\n      initial: 256,  // 16MB initial\n      maximum: 2048, // 128MB maximum\n    });\n\n    // Compile and instantiate the WASM module\n    // In production, this would load an actual WASM binary\n    // For now, we use a pure JS fallback\n    this.module = {\n      memory,\n      exports: this.createJSFallback(memory),\n    };\n\n    this.initialized = true;\n  }\n\n  /**\n   * Check SIMD support\n   */\n  private async checkSIMDSupport(): Promise<boolean> {\n    try {\n      // SIMD detection via feature detection\n      const simdTest = new Uint8Array([\n        0x00, 0x61, 0x73, 0x6d, 0x01, 0x00, 0x00, 0x00,\n        0x01, 0x05, 0x01, 0x60, 0x00, 0x01, 0x7b, 0x03,\n        0x02, 0x01, 0x00, 0x0a, 0x0a, 0x01, 0x08, 0x00,\n        0xfd, 0x0c, 0x00, 0x00, 0x00, 0x00, 0x0b\n      ]);\n      await WebAssembly.instantiate(simdTest);\n      return true;\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Create JavaScript fallback for WASM operations\n   */\n  private createJSFallback(memory: WebAssembly.Memory): WASMExports {\n    let nextPtr = 0;\n    const allocations: Map<number, number> = new Map();\n\n    return {\n      malloc: (size: number): number => {\n        const ptr = nextPtr;\n        nextPtr += size;\n        allocations.set(ptr, size);\n        return ptr;\n      },\n\n      free: (ptr: number): void => {\n        allocations.delete(ptr);\n      },\n\n      matmul_f32: (\n        aPtr: number, aRows: number, aCols: number,\n        bPtr: number, _bRows: number, bCols: number,\n        outPtr: number\n      ): void => {\n        const view = new Float32Array(memory.buffer);\n        const aOffset = aPtr / 4;\n        const bOffset = bPtr / 4;\n        const outOffset = outPtr / 4;\n\n        for (let i = 0; i < aRows; i++) {\n          for (let j = 0; j < bCols; j++) {\n            let sum = 0;\n            for (let k = 0; k < aCols; k++) {\n              sum += (view[aOffset + i * aCols + k] ?? 0) * (view[bOffset + k * bCols + j] ?? 0);\n            }\n            view[outOffset + i * bCols + j] = sum;\n          }\n        }\n      },\n\n      add_f32: (aPtr: number, bPtr: number, outPtr: number, size: number): void => {\n        const view = new Float32Array(memory.buffer);\n        const aOffset = aPtr / 4;\n        const bOffset = bPtr / 4;\n        const outOffset = outPtr / 4;\n\n        for (let i = 0; i < size; i++) {\n          view[outOffset + i] = (view[aOffset + i] ?? 0) + (view[bOffset + i] ?? 0);\n        }\n      },\n\n      mul_f32: (aPtr: number, bPtr: number, outPtr: number, size: number): void => {\n        const view = new Float32Array(memory.buffer);\n        const aOffset = aPtr / 4;\n        const bOffset = bPtr / 4;\n        const outOffset = outPtr / 4;\n\n        for (let i = 0; i < size; i++) {\n          view[outOffset + i] = (view[aOffset + i] ?? 0) * (view[bOffset + i] ?? 0);\n        }\n      },\n\n      relu_f32: (inputPtr: number, outputPtr: number, size: number): void => {\n        const view = new Float32Array(memory.buffer);\n        const inOffset = inputPtr / 4;\n        const outOffset = outputPtr / 4;\n\n        for (let i = 0; i < size; i++) {\n          view[outOffset + i] = Math.max(0, view[inOffset + i] ?? 0);\n        }\n      },\n\n      sigmoid_f32: (inputPtr: number, outputPtr: number, size: number): void => {\n        const view = new Float32Array(memory.buffer);\n        const inOffset = inputPtr / 4;\n        const outOffset = outputPtr / 4;\n\n        for (let i = 0; i < size; i++) {\n          view[outOffset + i] = 1 / (1 + Math.exp(-(view[inOffset + i] ?? 0)));\n        }\n      },\n\n      softmax_f32: (inputPtr: number, outputPtr: number, size: number): void => {\n        const view = new Float32Array(memory.buffer);\n        const inOffset = inputPtr / 4;\n        const outOffset = outputPtr / 4;\n\n        // Find max for numerical stability\n        let max = -Infinity;\n        for (let i = 0; i < size; i++) {\n          if ((view[inOffset + i] ?? 0) > max) max = view[inOffset + i] ?? 0;\n        }\n\n        // Compute exp and sum\n        let sum = 0;\n        for (let i = 0; i < size; i++) {\n          view[outOffset + i] = Math.exp((view[inOffset + i] ?? 0) - max);\n          sum += view[outOffset + i] ?? 0;\n        }\n\n        // Normalize\n        for (let i = 0; i < size; i++) {\n          view[outOffset + i] = (view[outOffset + i] ?? 0) / sum;\n        }\n      },\n    };\n  }\n\n  /**\n   * Load a model\n   */\n  async loadModel(\n    modelData: ArrayBuffer,\n    options: ModelLoadOptions = {}\n  ): Promise<LoadedModel> {\n    this.ensureInitialized();\n\n    // Parse model configuration\n    const config = this.parseModelConfig(modelData);\n\n    // Extract and store weights\n    const wasmData: WASMModelData = {\n      weights: new Map(),\n      config,\n      executionOrder: config.layers.map(l => l.name),\n    };\n\n    // Load weights into memory\n    await this.loadWeights(modelData, wasmData);\n\n    const modelId = `wasm_${Date.now().toString(36)}`;\n    this.models.set(modelId, wasmData);\n\n    // Create metadata\n    const metadata: ModelMetadata = {\n      name: config.name || options.metadata?.name || 'unknown',\n      version: config.version || '1.0.0',\n      inputs: config.inputs.map(i => ({\n        name: i.name,\n        dtype: i.dtype as 'float32',\n        shape: i.shape,\n      })),\n      outputs: config.outputs.map(o => ({\n        name: o.name,\n        dtype: o.dtype as 'float32',\n        shape: o.shape,\n      })),\n      sizeBytes: modelData.byteLength,\n      quantization: options.quantization ?? 'float32',\n      format: 'edgeflow',\n    };\n\n    // Create model instance\n    const model = new LoadedModelImpl(\n      metadata,\n      'wasm',\n      () => this.unloadModel(modelId)\n    );\n\n    // Track in memory manager\n    getMemoryManager().trackModel(model, () => model.dispose());\n\n    return model;\n  }\n\n  /**\n   * Run inference\n   */\n  async run(model: LoadedModel, inputs: Tensor[]): Promise<Tensor[]> {\n    this.ensureInitialized();\n    \n    // Execute model layers\n    return this.executeModel(inputs, model.metadata);\n  }\n\n  /**\n   * Execute model\n   */\n  private async executeModel(inputs: Tensor[], metadata: ModelMetadata): Promise<Tensor[]> {\n    const outputs: Tensor[] = [];\n\n    for (const outputSpec of metadata.outputs) {\n      const outputSize = outputSpec.shape.reduce((a, b) => a * b, 1);\n      \n      // Process based on output requirements\n      // This is a simplified implementation\n      let outputTensor: EdgeFlowTensor;\n\n      if (inputs.length > 0 && inputs[0]) {\n        const inputTensor = inputs[0] as EdgeFlowTensor;\n        \n        // Apply transformations based on layer types\n        // For demo, apply softmax to classification outputs\n        if (outputSpec.name.includes('logits') || outputSpec.name.includes('class')) {\n          outputTensor = tensorSoftmax(inputTensor) as EdgeFlowTensor;\n        } else if (outputSpec.name.includes('relu')) {\n          outputTensor = tensorRelu(inputTensor);\n        } else if (outputSpec.name.includes('sigmoid')) {\n          outputTensor = tensorSigmoid(inputTensor);\n        } else {\n          // Identity or feature extraction\n          const outputData = new Float32Array(outputSize);\n          const inputData = inputTensor.toFloat32Array();\n          for (let i = 0; i < Math.min(outputSize, inputData.length); i++) {\n            outputData[i] = inputData[i] ?? 0;\n          }\n          outputTensor = new EdgeFlowTensor(outputData, outputSpec.shape, 'float32');\n        }\n      } else {\n        outputTensor = new EdgeFlowTensor(new Float32Array(outputSize), outputSpec.shape, 'float32');\n      }\n\n      outputs.push(outputTensor);\n    }\n\n    return outputs;\n  }\n\n  /**\n   * Parse model configuration\n   */\n  private parseModelConfig(data: ArrayBuffer): WASMModelConfig {\n    try {\n      const decoder = new TextDecoder();\n      const text = decoder.decode(new Uint8Array(data, 0, Math.min(2048, data.byteLength)));\n      \n      if (text.trim().startsWith('{')) {\n        let jsonEnd = text.indexOf('\\n---\\n');\n        if (jsonEnd === -1) {\n          // Try to parse as pure JSON\n          try {\n            return JSON.parse(text) as WASMModelConfig;\n          } catch {\n            jsonEnd = data.byteLength;\n          }\n        }\n        \n        const jsonStr = decoder.decode(new Uint8Array(data, 0, jsonEnd));\n        return JSON.parse(jsonStr) as WASMModelConfig;\n      }\n    } catch {\n      // Not JSON format\n    }\n\n    return {\n      name: 'unknown',\n      version: '1.0.0',\n      layers: [],\n      inputs: [{ name: 'input', shape: [-1, 768], dtype: 'float32' }],\n      outputs: [{ name: 'output', shape: [-1, 768], dtype: 'float32' }],\n    };\n  }\n\n  /**\n   * Load weights into WASM memory\n   */\n  private async loadWeights(\n    _modelData: ArrayBuffer,\n    _wasmData: WASMModelData\n  ): Promise<void> {\n    // In a full implementation, extract and load weights\n    // This is a placeholder\n  }\n\n  /**\n   * Unload a model\n   */\n  private unloadModel(modelId: string): void {\n    const modelData = this.models.get(modelId);\n    if (modelData && this.module) {\n      // Free weight buffers\n      for (const weight of modelData.weights.values()) {\n        this.module.exports.free(weight.ptr);\n      }\n    }\n    this.models.delete(modelId);\n  }\n\n  /**\n   * Ensure runtime is initialized\n   */\n  private ensureInitialized(): void {\n    if (!this.initialized || !this.module) {\n      throw new EdgeFlowError(\n        'WASM runtime is not initialized',\n        ErrorCodes.RUNTIME_NOT_INITIALIZED\n      );\n    }\n  }\n\n  /**\n   * Check if SIMD is supported\n   */\n  hasSIMDSupport(): boolean {\n    return this.simdSupported;\n  }\n\n  /**\n   * Dispose the runtime\n   */\n  dispose(): void {\n    // Free all model weights\n    for (const modelId of this.models.keys()) {\n      this.unloadModel(modelId);\n    }\n\n    this.module = null;\n    this.initialized = false;\n  }\n}\n\n/**\n * Create WASM runtime factory\n */\nexport function createWASMRuntime(): Runtime {\n  return new WASMRuntime();\n}\n", "/**\n * edgeFlow.js - ONNX Runtime Backend\n * \n * Uses onnxruntime-web for real ONNX model inference.\n * Automatically loads ONNX Runtime from CDN when needed.\n */\n\nimport {\n  Runtime,\n  RuntimeType,\n  RuntimeCapabilities,\n  LoadedModel,\n  ModelLoadOptions,\n  ModelMetadata,\n  Tensor,\n  EdgeFlowError,\n  ErrorCodes,\n  DataType,\n} from '../core/types.js';\nimport { LoadedModelImpl } from '../core/runtime.js';\nimport { EdgeFlowTensor } from '../core/tensor.js';\nimport { getMemoryManager } from '../core/memory.js';\n\n// ONNX Runtime CDN configuration\nconst ONNX_VERSION = '1.17.0';\nconst ONNX_CDN_BASE = `https://cdn.jsdelivr.net/npm/onnxruntime-web@${ONNX_VERSION}/dist/`;\nconst ONNX_SCRIPT_URL = `${ONNX_CDN_BASE}ort.min.js`;\n\n// Global ONNX Runtime reference (loaded dynamically)\nlet ort: typeof import('onnxruntime-web') | null = null;\nlet ortLoadPromise: Promise<typeof import('onnxruntime-web')> | null = null;\n\n/**\n * Dynamically load ONNX Runtime from CDN\n */\nasync function loadONNXRuntime(): Promise<typeof import('onnxruntime-web')> {\n  // Return cached instance\n  if (ort) return ort;\n  \n  // Return existing load promise to avoid duplicate loading\n  if (ortLoadPromise) return ortLoadPromise;\n  \n  ortLoadPromise = new Promise((resolve, reject) => {\n    // Check if already loaded globally (e.g., via script tag)\n    if (typeof window !== 'undefined' && (window as any).ort) {\n      ort = (window as any).ort;\n      // Configure WASM paths\n      ort!.env.wasm.wasmPaths = ONNX_CDN_BASE;\n      resolve(ort!);\n      return;\n    }\n    \n    // Dynamically load the script\n    const script = document.createElement('script');\n    script.src = ONNX_SCRIPT_URL;\n    script.async = true;\n    \n    script.onload = () => {\n      if ((window as any).ort) {\n        ort = (window as any).ort;\n        // Configure WASM paths\n        ort!.env.wasm.wasmPaths = ONNX_CDN_BASE;\n        console.log(`\u2713 ONNX Runtime v${ONNX_VERSION} loaded from CDN`);\n        resolve(ort!);\n      } else {\n        reject(new Error('ONNX Runtime loaded but ort global not found'));\n      }\n    };\n    \n    script.onerror = () => {\n      reject(new Error(`Failed to load ONNX Runtime from ${ONNX_SCRIPT_URL}`));\n    };\n    \n    document.head.appendChild(script);\n  });\n  \n  return ortLoadPromise;\n}\n\n/**\n * Get ONNX Runtime instance (loads if needed)\n */\nasync function getOrt(): Promise<typeof import('onnxruntime-web')> {\n  if (!ort) {\n    ort = await loadONNXRuntime();\n  }\n  return ort;\n}\n\n// ============================================================================\n// ONNX Session Storage\n// ============================================================================\n\ninterface ONNXSessionData {\n  session: any; // ort.InferenceSession\n  inputNames: string[];\n  outputNames: string[];\n}\n\nconst sessionStore: Map<string, ONNXSessionData> = new Map();\n\n// ============================================================================\n// ONNX Runtime Implementation\n// ============================================================================\n\n/**\n * ONNXRuntime - Real ONNX model inference using onnxruntime-web\n * Automatically loads ONNX Runtime from CDN when first used.\n */\nexport class ONNXRuntime implements Runtime {\n  readonly name: RuntimeType = 'wasm'; // Register as wasm since it's the fallback\n  \n  private initialized = false;\n  private executionProvider: 'webgpu' | 'wasm' = 'wasm';\n\n  get capabilities(): RuntimeCapabilities {\n    return {\n      concurrency: true,\n      quantization: true,\n      float16: this.executionProvider === 'webgpu',\n      dynamicShapes: true,\n      maxBatchSize: 32,\n      availableMemory: 512 * 1024 * 1024, // 512MB\n    };\n  }\n\n  /**\n   * Check if ONNX Runtime is available (always true - will be loaded from CDN)\n   */\n  async isAvailable(): Promise<boolean> {\n    // Always return true - we'll load ONNX Runtime from CDN when needed\n    return true;\n  }\n\n  /**\n   * Initialize the ONNX runtime (loads from CDN if needed)\n   */\n  async initialize(): Promise<void> {\n    if (this.initialized) return;\n\n    // Load ONNX Runtime from CDN\n    const ortInstance = await getOrt();\n    \n    // Configure WASM paths\n    ortInstance.env.wasm.wasmPaths = ONNX_CDN_BASE;\n    \n    // Use WASM execution provider (most compatible)\n    this.executionProvider = 'wasm';\n\n    this.initialized = true;\n  }\n\n  /**\n   * Load a model from ArrayBuffer\n   */\n  async loadModel(\n    modelData: ArrayBuffer,\n    options: ModelLoadOptions = {}\n  ): Promise<LoadedModel> {\n    if (!this.initialized) {\n      await this.initialize();\n    }\n\n    const ortInstance = await getOrt();\n\n    try {\n      // Create session options\n      const sessionOptions = {\n        executionProviders: [this.executionProvider],\n        graphOptimizationLevel: 'all' as const,\n      };\n\n      // Create inference session (convert ArrayBuffer to Uint8Array)\n      const modelBytes = new Uint8Array(modelData);\n      const session = await ortInstance.InferenceSession.create(modelBytes, sessionOptions);\n      \n      // Get input/output names\n      const inputNames = session.inputNames;\n      const outputNames = session.outputNames;\n\n      // Generate model ID\n      const modelId = `onnx_${Date.now().toString(36)}_${Math.random().toString(36).slice(2, 8)}`;\n\n      // Store session\n      sessionStore.set(modelId, {\n        session,\n        inputNames: [...inputNames],\n        outputNames: [...outputNames],\n      });\n\n      // Create metadata\n      const metadata: ModelMetadata = {\n        name: options.metadata?.name ?? 'onnx-model',\n        version: '1.0.0',\n        inputs: inputNames.map(name => ({\n          name,\n          dtype: 'float32' as DataType,\n          shape: [-1], // Dynamic shape\n        })),\n        outputs: outputNames.map(name => ({\n          name,\n          dtype: 'float32' as DataType,\n          shape: [-1],\n        })),\n        sizeBytes: modelData.byteLength,\n        quantization: options.quantization ?? 'float32',\n        format: 'onnx',\n      };\n\n      // Create model instance\n      const model = new LoadedModelImpl(\n        metadata,\n        'wasm',\n        () => this.unloadModel(modelId)\n      );\n\n      // Override the ID to match our stored session\n      Object.defineProperty(model, 'id', { value: modelId, writable: false });\n\n      // Track in memory manager\n      getMemoryManager().trackModel(model, () => model.dispose());\n\n      return model;\n    } catch (error) {\n      throw new EdgeFlowError(\n        `Failed to load ONNX model: ${error instanceof Error ? error.message : String(error)}`,\n        ErrorCodes.MODEL_LOAD_FAILED,\n        { error }\n      );\n    }\n  }\n\n  /**\n   * Run inference\n   */\n  async run(model: LoadedModel, inputs: Tensor[]): Promise<Tensor[]> {\n    const sessionData = sessionStore.get(model.id);\n    if (!sessionData) {\n      throw new EdgeFlowError(\n        `ONNX session not found for model ${model.id}`,\n        ErrorCodes.MODEL_NOT_LOADED,\n        { modelId: model.id }\n      );\n    }\n\n    const ortInstance = await getOrt();\n    const { session, inputNames, outputNames } = sessionData;\n\n    try {\n      // Prepare input feeds\n      const feeds: Record<string, any> = {};\n      \n      for (let i = 0; i < Math.min(inputs.length, inputNames.length); i++) {\n        const inputName = inputNames[i];\n        const inputTensor = inputs[i] as EdgeFlowTensor;\n        \n        if (inputName && inputTensor) {\n          // Convert to ONNX tensor with correct dtype\n          const dtype = inputTensor.dtype;\n          let ortTensor: any;\n          \n          if (dtype === 'int64') {\n            // Get raw BigInt64Array data directly\n            const data = inputTensor.data as unknown as BigInt64Array;\n            ortTensor = new ortInstance.Tensor('int64', data, inputTensor.shape as number[]);\n          } else if (dtype === 'int32') {\n            const data = inputTensor.data as Int32Array;\n            ortTensor = new ortInstance.Tensor('int32', data, inputTensor.shape as number[]);\n          } else {\n            const data = inputTensor.toFloat32Array();\n            ortTensor = new ortInstance.Tensor('float32', data, inputTensor.shape as number[]);\n          }\n          \n          feeds[inputName] = ortTensor;\n        }\n      }\n\n      // Run inference\n      const results = await session.run(feeds);\n\n      // Convert outputs to EdgeFlowTensor\n      const outputs: Tensor[] = [];\n      \n      for (const outputName of outputNames) {\n        const ortTensor = results[outputName];\n        if (ortTensor) {\n          const data = ortTensor.data as Float32Array;\n          const shape = Array.from(ortTensor.dims).map(d => Number(d));\n          outputs.push(new EdgeFlowTensor(new Float32Array(data), shape, 'float32'));\n        }\n      }\n\n      return outputs;\n    } catch (error) {\n      throw new EdgeFlowError(\n        `ONNX inference failed: ${error instanceof Error ? error.message : String(error)}`,\n        ErrorCodes.INFERENCE_FAILED,\n        { modelId: model.id, error }\n      );\n    }\n  }\n\n  /**\n   * Unload a model\n   */\n  private async unloadModel(modelId: string): Promise<void> {\n    const sessionData = sessionStore.get(modelId);\n    if (sessionData) {\n      // Release session will be handled by GC\n      sessionStore.delete(modelId);\n    }\n  }\n\n  /**\n   * Dispose the runtime\n   */\n  dispose(): void {\n    // Clear all sessions\n    sessionStore.clear();\n    this.initialized = false;\n  }\n}\n\n/**\n * Create ONNX runtime factory\n */\nexport function createONNXRuntime(): Runtime {\n  return new ONNXRuntime();\n}\n", "/**\n * edgeFlow.js - Backend Exports\n */\n\n// WebGPU Backend\nexport { WebGPURuntime, createWebGPURuntime } from './webgpu.js';\n\n// WebNN Backend\nexport { WebNNRuntime, createWebNNRuntime } from './webnn.js';\n\n// WASM Backend (basic tensor ops)\nexport { WASMRuntime, createWASMRuntime } from './wasm.js';\n\n// ONNX Runtime Backend (real model inference)\nexport { ONNXRuntime, createONNXRuntime } from './onnx.js';\n\n// Re-export types\nexport type { Runtime, RuntimeType, RuntimeCapabilities } from '../core/types.js';\n\n/**\n * Initialize all backends with the runtime manager\n */\nimport { registerRuntime } from '../core/runtime.js';\nimport { createWebGPURuntime } from './webgpu.js';\nimport { createWebNNRuntime } from './webnn.js';\nimport { createONNXRuntime } from './onnx.js';\n\n/**\n * Register all available backends\n */\nexport function registerAllBackends(): void {\n  registerRuntime('webgpu', createWebGPURuntime);\n  registerRuntime('webnn', createWebNNRuntime);\n  // Use ONNX Runtime as the WASM backend for real model inference\n  registerRuntime('wasm', createONNXRuntime);\n}\n\n/**\n * Auto-register backends on module load\n */\nregisterAllBackends();\n", "/**\n * edgeFlow.js - Caching Utilities\n * \n * Smart caching for models, tensors, and inference results.\n */\n\n// ============================================================================\n// Cache Types\n// ============================================================================\n\n/**\n * Cache strategy types\n */\nexport type CacheStrategy = 'lru' | 'lfu' | 'fifo' | 'ttl';\n\n/**\n * Cache entry\n */\ninterface CacheEntry<T> {\n  value: T;\n  size: number;\n  createdAt: number;\n  accessedAt: number;\n  accessCount: number;\n  ttl?: number;\n}\n\n/**\n * Cache options\n */\nexport interface CacheOptions {\n  /** Cache strategy */\n  strategy?: CacheStrategy;\n  /** Maximum cache size in bytes */\n  maxSize?: number;\n  /** Maximum number of entries */\n  maxEntries?: number;\n  /** Default TTL in milliseconds */\n  ttl?: number;\n  /** Enable persistence to IndexedDB */\n  persistent?: boolean;\n  /** Cache name for persistence */\n  name?: string;\n}\n\n/**\n * Cache statistics\n */\nexport interface CacheStats {\n  /** Number of entries */\n  entries: number;\n  /** Total size in bytes */\n  size: number;\n  /** Cache hits */\n  hits: number;\n  /** Cache misses */\n  misses: number;\n  /** Hit rate (0-1) */\n  hitRate: number;\n}\n\n// ============================================================================\n// Cache Implementation\n// ============================================================================\n\n/**\n * Cache - Generic cache implementation\n */\nexport class Cache<T> {\n  private readonly options: Required<CacheOptions>;\n  private readonly cache: Map<string, CacheEntry<T>> = new Map();\n  private currentSize = 0;\n  private hits = 0;\n  private misses = 0;\n\n  constructor(options: CacheOptions = {}) {\n    this.options = {\n      strategy: options.strategy ?? 'lru',\n      maxSize: options.maxSize ?? 100 * 1024 * 1024, // 100MB\n      maxEntries: options.maxEntries ?? 1000,\n      ttl: options.ttl ?? 0, // 0 = no TTL\n      persistent: options.persistent ?? false,\n      name: options.name ?? 'edgeflow-cache',\n    };\n\n    // Load from persistent storage if enabled\n    if (this.options.persistent) {\n      this.loadFromStorage();\n    }\n  }\n\n  /**\n   * Get value from cache\n   */\n  get(key: string): T | undefined {\n    const entry = this.cache.get(key);\n    \n    if (!entry) {\n      this.misses++;\n      return undefined;\n    }\n\n    // Check TTL\n    if (entry.ttl && Date.now() - entry.createdAt > entry.ttl) {\n      this.delete(key);\n      this.misses++;\n      return undefined;\n    }\n\n    // Update access stats\n    entry.accessedAt = Date.now();\n    entry.accessCount++;\n    this.hits++;\n\n    return entry.value;\n  }\n\n  /**\n   * Set value in cache\n   */\n  set(key: string, value: T, size: number, ttl?: number): void {\n    // Remove existing entry if present\n    if (this.cache.has(key)) {\n      this.delete(key);\n    }\n\n    // Evict entries if necessary\n    while (\n      (this.currentSize + size > this.options.maxSize ||\n       this.cache.size >= this.options.maxEntries) &&\n      this.cache.size > 0\n    ) {\n      this.evict();\n    }\n\n    // Determine TTL value\n    const entryTtl = ttl !== undefined ? ttl : (this.options.ttl > 0 ? this.options.ttl : undefined);\n\n    // Add new entry\n    const entry: CacheEntry<T> = {\n      value,\n      size,\n      createdAt: Date.now(),\n      accessedAt: Date.now(),\n      accessCount: 1,\n      ttl: entryTtl,\n    };\n\n    this.cache.set(key, entry);\n    this.currentSize += size;\n\n    // Persist if enabled\n    if (this.options.persistent) {\n      this.saveToStorage();\n    }\n  }\n\n  /**\n   * Check if key exists\n   */\n  has(key: string): boolean {\n    const entry = this.cache.get(key);\n    if (!entry) return false;\n\n    // Check TTL\n    if (entry.ttl && Date.now() - entry.createdAt > entry.ttl) {\n      this.delete(key);\n      return false;\n    }\n\n    return true;\n  }\n\n  /**\n   * Delete entry\n   */\n  delete(key: string): boolean {\n    const entry = this.cache.get(key);\n    if (entry) {\n      this.currentSize -= entry.size;\n      this.cache.delete(key);\n      \n      if (this.options.persistent) {\n        this.saveToStorage();\n      }\n      \n      return true;\n    }\n    return false;\n  }\n\n  /**\n   * Clear the cache\n   */\n  clear(): void {\n    this.cache.clear();\n    this.currentSize = 0;\n    this.hits = 0;\n    this.misses = 0;\n\n    if (this.options.persistent) {\n      this.clearStorage();\n    }\n  }\n\n  /**\n   * Get cache statistics\n   */\n  getStats(): CacheStats {\n    const total = this.hits + this.misses;\n    return {\n      entries: this.cache.size,\n      size: this.currentSize,\n      hits: this.hits,\n      misses: this.misses,\n      hitRate: total > 0 ? this.hits / total : 0,\n    };\n  }\n\n  /**\n   * Evict an entry based on strategy\n   */\n  private evict(): void {\n    let keyToEvict: string | null = null;\n\n    switch (this.options.strategy) {\n      case 'lru':\n        keyToEvict = this.findLRU();\n        break;\n      case 'lfu':\n        keyToEvict = this.findLFU();\n        break;\n      case 'fifo':\n        keyToEvict = this.findOldest();\n        break;\n      case 'ttl':\n        keyToEvict = this.findExpired() ?? this.findOldest();\n        break;\n    }\n\n    if (keyToEvict) {\n      this.delete(keyToEvict);\n    }\n  }\n\n  /**\n   * Find least recently used entry\n   */\n  private findLRU(): string | null {\n    let oldest: string | null = null;\n    let oldestTime = Infinity;\n\n    for (const [key, entry] of this.cache) {\n      if (entry.accessedAt < oldestTime) {\n        oldestTime = entry.accessedAt;\n        oldest = key;\n      }\n    }\n\n    return oldest;\n  }\n\n  /**\n   * Find least frequently used entry\n   */\n  private findLFU(): string | null {\n    let lfu: string | null = null;\n    let minCount = Infinity;\n\n    for (const [key, entry] of this.cache) {\n      if (entry.accessCount < minCount) {\n        minCount = entry.accessCount;\n        lfu = key;\n      }\n    }\n\n    return lfu;\n  }\n\n  /**\n   * Find oldest entry (FIFO)\n   */\n  private findOldest(): string | null {\n    let oldest: string | null = null;\n    let oldestTime = Infinity;\n\n    for (const [key, entry] of this.cache) {\n      if (entry.createdAt < oldestTime) {\n        oldestTime = entry.createdAt;\n        oldest = key;\n      }\n    }\n\n    return oldest;\n  }\n\n  /**\n   * Find expired entry\n   */\n  private findExpired(): string | null {\n    const now = Date.now();\n\n    for (const [key, entry] of this.cache) {\n      if (entry.ttl && now - entry.createdAt > entry.ttl) {\n        return key;\n      }\n    }\n\n    return null;\n  }\n\n  /**\n   * Load cache from IndexedDB\n   */\n  private async loadFromStorage(): Promise<void> {\n    if (typeof indexedDB === 'undefined') return;\n\n    try {\n      const db = await this.openDB();\n      const tx = db.transaction('cache', 'readonly');\n      const store = tx.objectStore('cache');\n      const request = store.getAll();\n\n      return new Promise((resolve, reject) => {\n        request.onsuccess = () => {\n          const entries = request.result as Array<{ key: string; entry: CacheEntry<T> }>;\n          for (const { key, entry } of entries) {\n            this.cache.set(key, entry);\n            this.currentSize += entry.size;\n          }\n          resolve();\n        };\n        request.onerror = () => reject(request.error);\n      });\n    } catch {\n      // Ignore storage errors\n    }\n  }\n\n  /**\n   * Save cache to IndexedDB\n   */\n  private async saveToStorage(): Promise<void> {\n    if (typeof indexedDB === 'undefined') return;\n\n    try {\n      const db = await this.openDB();\n      const tx = db.transaction('cache', 'readwrite');\n      const store = tx.objectStore('cache');\n\n      // Clear existing entries\n      store.clear();\n\n      // Add current entries\n      for (const [key, entry] of this.cache) {\n        store.put({ key, entry });\n      }\n\n      return new Promise((resolve, reject) => {\n        tx.oncomplete = () => resolve();\n        tx.onerror = () => reject(tx.error);\n      });\n    } catch {\n      // Ignore storage errors\n    }\n  }\n\n  /**\n   * Clear IndexedDB storage\n   */\n  private async clearStorage(): Promise<void> {\n    if (typeof indexedDB === 'undefined') return;\n\n    try {\n      const db = await this.openDB();\n      const tx = db.transaction('cache', 'readwrite');\n      const store = tx.objectStore('cache');\n      store.clear();\n    } catch {\n      // Ignore storage errors\n    }\n  }\n\n  /**\n   * Open IndexedDB database\n   */\n  private openDB(): Promise<IDBDatabase> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open(this.options.name, 1);\n\n      request.onupgradeneeded = () => {\n        const db = request.result;\n        if (!db.objectStoreNames.contains('cache')) {\n          db.createObjectStore('cache', { keyPath: 'key' });\n        }\n      };\n\n      request.onsuccess = () => resolve(request.result);\n      request.onerror = () => reject(request.error);\n    });\n  }\n}\n\n// ============================================================================\n// Inference Result Cache\n// ============================================================================\n\n/**\n * InferenceCache - Cache for inference results\n */\nexport class InferenceCache extends Cache<Float32Array> {\n  /**\n   * Generate cache key from input\n   */\n  generateKey(modelId: string, input: Float32Array | number[]): string {\n    // Create hash from input data\n    const inputArray = Array.isArray(input) ? input : Array.from(input);\n    const hash = this.hashArray(inputArray);\n    return `${modelId}:${hash}`;\n  }\n\n  /**\n   * Simple hash function for arrays\n   */\n  private hashArray(arr: number[]): string {\n    let hash = 0;\n    const sample = arr.length > 100 \n      ? arr.filter((_, i) => i % Math.floor(arr.length / 100) === 0)\n      : arr;\n    \n    for (let i = 0; i < sample.length; i++) {\n      const value = sample[i] ?? 0;\n      hash = ((hash << 5) - hash) + (value * 1000 | 0);\n      hash |= 0;\n    }\n    \n    return hash.toString(36);\n  }\n}\n\n// ============================================================================\n// Model Cache\n// ============================================================================\n\n/**\n * Model download cache using Cache API\n */\nexport class ModelDownloadCache {\n  private readonly cacheName: string;\n  private cache: globalThis.Cache | null = null;\n\n  constructor(cacheName: string = 'edgeflow-models') {\n    this.cacheName = cacheName;\n  }\n\n  /**\n   * Initialize cache\n   */\n  private async ensureCache(): Promise<globalThis.Cache> {\n    if (!this.cache) {\n      if (typeof caches === 'undefined') {\n        throw new Error('Cache API is not available');\n      }\n      this.cache = await caches.open(this.cacheName);\n    }\n    return this.cache;\n  }\n\n  /**\n   * Get cached response\n   */\n  async get(url: string): Promise<Response | undefined> {\n    try {\n      const cache = await this.ensureCache();\n      return await cache.match(url) ?? undefined;\n    } catch {\n      return undefined;\n    }\n  }\n\n  /**\n   * Store response in cache\n   */\n  async put(url: string, response: Response): Promise<void> {\n    try {\n      const cache = await this.ensureCache();\n      await cache.put(url, response.clone());\n    } catch {\n      // Ignore cache errors\n    }\n  }\n\n  /**\n   * Delete cached response\n   */\n  async delete(url: string): Promise<boolean> {\n    try {\n      const cache = await this.ensureCache();\n      return await cache.delete(url);\n    } catch {\n      return false;\n    }\n  }\n\n  /**\n   * Clear all cached models\n   */\n  async clear(): Promise<void> {\n    try {\n      await caches.delete(this.cacheName);\n      this.cache = null;\n    } catch {\n      // Ignore cache errors\n    }\n  }\n\n  /**\n   * Get all cached URLs\n   */\n  async keys(): Promise<string[]> {\n    try {\n      const cache = await this.ensureCache();\n      const requests = await cache.keys();\n      return requests.map(r => r.url);\n    } catch {\n      return [];\n    }\n  }\n}\n\n// ============================================================================\n// Factory Functions\n// ============================================================================\n\n/**\n * Create a cache with common presets\n */\nexport function createCache<T>(\n  preset: 'small' | 'medium' | 'large' | 'custom' = 'medium',\n  options: CacheOptions = {}\n): Cache<T> {\n  const presets: Record<string, CacheOptions> = {\n    small: {\n      maxSize: 10 * 1024 * 1024, // 10MB\n      maxEntries: 100,\n    },\n    medium: {\n      maxSize: 100 * 1024 * 1024, // 100MB\n      maxEntries: 500,\n    },\n    large: {\n      maxSize: 500 * 1024 * 1024, // 500MB\n      maxEntries: 2000,\n    },\n    custom: {},\n  };\n\n  return new Cache<T>({ ...presets[preset], ...options });\n}\n", "/**\n * edgeFlow.js - Base Pipeline\n * \n * Base class and utilities for all pipeline implementations.\n */\n\nimport {\n  LoadedModel,\n  PipelineConfig,\n  PipelineOptions,\n  PipelineTask,\n} from '../core/types.js';\nimport { loadModel, runInference } from '../core/runtime.js';\nimport { EdgeFlowTensor } from '../core/tensor.js';\nimport { ModelCache } from '../core/memory.js';\nimport { ModelDownloadCache } from '../utils/cache.js';\n\n// ============================================================================\n// Pipeline Types\n// ============================================================================\n\n/**\n * Pipeline result base interface\n */\nexport interface PipelineResult {\n  /** Processing time in milliseconds */\n  processingTime?: number;\n}\n\n/**\n * Text classification result\n */\nexport interface TextClassificationResult extends PipelineResult {\n  label: string;\n  score: number;\n}\n\n/**\n * Feature extraction result\n */\nexport interface FeatureExtractionResult extends PipelineResult {\n  embeddings: number[];\n}\n\n/**\n * Image classification result\n */\nexport interface ImageClassificationResult extends PipelineResult {\n  label: string;\n  score: number;\n}\n\n/**\n * Object detection result\n */\nexport interface ObjectDetectionResult extends PipelineResult {\n  label: string;\n  score: number;\n  box: { x: number; y: number; width: number; height: number };\n}\n\n// ============================================================================\n// Base Pipeline Class\n// ============================================================================\n\n/**\n * BasePipeline - Abstract base class for all pipelines\n */\nexport abstract class BasePipeline<TInput, TOutput extends PipelineResult | PipelineResult[]> {\n  protected model: LoadedModel | null = null;\n  protected readonly config: PipelineConfig;\n  protected readonly modelCache: ModelCache;\n  protected readonly downloadCache: ModelDownloadCache;\n  protected isReady = false;\n\n  constructor(config: PipelineConfig) {\n    this.config = config;\n    this.modelCache = new ModelCache();\n    this.downloadCache = new ModelDownloadCache();\n  }\n\n  /**\n   * Initialize the pipeline (load model)\n   */\n  async initialize(): Promise<void> {\n    if (this.isReady && this.model) return;\n\n    // Check model cache first\n    const cachedModel = this.modelCache.get(this.config.model);\n    if (cachedModel) {\n      this.model = cachedModel;\n      this.isReady = true;\n      return;\n    }\n\n    // Load model\n    this.model = await this.loadModelWithCache(this.config.model);\n    this.isReady = true;\n  }\n\n  /**\n   * Load model with caching\n   */\n  protected async loadModelWithCache(modelPath: string): Promise<LoadedModel> {\n    // Try download cache first\n    const cachedResponse = await this.downloadCache.get(modelPath);\n    if (cachedResponse) {\n      // Use cached data\n    }\n\n    // Download and cache (or use mock for now)\n    try {\n      const response = await fetch(modelPath);\n      if (response.ok) {\n        // Cache the response\n        await this.downloadCache.put(modelPath, response.clone());\n      }\n    } catch {\n      // Ignore fetch errors for demo\n    }\n\n    // Load into runtime\n    return loadModel(modelPath, {\n      runtime: this.config.runtime,\n      quantization: this.config.quantization,\n      cache: this.config.cache,\n    });\n  }\n\n  /**\n   * Run inference (single input)\n   */\n  async run(input: TInput, options?: PipelineOptions): Promise<TOutput> {\n    await this.initialize();\n    \n    const startTime = performance.now();\n    \n    // Preprocess\n    const preprocessed = await this.preprocess(input);\n    \n    // Run inference\n    const outputs = await runInference(this.model!, preprocessed);\n    \n    // Postprocess\n    const result = await this.postprocess(outputs as EdgeFlowTensor[], options);\n    \n    if (result && typeof result === 'object' && 'processingTime' in result) {\n      (result as PipelineResult).processingTime = performance.now() - startTime;\n    }\n    \n    return result;\n  }\n\n  /**\n   * Run batch inference\n   */\n  async runBatch(inputs: TInput[], options?: PipelineOptions): Promise<TOutput[]> {\n    await this.initialize();\n    \n    // Process all inputs\n    const results = await Promise.all(\n      inputs.map(input => this.run(input, options))\n    );\n    \n    return results;\n  }\n\n  /**\n   * Preprocess input - must be implemented by subclasses\n   */\n  protected abstract preprocess(input: TInput): Promise<EdgeFlowTensor[]>;\n\n  /**\n   * Postprocess output - must be implemented by subclasses\n   */\n  protected abstract postprocess(\n    outputs: EdgeFlowTensor[],\n    options?: PipelineOptions\n  ): Promise<TOutput>;\n\n  /**\n   * Get the task type\n   */\n  get task(): PipelineTask {\n    return this.config.task;\n  }\n\n  /**\n   * Check if pipeline is ready\n   */\n  get ready(): boolean {\n    return this.isReady;\n  }\n\n  /**\n   * Dispose the pipeline\n   */\n  dispose(): void {\n    if (this.model) {\n      this.model.dispose();\n      this.model = null;\n    }\n    this.isReady = false;\n  }\n}\n\n// ============================================================================\n// Pipeline Registry\n// ============================================================================\n\n/**\n * Pipeline factory function type\n */\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype PipelineFactory = (config: PipelineConfig) => BasePipeline<any, any>;\n\n/**\n * Registered pipeline factories\n */\nconst pipelineFactories: Map<PipelineTask, PipelineFactory> = new Map();\n\n/**\n * Register a pipeline factory\n */\nexport function registerPipeline(task: PipelineTask, factory: PipelineFactory): void {\n  pipelineFactories.set(task, factory);\n}\n\n/**\n * Get a pipeline factory\n */\nexport function getPipelineFactory(task: PipelineTask): PipelineFactory | undefined {\n  return pipelineFactories.get(task);\n}\n\n// ============================================================================\n// Default Label Maps\n// ============================================================================\n\n/**\n * Common sentiment labels\n */\nexport const SENTIMENT_LABELS = ['negative', 'positive'];\n\n/**\n * Common emotion labels\n */\nexport const EMOTION_LABELS = [\n  'anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral'\n];\n\n/**\n * ImageNet top-10 labels (for demo)\n */\nexport const IMAGENET_LABELS = [\n  'tench', 'goldfish', 'great white shark', 'tiger shark', 'hammerhead',\n  'electric ray', 'stingray', 'cock', 'hen', 'ostrich'\n];\n", "/**\n * edgeFlow.js - Tokenizer\n * \n * Full-featured tokenizer supporting HuggingFace tokenizer.json format.\n * Supports BPE, WordPiece, and Unigram tokenization.\n */\n\nimport {\n  TokenizerConfig,\n  TokenizedOutput,\n  EdgeFlowError,\n  ErrorCodes,\n} from '../core/types.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\nexport type TokenizerModel = 'BPE' | 'WordPiece' | 'Unigram' | 'basic';\n\nexport interface TokenizerOptions {\n  addSpecialTokens?: boolean;\n  maxLength?: number;\n  padding?: 'max_length' | 'longest' | 'do_not_pad';\n  truncation?: boolean;\n  returnAttentionMask?: boolean;\n  returnTokenTypeIds?: boolean;\n  textPair?: string;\n}\n\n/**\n * HuggingFace tokenizer.json format\n */\ninterface HFTokenizerJSON {\n  version?: string;\n  truncation?: {\n    max_length: number;\n    strategy: string;\n  };\n  padding?: {\n    strategy: string;\n    pad_id: number;\n    pad_token: string;\n  };\n  added_tokens?: Array<{\n    id: number;\n    content: string;\n    single_word: boolean;\n    lstrip: boolean;\n    rstrip: boolean;\n    normalized: boolean;\n    special: boolean;\n  }>;\n  normalizer?: {\n    type: string;\n    lowercase?: boolean;\n    strip_accents?: boolean;\n    [key: string]: unknown;\n  };\n  pre_tokenizer?: {\n    type: string;\n    [key: string]: unknown;\n  };\n  post_processor?: {\n    type: string;\n    single?: Array<{ id: string; type_id: number } | { SpecialToken: { id: string; type_id: number } } | { Sequence: { id: string; type_id: number } }>;\n    pair?: Array<{ id: string; type_id: number } | { SpecialToken: { id: string; type_id: number } } | { Sequence: { id: string; type_id: number } }>;\n    special_tokens?: Record<string, { id: string; ids: number[]; tokens: string[] }>;\n    [key: string]: unknown;\n  };\n  decoder?: {\n    type: string;\n    [key: string]: unknown;\n  };\n  model: {\n    type: string;\n    vocab?: Record<string, number>;\n    merges?: string[];\n    unk_token?: string;\n    continuing_subword_prefix?: string;\n    end_of_word_suffix?: string;\n    fuse_unk?: boolean;\n    byte_fallback?: boolean;\n    [key: string]: unknown;\n  };\n}\n\n\n// ============================================================================\n// Tokenizer Implementation\n// ============================================================================\n\n/**\n * Tokenizer - Full-featured tokenizer supporting HuggingFace format\n */\nexport class Tokenizer {\n  private vocab: Map<string, number> = new Map();\n  private reverseVocab: Map<number, string> = new Map();\n  private merges: Map<string, number> = new Map();\n  private addedTokens: Map<string, number> = new Map();\n  private specialTokens: Set<string> = new Set();\n  \n  private modelType: TokenizerModel = 'BPE';\n  private unkToken: string = '[UNK]';\n  private continuingSubwordPrefix: string = '##';\n  \n  // Special token IDs\n  private padTokenId: number = 0;\n  private unkTokenId: number = 0;\n  private clsTokenId?: number;\n  private sepTokenId?: number;\n  private maskTokenId?: number;\n  private bosTokenId?: number;\n  private eosTokenId?: number;\n  \n  // Config\n  private maxLength: number = 512;\n  private doLowerCase: boolean = false;\n  private stripAccents: boolean = false;\n  \n  // Post-processor config\n  private postProcessor?: HFTokenizerJSON['post_processor'];\n  \n  // Byte encoder for BPE\n  private byteEncoder: Map<number, string> = new Map();\n  private byteDecoder: Map<string, number> = new Map();\n\n  constructor() {\n    this.initByteEncoder();\n  }\n\n  /**\n   * Initialize byte encoder/decoder for BPE\n   */\n  private initByteEncoder(): void {\n    const bytes: number[] = [];\n    \n    // Printable ASCII\n    for (let i = 33; i <= 126; i++) bytes.push(i);\n    for (let i = 161; i <= 172; i++) bytes.push(i);\n    for (let i = 174; i <= 255; i++) bytes.push(i);\n    \n    const chars = [...bytes];\n    let n = 0;\n    \n    for (let i = 0; i < 256; i++) {\n      if (!bytes.includes(i)) {\n        bytes.push(i);\n        chars.push(256 + n);\n        n++;\n      }\n    }\n    \n    for (let i = 0; i < bytes.length; i++) {\n      const byte = bytes[i]!;\n      const char = String.fromCharCode(chars[i]!);\n      this.byteEncoder.set(byte, char);\n      this.byteDecoder.set(char, byte);\n    }\n  }\n\n  /**\n   * Load from HuggingFace tokenizer.json\n   */\n  static async fromJSON(json: HFTokenizerJSON | string): Promise<Tokenizer> {\n    const tokenizer = new Tokenizer();\n    const data = typeof json === 'string' ? JSON.parse(json) as HFTokenizerJSON : json;\n    \n    // Load model config\n    if (data.model) {\n      tokenizer.modelType = data.model.type as TokenizerModel;\n      \n      // Load vocabulary\n      if (data.model.vocab) {\n        for (const [token, id] of Object.entries(data.model.vocab)) {\n          tokenizer.vocab.set(token, id);\n          tokenizer.reverseVocab.set(id, token);\n        }\n      }\n      \n      // Load merges for BPE\n      if (data.model.merges) {\n        for (let i = 0; i < data.model.merges.length; i++) {\n          tokenizer.merges.set(data.model.merges[i]!, i);\n        }\n      }\n      \n      // Model-specific config\n      tokenizer.unkToken = data.model.unk_token ?? '[UNK]';\n      tokenizer.continuingSubwordPrefix = data.model.continuing_subword_prefix ?? '##';\n    }\n    \n    // Load added tokens\n    if (data.added_tokens) {\n      for (const token of data.added_tokens) {\n        tokenizer.addedTokens.set(token.content, token.id);\n        tokenizer.reverseVocab.set(token.id, token.content);\n        if (token.special) {\n          tokenizer.specialTokens.add(token.content);\n        }\n        \n        // Detect special token types\n        const content = token.content.toLowerCase();\n        if (content.includes('pad')) tokenizer.padTokenId = token.id;\n        if (content.includes('unk')) tokenizer.unkTokenId = token.id;\n        if (content.includes('cls') || content === '[cls]') tokenizer.clsTokenId = token.id;\n        if (content.includes('sep') || content === '[sep]') tokenizer.sepTokenId = token.id;\n        if (content.includes('mask')) tokenizer.maskTokenId = token.id;\n        if (content.includes('bos') || content === '<s>') tokenizer.bosTokenId = token.id;\n        if (content.includes('eos') || content === '</s>') tokenizer.eosTokenId = token.id;\n      }\n    }\n    \n    // Load normalizer config\n    if (data.normalizer) {\n      tokenizer.doLowerCase = data.normalizer.lowercase ?? false;\n      tokenizer.stripAccents = data.normalizer.strip_accents ?? false;\n    }\n    \n    // Load truncation config\n    if (data.truncation) {\n      tokenizer.maxLength = data.truncation.max_length;\n    }\n    \n    // Load post-processor\n    if (data.post_processor) {\n      tokenizer.postProcessor = data.post_processor;\n    }\n    \n    return tokenizer;\n  }\n\n  /**\n   * Load from URL (tokenizer.json)\n   */\n  static async fromUrl(url: string): Promise<Tokenizer> {\n    const response = await fetch(url);\n    if (!response.ok) {\n      throw new EdgeFlowError(\n        `Failed to load tokenizer from ${url}: ${response.status}`,\n        ErrorCodes.MODEL_NOT_FOUND\n      );\n    }\n    const json = await response.json() as HFTokenizerJSON;\n    return Tokenizer.fromJSON(json);\n  }\n\n  /**\n   * Load from HuggingFace Hub\n   */\n  static async fromHuggingFace(modelId: string, options?: { revision?: string }): Promise<Tokenizer> {\n    const revision = options?.revision ?? 'main';\n    const url = `https://huggingface.co/${modelId}/resolve/${revision}/tokenizer.json`;\n    return Tokenizer.fromUrl(url);\n  }\n\n  /**\n   * Normalize text\n   */\n  private normalize(text: string): string {\n    let result = text;\n    \n    if (this.doLowerCase) {\n      result = result.toLowerCase();\n    }\n    \n    if (this.stripAccents) {\n      result = result.normalize('NFD').replace(/[\\u0300-\\u036f]/g, '');\n    }\n    \n    // Normalize whitespace\n    result = result.replace(/\\s+/g, ' ').trim();\n    \n    return result;\n  }\n\n  /**\n   * Pre-tokenize text (split into words)\n   */\n  private preTokenize(text: string): string[] {\n    // GPT-2 style: split on whitespace and punctuation, keeping them\n    const pattern = /'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+/gu;\n    const matches = text.match(pattern);\n    return matches ?? [text];\n  }\n\n  /**\n   * Encode text to bytes (for BPE)\n   */\n  private textToBytes(text: string): string {\n    const encoder = new TextEncoder();\n    const bytes = encoder.encode(text);\n    return Array.from(bytes).map(b => this.byteEncoder.get(b) ?? '').join('');\n  }\n\n  /**\n   * Decode bytes to text (for BPE)\n   */\n  private bytesToText(text: string): string {\n    const bytes = new Uint8Array(\n      text.split('').map(c => this.byteDecoder.get(c) ?? 0)\n    );\n    const decoder = new TextDecoder('utf-8', { fatal: false });\n    return decoder.decode(bytes);\n  }\n\n  /**\n   * Get BPE pairs from word\n   */\n  private getPairs(word: string[]): Set<string> {\n    const pairs = new Set<string>();\n    for (let i = 0; i < word.length - 1; i++) {\n      pairs.add(`${word[i]} ${word[i + 1]}`);\n    }\n    return pairs;\n  }\n\n  /**\n   * Apply BPE to a word\n   */\n  private bpe(token: string): string[] {\n    if (this.vocab.has(token)) {\n      return [token];\n    }\n    \n    let word = token.split('');\n    let pairs = this.getPairs(word);\n    \n    if (pairs.size === 0) {\n      return [token];\n    }\n    \n    while (true) {\n      // Find the pair with lowest merge rank\n      let minPair: string | null = null;\n      let minRank = Infinity;\n      \n      for (const pair of pairs) {\n        const rank = this.merges.get(pair);\n        if (rank !== undefined && rank < minRank) {\n          minRank = rank;\n          minPair = pair;\n        }\n      }\n      \n      if (minPair === null) break;\n      \n      const parts = minPair.split(' ');\n      const first = parts[0];\n      const second = parts[1];\n      if (!first || !second) break;\n      \n      const newWord: string[] = [];\n      let i = 0;\n      \n      while (i < word.length) {\n        const j = word.indexOf(first, i);\n        if (j === -1) {\n          newWord.push(...word.slice(i));\n          break;\n        }\n        \n        newWord.push(...word.slice(i, j));\n        \n        if (word[j] === first && j < word.length - 1 && word[j + 1] === second) {\n          newWord.push(first + second);\n          i = j + 2;\n        } else {\n          newWord.push(word[j]!);\n          i = j + 1;\n        }\n      }\n      \n      word = newWord;\n      \n      if (word.length === 1) break;\n      \n      pairs = this.getPairs(word);\n    }\n    \n    return word;\n  }\n\n  /**\n   * WordPiece tokenization\n   */\n  private wordPiece(word: string): string[] {\n    if (this.vocab.has(word)) {\n      return [word];\n    }\n    \n    const tokens: string[] = [];\n    let start = 0;\n    \n    while (start < word.length) {\n      let end = word.length;\n      let curSubstr: string | null = null;\n      \n      while (start < end) {\n        let substr = word.slice(start, end);\n        if (start > 0) {\n          substr = this.continuingSubwordPrefix + substr;\n        }\n        \n        if (this.vocab.has(substr)) {\n          curSubstr = substr;\n          break;\n        }\n        end--;\n      }\n      \n      if (curSubstr === null) {\n        tokens.push(this.unkToken);\n        start++;\n      } else {\n        tokens.push(curSubstr);\n        start = end;\n      }\n    }\n    \n    return tokens;\n  }\n\n  /**\n   * Tokenize a single word\n   */\n  private tokenizeWord(word: string): string[] {\n    // Check added tokens first\n    if (this.addedTokens.has(word)) {\n      return [word];\n    }\n    \n    switch (this.modelType) {\n      case 'BPE': {\n        // Convert to byte representation\n        const byteStr = this.textToBytes(word);\n        return this.bpe(byteStr);\n      }\n      case 'WordPiece':\n        return this.wordPiece(word);\n      default:\n        return this.vocab.has(word) ? [word] : [this.unkToken];\n    }\n  }\n\n  /**\n   * Main tokenization\n   */\n  private tokenize(text: string): string[] {\n    // Normalize\n    const normalized = this.normalize(text);\n    \n    // Check for added tokens (special tokens)\n    const tokens: string[] = [];\n    let remaining = normalized;\n    \n    // Sort added tokens by length (longest first) for greedy matching\n    const sortedAddedTokens = Array.from(this.addedTokens.keys())\n      .sort((a, b) => b.length - a.length);\n    \n    // Split by added tokens\n    for (const addedToken of sortedAddedTokens) {\n      if (remaining.includes(addedToken)) {\n        const parts = remaining.split(addedToken);\n        const newRemaining: string[] = [];\n        \n        for (let i = 0; i < parts.length; i++) {\n          if (parts[i]) {\n            newRemaining.push(parts[i]!);\n          }\n          if (i < parts.length - 1) {\n            tokens.push(addedToken);\n          }\n        }\n        \n        remaining = newRemaining.join(' ');\n      }\n    }\n    \n    // Pre-tokenize remaining text\n    if (remaining.trim()) {\n      const words = this.preTokenize(remaining);\n      \n      for (const word of words) {\n        if (!word) continue;\n        const wordTokens = this.tokenizeWord(word);\n        tokens.push(...wordTokens);\n      }\n    }\n    \n    return tokens;\n  }\n\n  /**\n   * Convert tokens to IDs\n   */\n  private convertTokensToIds(tokens: string[]): number[] {\n    return tokens.map(token => {\n      // Check added tokens first\n      const addedId = this.addedTokens.get(token);\n      if (addedId !== undefined) return addedId;\n      \n      // Check vocabulary\n      const vocabId = this.vocab.get(token);\n      if (vocabId !== undefined) return vocabId;\n      \n      // Return UNK\n      return this.unkTokenId;\n    });\n  }\n\n  /**\n   * Convert IDs to tokens\n   */\n  private convertIdsToTokens(ids: number[]): string[] {\n    return ids.map(id => this.reverseVocab.get(id) ?? this.unkToken);\n  }\n\n  /**\n   * Apply post-processing (add special tokens)\n   */\n  private postProcess(\n    ids: number[],\n    pairIds?: number[]\n  ): { ids: number[]; typeIds: number[] } {\n    if (!this.postProcessor) {\n      // Default: [CLS] tokens [SEP] or [CLS] tokens [SEP] pair [SEP]\n      const result: number[] = [];\n      const typeIds: number[] = [];\n      \n      if (this.clsTokenId !== undefined) {\n        result.push(this.clsTokenId);\n        typeIds.push(0);\n      }\n      \n      result.push(...ids);\n      typeIds.push(...ids.map(() => 0));\n      \n      if (this.sepTokenId !== undefined) {\n        result.push(this.sepTokenId);\n        typeIds.push(0);\n      }\n      \n      if (pairIds) {\n        result.push(...pairIds);\n        typeIds.push(...pairIds.map(() => 1));\n        \n        if (this.sepTokenId !== undefined) {\n          result.push(this.sepTokenId);\n          typeIds.push(1);\n        }\n      }\n      \n      return { ids: result, typeIds };\n    }\n    \n    // Use post-processor config\n    const template = pairIds ? this.postProcessor.pair : this.postProcessor.single;\n    if (!template) {\n      return { ids, typeIds: ids.map(() => 0) };\n    }\n    \n    const result: number[] = [];\n    const typeIds: number[] = [];\n    \n    for (const item of template) {\n      if ('SpecialToken' in item) {\n        const specialToken = this.postProcessor.special_tokens?.[item.SpecialToken.id];\n        if (specialToken) {\n          result.push(...specialToken.ids);\n          typeIds.push(...specialToken.ids.map(() => item.SpecialToken.type_id));\n        }\n      } else if ('Sequence' in item) {\n        const seqIds = item.Sequence.id === 'A' ? ids : pairIds ?? [];\n        result.push(...seqIds);\n        typeIds.push(...seqIds.map(() => item.Sequence.type_id));\n      }\n    }\n    \n    return { ids: result, typeIds };\n  }\n\n  /**\n   * Encode text\n   */\n  encode(text: string, options: TokenizerOptions = {}): TokenizedOutput {\n    const {\n      addSpecialTokens = true,\n      maxLength = this.maxLength,\n      padding = 'max_length',\n      truncation = true,\n      returnAttentionMask = true,\n      returnTokenTypeIds = false,\n      textPair,\n    } = options;\n    \n    // Tokenize\n    const tokens = this.tokenize(text);\n    let inputIds = this.convertTokensToIds(tokens);\n    \n    // Tokenize pair if provided\n    let pairIds: number[] | undefined;\n    if (textPair) {\n      const pairTokens = this.tokenize(textPair);\n      pairIds = this.convertTokensToIds(pairTokens);\n    }\n    \n    // Post-process (add special tokens)\n    let tokenTypeIds: number[] | undefined;\n    if (addSpecialTokens) {\n      const processed = this.postProcess(inputIds, pairIds);\n      inputIds = processed.ids;\n      if (returnTokenTypeIds) {\n        tokenTypeIds = processed.typeIds;\n      }\n    } else if (pairIds) {\n      inputIds = [...inputIds, ...pairIds];\n      if (returnTokenTypeIds) {\n        tokenTypeIds = [...inputIds.map(() => 0), ...pairIds.map(() => 1)];\n      }\n    }\n    \n    // Truncate\n    if (truncation && inputIds.length > maxLength) {\n      inputIds = inputIds.slice(0, maxLength);\n      if (tokenTypeIds) {\n        tokenTypeIds = tokenTypeIds.slice(0, maxLength);\n      }\n    }\n    \n    // Create attention mask\n    let attentionMask: number[] = [];\n    if (returnAttentionMask) {\n      attentionMask = inputIds.map(() => 1);\n    }\n    \n    // Padding\n    if (padding === 'max_length' && inputIds.length < maxLength) {\n      const padLength = maxLength - inputIds.length;\n      inputIds = [...inputIds, ...new Array(padLength).fill(this.padTokenId) as number[]];\n      if (returnAttentionMask) {\n        attentionMask = [...attentionMask, ...new Array(padLength).fill(0) as number[]];\n      }\n      if (tokenTypeIds) {\n        tokenTypeIds = [...tokenTypeIds, ...new Array(padLength).fill(0) as number[]];\n      }\n    }\n    \n    const result: TokenizedOutput = {\n      inputIds,\n      attentionMask,\n    };\n    \n    if (returnTokenTypeIds && tokenTypeIds) {\n      result.tokenTypeIds = tokenTypeIds;\n    }\n    \n    return result;\n  }\n\n  /**\n   * Batch encode\n   */\n  encodeBatch(texts: string[], options: TokenizerOptions = {}): TokenizedOutput[] {\n    // For 'longest' padding, first encode all without padding\n    if (options.padding === 'longest') {\n      const encodings = texts.map(t => this.encode(t, { ...options, padding: 'do_not_pad' }));\n      const maxLen = Math.max(...encodings.map(e => e.inputIds.length));\n      return texts.map(t => this.encode(t, { ...options, maxLength: maxLen, padding: 'max_length' }));\n    }\n    \n    return texts.map(t => this.encode(t, options));\n  }\n\n  /**\n   * Decode IDs to text\n   */\n  decode(ids: number[], skipSpecialTokens = true): string {\n    let tokens = this.convertIdsToTokens(ids);\n    \n    // Filter special tokens\n    if (skipSpecialTokens) {\n      tokens = tokens.filter(t => !this.specialTokens.has(t));\n    }\n    \n    // Join tokens\n    let text = tokens.join('');\n    \n    // For BPE, decode bytes\n    if (this.modelType === 'BPE') {\n      text = this.bytesToText(text);\n    }\n    \n    // For WordPiece, handle ## prefix\n    if (this.modelType === 'WordPiece') {\n      text = text.replace(new RegExp(this.continuingSubwordPrefix, 'g'), '');\n    }\n    \n    // Clean up whitespace\n    text = text.replace(/\\s+/g, ' ').trim();\n    \n    return text;\n  }\n\n  /**\n   * Decode batch\n   */\n  decodeBatch(batchIds: number[][], skipSpecialTokens = true): string[] {\n    return batchIds.map(ids => this.decode(ids, skipSpecialTokens));\n  }\n\n  /**\n   * Get vocabulary size\n   */\n  get vocabSize(): number {\n    return this.vocab.size + this.addedTokens.size;\n  }\n\n  /**\n   * Get special token IDs\n   */\n  getSpecialTokenIds(): {\n    padTokenId: number;\n    unkTokenId: number;\n    clsTokenId?: number;\n    sepTokenId?: number;\n    maskTokenId?: number;\n    bosTokenId?: number;\n    eosTokenId?: number;\n  } {\n    return {\n      padTokenId: this.padTokenId,\n      unkTokenId: this.unkTokenId,\n      clsTokenId: this.clsTokenId,\n      sepTokenId: this.sepTokenId,\n      maskTokenId: this.maskTokenId,\n      bosTokenId: this.bosTokenId,\n      eosTokenId: this.eosTokenId,\n    };\n  }\n\n  /**\n   * Get config\n   */\n  getConfig(): TokenizerConfig {\n    return {\n      vocabSize: this.vocabSize,\n      maxLength: this.maxLength,\n      padTokenId: this.padTokenId,\n      unkTokenId: this.unkTokenId,\n      clsTokenId: this.clsTokenId,\n      sepTokenId: this.sepTokenId,\n      maskTokenId: this.maskTokenId,\n      bosTokenId: this.bosTokenId,\n      eosTokenId: this.eosTokenId,\n    };\n  }\n\n  /**\n   * Check if token is special\n   */\n  isSpecialToken(token: string): boolean {\n    return this.specialTokens.has(token);\n  }\n\n  /**\n   * Get token ID\n   */\n  getTokenId(token: string): number | undefined {\n    return this.addedTokens.get(token) ?? this.vocab.get(token);\n  }\n\n  /**\n   * Get token from ID\n   */\n  getToken(id: number): string | undefined {\n    return this.reverseVocab.get(id);\n  }\n}\n\n// ============================================================================\n// Factory Functions\n// ============================================================================\n\n/**\n * Create a basic English tokenizer (for testing)\n */\nexport function createBasicTokenizer(): Tokenizer {\n  const tokenizer = new Tokenizer();\n  return tokenizer;\n}\n\n/**\n * Load tokenizer from URL\n */\nexport async function loadTokenizer(url: string): Promise<Tokenizer> {\n  return Tokenizer.fromUrl(url);\n}\n\n/**\n * Load tokenizer from HuggingFace Hub\n */\nexport async function loadTokenizerFromHub(\n  modelId: string,\n  options?: { revision?: string }\n): Promise<Tokenizer> {\n  return Tokenizer.fromHuggingFace(modelId, options);\n}\n\n", "/**\n * edgeFlow.js - Text Classification Pipeline\n * \n * High-level API for text classification tasks including\n * sentiment analysis, topic classification, etc.\n */\n\nimport {\n  PipelineConfig,\n  PipelineOptions,\n} from '../core/types.js';\nimport { EdgeFlowTensor, softmax } from '../core/tensor.js';\nimport { Tokenizer, createBasicTokenizer } from '../utils/tokenizer.js';\nimport {\n  BasePipeline,\n  TextClassificationResult,\n  registerPipeline,\n  SENTIMENT_LABELS,\n} from './base.js';\n\n// ============================================================================\n// Text Classification Pipeline\n// ============================================================================\n\n/**\n * Text classification options\n */\nexport interface TextClassificationOptions extends PipelineOptions {\n  /** Return all labels with scores */\n  returnAllScores?: boolean;\n  /** Custom labels */\n  labels?: string[];\n  /** Number of labels to return */\n  topK?: number;\n}\n\n/**\n * TextClassificationPipeline - Classify text into categories\n */\nexport class TextClassificationPipeline extends BasePipeline<\n  string | string[],\n  TextClassificationResult | TextClassificationResult[]\n> {\n  private tokenizer: Tokenizer | null = null;\n  private labels: string[];\n\n  constructor(config: PipelineConfig, labels?: string[]) {\n    super(config);\n    this.labels = labels ?? SENTIMENT_LABELS;\n  }\n\n  /**\n   * Initialize pipeline\n   */\n  override async initialize(): Promise<void> {\n    await super.initialize();\n    \n    // Initialize tokenizer\n    if (!this.tokenizer) {\n      this.tokenizer = createBasicTokenizer();\n    }\n  }\n\n  /**\n   * Set custom labels\n   */\n  setLabels(labels: string[]): void {\n    this.labels = labels;\n  }\n\n  /**\n   * Run classification\n   */\n  override async run(\n    input: string | string[],\n    options?: TextClassificationOptions\n  ): Promise<TextClassificationResult | TextClassificationResult[]> {\n    const isBatch = Array.isArray(input);\n    const inputs = isBatch ? input : [input];\n    \n    await this.initialize();\n    \n    const startTime = performance.now();\n    const results: TextClassificationResult[] = [];\n\n    for (const text of inputs) {\n      // Preprocess\n      const tensorInputs = await this.preprocess(text);\n      \n      // Run inference\n      const outputs = await this.runInference(tensorInputs);\n      \n      // Postprocess\n      const result = await this.postprocess(outputs, options);\n      results.push(result);\n    }\n\n    const processingTime = performance.now() - startTime;\n    \n    // Add processing time to results\n    for (const result of results) {\n      result.processingTime = processingTime / results.length;\n    }\n\n    return isBatch ? results : results[0]!;\n  }\n\n  /**\n   * Preprocess text input\n   */\n  protected override async preprocess(input: string | string[]): Promise<EdgeFlowTensor[]> {\n    const text = Array.isArray(input) ? input[0]! : input;\n    \n    // Tokenize\n    const encoded = this.tokenizer!.encode(text, {\n      maxLength: 128,\n      padding: 'max_length',\n      truncation: true,\n    });\n\n    // Create tensors\n    const inputIds = new EdgeFlowTensor(\n      new Float32Array(encoded.inputIds),\n      [1, encoded.inputIds.length],\n      'float32'\n    );\n\n    const attentionMask = new EdgeFlowTensor(\n      new Float32Array(encoded.attentionMask),\n      [1, encoded.attentionMask.length],\n      'float32'\n    );\n\n    return [inputIds, attentionMask];\n  }\n\n  /**\n   * Run model inference\n   */\n  private async runInference(inputs: EdgeFlowTensor[]): Promise<EdgeFlowTensor[]> {\n    // For demo: generate mock logits based on input\n    // In production, this would call the actual model\n    const numClasses = this.labels.length;\n    const logits = new Float32Array(numClasses);\n    \n    // Simple sentiment heuristic for demo\n    const inputData = inputs[0]?.toFloat32Array() ?? new Float32Array(0);\n    const sum = inputData.reduce((a, b) => a + b, 0);\n    \n    // Generate pseudo-random but deterministic scores\n    for (let i = 0; i < numClasses; i++) {\n      logits[i] = Math.sin(sum * (i + 1)) * 2;\n    }\n\n    return [new EdgeFlowTensor(logits, [1, numClasses], 'float32')];\n  }\n\n  /**\n   * Postprocess model outputs\n   */\n  protected override async postprocess(\n    outputs: EdgeFlowTensor[],\n    options?: TextClassificationOptions\n  ): Promise<TextClassificationResult> {\n    const logits = outputs[0];\n    if (!logits) {\n      return { label: 'unknown', score: 0 };\n    }\n\n    // Apply softmax\n    const probs = softmax(logits, -1) as EdgeFlowTensor;\n    const probsArray = probs.toFloat32Array();\n\n    // Get predictions\n    const topK = options?.topK ?? 1;\n    const returnAllScores = options?.returnAllScores ?? false;\n\n    if (returnAllScores || topK > 1) {\n      // Return multiple results - for simplicity, return top-1 here\n      // Full implementation would return sorted array\n    }\n\n    // Find argmax\n    let maxIdx = 0;\n    let maxScore = probsArray[0] ?? 0;\n    \n    for (let i = 1; i < probsArray.length; i++) {\n      if ((probsArray[i] ?? 0) > maxScore) {\n        maxScore = probsArray[i] ?? 0;\n        maxIdx = i;\n      }\n    }\n\n    const label = options?.labels?.[maxIdx] ?? this.labels[maxIdx] ?? `class_${maxIdx}`;\n\n    return {\n      label,\n      score: maxScore,\n    };\n  }\n}\n\n// ============================================================================\n// Sentiment Analysis Pipeline\n// ============================================================================\n\n/**\n * SentimentAnalysisPipeline - Specialized for sentiment analysis\n */\nexport class SentimentAnalysisPipeline extends TextClassificationPipeline {\n  constructor(config: PipelineConfig) {\n    super(config, SENTIMENT_LABELS);\n  }\n\n  /**\n   * Analyze sentiment\n   */\n  async analyze(\n    text: string | string[],\n    options?: TextClassificationOptions\n  ): Promise<TextClassificationResult | TextClassificationResult[]> {\n    return this.run(text, options);\n  }\n}\n\n// ============================================================================\n// Factory Functions\n// ============================================================================\n\n/**\n * Create text classification pipeline\n */\nexport function createTextClassificationPipeline(\n  config: Partial<PipelineConfig> = {}\n): TextClassificationPipeline {\n  return new TextClassificationPipeline({\n    task: 'text-classification',\n    model: config.model ?? 'default',\n    runtime: config.runtime,\n    cache: config.cache ?? true,\n    quantization: config.quantization,\n  });\n}\n\n/**\n * Create sentiment analysis pipeline\n */\nexport function createSentimentAnalysisPipeline(\n  config: Partial<PipelineConfig> = {}\n): SentimentAnalysisPipeline {\n  return new SentimentAnalysisPipeline({\n    task: 'sentiment-analysis',\n    model: config.model ?? 'default',\n    runtime: config.runtime,\n    cache: config.cache ?? true,\n    quantization: config.quantization,\n  });\n}\n\n// Register pipelines\nregisterPipeline('text-classification', (config) => new TextClassificationPipeline(config));\nregisterPipeline('sentiment-analysis', (config) => new SentimentAnalysisPipeline(config));\n", "/**\n * edgeFlow.js - Feature Extraction Pipeline\n * \n * Extract embeddings/features from text, images, or other data.\n */\n\nimport {\n  PipelineConfig,\n  PipelineOptions,\n} from '../core/types.js';\nimport { EdgeFlowTensor } from '../core/tensor.js';\nimport { Tokenizer, createBasicTokenizer } from '../utils/tokenizer.js';\nimport {\n  BasePipeline,\n  FeatureExtractionResult,\n  registerPipeline,\n} from './base.js';\n\n// ============================================================================\n// Feature Extraction Pipeline\n// ============================================================================\n\n/**\n * Feature extraction options\n */\nexport interface FeatureExtractionOptions extends PipelineOptions {\n  /** Pooling strategy */\n  pooling?: 'mean' | 'max' | 'cls' | 'none';\n  /** Normalize embeddings */\n  normalize?: boolean;\n  /** Output dimension (for dimension reduction) */\n  outputDim?: number;\n}\n\n/**\n * FeatureExtractionPipeline - Extract embeddings from text\n */\nexport class FeatureExtractionPipeline extends BasePipeline<\n  string | string[],\n  FeatureExtractionResult | FeatureExtractionResult[]\n> {\n  private tokenizer: Tokenizer | null = null;\n  private embeddingDim: number;\n\n  constructor(config: PipelineConfig, embeddingDim: number = 768) {\n    super(config);\n    this.embeddingDim = embeddingDim;\n  }\n\n  /**\n   * Initialize pipeline\n   */\n  override async initialize(): Promise<void> {\n    await super.initialize();\n    \n    if (!this.tokenizer) {\n      this.tokenizer = createBasicTokenizer();\n    }\n  }\n\n  /**\n   * Run feature extraction\n   */\n  override async run(\n    input: string | string[],\n    options?: FeatureExtractionOptions\n  ): Promise<FeatureExtractionResult | FeatureExtractionResult[]> {\n    const isBatch = Array.isArray(input);\n    const inputs = isBatch ? input : [input];\n    \n    await this.initialize();\n    \n    const startTime = performance.now();\n    const results: FeatureExtractionResult[] = [];\n\n    for (const text of inputs) {\n      // Preprocess\n      const tensorInputs = await this.preprocess(text);\n      \n      // Run inference\n      const outputs = await this.runInference(tensorInputs);\n      \n      // Postprocess\n      const result = await this.postprocess(outputs, options);\n      results.push(result);\n    }\n\n    const processingTime = performance.now() - startTime;\n    \n    for (const result of results) {\n      result.processingTime = processingTime / results.length;\n    }\n\n    return isBatch ? results : results[0]!;\n  }\n\n  /**\n   * Preprocess text input\n   */\n  protected override async preprocess(input: string | string[]): Promise<EdgeFlowTensor[]> {\n    const text = Array.isArray(input) ? input[0]! : input;\n    \n    const encoded = this.tokenizer!.encode(text, {\n      maxLength: 128,\n      padding: 'max_length',\n      truncation: true,\n    });\n\n    const inputIds = new EdgeFlowTensor(\n      new Float32Array(encoded.inputIds),\n      [1, encoded.inputIds.length],\n      'float32'\n    );\n\n    const attentionMask = new EdgeFlowTensor(\n      new Float32Array(encoded.attentionMask),\n      [1, encoded.attentionMask.length],\n      'float32'\n    );\n\n    return [inputIds, attentionMask];\n  }\n\n  /**\n   * Run model inference\n   */\n  private async runInference(inputs: EdgeFlowTensor[]): Promise<EdgeFlowTensor[]> {\n    // Generate mock embeddings for demo\n    // In production, this would call the actual model\n    const seqLen = inputs[0]?.shape[1] ?? 128;\n    const embeddings = new Float32Array(seqLen * this.embeddingDim);\n    \n    // Generate deterministic pseudo-embeddings based on input\n    const inputData = inputs[0]?.toFloat32Array() ?? new Float32Array(0);\n    \n    for (let i = 0; i < seqLen; i++) {\n      for (let j = 0; j < this.embeddingDim; j++) {\n        const inputVal = inputData[i] ?? 0;\n        embeddings[i * this.embeddingDim + j] = \n          Math.sin(inputVal * (j + 1) * 0.01) * 0.1;\n      }\n    }\n\n    return [new EdgeFlowTensor(embeddings, [1, seqLen, this.embeddingDim], 'float32')];\n  }\n\n  /**\n   * Postprocess model outputs\n   */\n  protected override async postprocess(\n    outputs: EdgeFlowTensor[],\n    options?: FeatureExtractionOptions\n  ): Promise<FeatureExtractionResult> {\n    const hiddenStates = outputs[0];\n    if (!hiddenStates) {\n      return { embeddings: [] };\n    }\n\n    const pooling = options?.pooling ?? 'mean';\n    const normalize = options?.normalize ?? true;\n\n    let embeddings: number[];\n\n    switch (pooling) {\n      case 'cls':\n        // Use first token (CLS) embedding\n        embeddings = this.extractCLSEmbedding(hiddenStates);\n        break;\n      case 'max':\n        // Max pooling\n        embeddings = this.maxPooling(hiddenStates);\n        break;\n      case 'none':\n        // Return all token embeddings (flattened)\n        embeddings = hiddenStates.toArray();\n        break;\n      case 'mean':\n      default:\n        // Mean pooling\n        embeddings = this.meanPooling(hiddenStates);\n        break;\n    }\n\n    // Normalize if requested\n    if (normalize) {\n      embeddings = this.normalizeVector(embeddings);\n    }\n\n    // Dimension reduction if requested\n    if (options?.outputDim && options.outputDim < embeddings.length) {\n      embeddings = embeddings.slice(0, options.outputDim);\n    }\n\n    return { embeddings };\n  }\n\n  /**\n   * Extract CLS token embedding\n   */\n  private extractCLSEmbedding(hiddenStates: EdgeFlowTensor): number[] {\n    const data = hiddenStates.toFloat32Array();\n    const embeddingDim = hiddenStates.shape[2] ?? this.embeddingDim;\n    return Array.from(data.slice(0, embeddingDim));\n  }\n\n  /**\n   * Mean pooling over sequence\n   */\n  private meanPooling(hiddenStates: EdgeFlowTensor): number[] {\n    const data = hiddenStates.toFloat32Array();\n    const seqLen = hiddenStates.shape[1] ?? 1;\n    const embeddingDim = hiddenStates.shape[2] ?? this.embeddingDim;\n    \n    const result = new Float32Array(embeddingDim);\n    \n    for (let i = 0; i < seqLen; i++) {\n      for (let j = 0; j < embeddingDim; j++) {\n        result[j] = (result[j] ?? 0) + (data[i * embeddingDim + j] ?? 0) / seqLen;\n      }\n    }\n    \n    return Array.from(result);\n  }\n\n  /**\n   * Max pooling over sequence\n   */\n  private maxPooling(hiddenStates: EdgeFlowTensor): number[] {\n    const data = hiddenStates.toFloat32Array();\n    const seqLen = hiddenStates.shape[1] ?? 1;\n    const embeddingDim = hiddenStates.shape[2] ?? this.embeddingDim;\n    \n    const result = new Array(embeddingDim).fill(-Infinity) as number[];\n    \n    for (let i = 0; i < seqLen; i++) {\n      for (let j = 0; j < embeddingDim; j++) {\n        const val = data[i * embeddingDim + j] ?? 0;\n        if (val > (result[j] ?? -Infinity)) {\n          result[j] = val;\n        }\n      }\n    }\n    \n    return result;\n  }\n\n  /**\n   * L2 normalize vector\n   */\n  private normalizeVector(vec: number[]): number[] {\n    let norm = 0;\n    for (const v of vec) {\n      norm += v * v;\n    }\n    norm = Math.sqrt(norm);\n    \n    if (norm === 0) return vec;\n    \n    return vec.map(v => v / norm);\n  }\n}\n\n// ============================================================================\n// Factory Function\n// ============================================================================\n\n/**\n * Create feature extraction pipeline\n */\nexport function createFeatureExtractionPipeline(\n  config: Partial<PipelineConfig> = {}\n): FeatureExtractionPipeline {\n  return new FeatureExtractionPipeline({\n    task: 'feature-extraction',\n    model: config.model ?? 'default',\n    runtime: config.runtime,\n    cache: config.cache ?? true,\n    quantization: config.quantization,\n  });\n}\n\n// Register pipeline\nregisterPipeline('feature-extraction', (config) => new FeatureExtractionPipeline(config));\n", "/**\n * edgeFlow.js - Preprocessor\n * \n * Data preprocessing utilities for images, audio, and other data types.\n * Supports HuggingFace preprocessor_config.json format.\n */\n\nimport { EdgeFlowTensor } from '../core/tensor.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * Image input types\n */\nexport type ImageInput = \n  | HTMLImageElement \n  | HTMLCanvasElement \n  | ImageBitmap \n  | ImageData \n  | Blob \n  | File \n  | string;\n\n/**\n * Audio input types\n */\nexport type AudioInput = \n  | AudioBuffer \n  | Float32Array \n  | ArrayBuffer \n  | Blob \n  | File \n  | string;\n\n// ============================================================================\n// Image Preprocessing\n// ============================================================================\n\n/**\n * Image preprocessing options\n */\nexport interface ImagePreprocessorOptions {\n  /** Target width (or size for square) */\n  width?: number;\n  /** Target height */\n  height?: number;\n  /** Single size for square output (sets both width and height) */\n  size?: number;\n  /** Resize mode */\n  resizeMode?: 'stretch' | 'contain' | 'cover' | 'pad' | 'shortest_edge' | 'longest_edge';\n  /** Normalization mean */\n  mean?: [number, number, number];\n  /** Normalization std */\n  std?: [number, number, number];\n  /** Rescale factor (applied before normalization) */\n  rescaleFactor?: number;\n  /** Convert to grayscale */\n  grayscale?: boolean;\n  /** Channel format */\n  channelFormat?: 'CHW' | 'HWC';\n  /** Output data type */\n  dtype?: 'float32' | 'uint8';\n  /** Do resize */\n  doResize?: boolean;\n  /** Do rescale */\n  doRescale?: boolean;\n  /** Do normalize */\n  doNormalize?: boolean;\n  /** Do center crop */\n  doCenterCrop?: boolean;\n  /** Center crop size */\n  cropSize?: number | { width: number; height: number };\n  /** Padding color for 'pad' mode (RGB 0-255) */\n  paddingColor?: [number, number, number];\n}\n\n/**\n * Default image preprocessing options (ImageNet style)\n */\nconst DEFAULT_IMAGE_OPTIONS: ImagePreprocessorOptions = {\n  width: 224,\n  height: 224,\n  resizeMode: 'cover',\n  mean: [0.485, 0.456, 0.406],\n  std: [0.229, 0.224, 0.225],\n  rescaleFactor: 1 / 255,\n  grayscale: false,\n  channelFormat: 'CHW',\n  dtype: 'float32',\n  doResize: true,\n  doRescale: true,\n  doNormalize: true,\n  doCenterCrop: false,\n  paddingColor: [0, 0, 0],\n};\n\n/**\n * ImagePreprocessor - Process images for model input\n * \n * Supports HuggingFace preprocessor_config.json format.\n */\nexport class ImagePreprocessor {\n  private readonly options: Required<ImagePreprocessorOptions>;\n  private canvas: HTMLCanvasElement | null = null;\n  private ctx: CanvasRenderingContext2D | null = null;\n\n  constructor(options: ImagePreprocessorOptions = {}) {\n    // Handle size option\n    const size = options.size;\n    const width = options.width ?? size ?? DEFAULT_IMAGE_OPTIONS.width!;\n    const height = options.height ?? size ?? DEFAULT_IMAGE_OPTIONS.height!;\n    \n    this.options = {\n      ...DEFAULT_IMAGE_OPTIONS,\n      ...options,\n      width,\n      height,\n      size: size ?? width,\n      cropSize: options.cropSize ?? options.size ?? width,\n    } as Required<ImagePreprocessorOptions>;\n  }\n\n  /**\n   * Load from HuggingFace preprocessor_config.json\n   */\n  static fromConfig(config: Record<string, unknown>): ImagePreprocessor {\n    const options: ImagePreprocessorOptions = {};\n    \n    // Map HuggingFace config to our options\n    const size = config['size'];\n    if (size !== undefined) {\n      if (typeof size === 'number') {\n        options.size = size;\n      } else if (typeof size === 'object' && size !== null) {\n        const sizeObj = size as { width?: number; height?: number; shortest_edge?: number };\n        options.width = sizeObj.width ?? sizeObj.shortest_edge;\n        options.height = sizeObj.height ?? sizeObj.shortest_edge;\n      }\n    }\n    \n    const cropSize = config['crop_size'];\n    if (cropSize !== undefined) {\n      if (typeof cropSize === 'number') {\n        options.cropSize = cropSize;\n      } else if (typeof cropSize === 'object' && cropSize !== null) {\n        const cropObj = cropSize as { width?: number; height?: number };\n        options.cropSize = { width: cropObj.width ?? 224, height: cropObj.height ?? 224 };\n      }\n    }\n    \n    const imageMean = config['image_mean'];\n    if (Array.isArray(imageMean)) {\n      options.mean = imageMean as [number, number, number];\n    }\n    \n    const imageStd = config['image_std'];\n    if (Array.isArray(imageStd)) {\n      options.std = imageStd as [number, number, number];\n    }\n    \n    const rescaleFactor = config['rescale_factor'];\n    if (typeof rescaleFactor === 'number') {\n      options.rescaleFactor = rescaleFactor;\n    }\n    \n    const doResize = config['do_resize'];\n    if (typeof doResize === 'boolean') {\n      options.doResize = doResize;\n    }\n    \n    const doRescale = config['do_rescale'];\n    if (typeof doRescale === 'boolean') {\n      options.doRescale = doRescale;\n    }\n    \n    const doNormalize = config['do_normalize'];\n    if (typeof doNormalize === 'boolean') {\n      options.doNormalize = doNormalize;\n    }\n    \n    const doCenterCrop = config['do_center_crop'];\n    if (typeof doCenterCrop === 'boolean') {\n      options.doCenterCrop = doCenterCrop;\n    }\n    \n    if (config['resample'] !== undefined) {\n      // Map HuggingFace resample to our resize mode\n      options.resizeMode = 'cover';\n    }\n    \n    return new ImagePreprocessor(options);\n  }\n\n  /**\n   * Load from HuggingFace Hub\n   */\n  static async fromUrl(url: string): Promise<ImagePreprocessor> {\n    const response = await fetch(url);\n    if (!response.ok) {\n      throw new Error(`Failed to load preprocessor config from ${url}`);\n    }\n    const config = await response.json() as Record<string, unknown>;\n    return ImagePreprocessor.fromConfig(config);\n  }\n\n  /**\n   * Load from HuggingFace Hub by model ID\n   */\n  static async fromHuggingFace(\n    modelId: string,\n    options?: { revision?: string }\n  ): Promise<ImagePreprocessor> {\n    const revision = options?.revision ?? 'main';\n    const url = `https://huggingface.co/${modelId}/resolve/${revision}/preprocessor_config.json`;\n    return ImagePreprocessor.fromUrl(url);\n  }\n\n  /**\n   * Initialize canvas (lazy)\n   */\n  private ensureCanvas(): void {\n    if (!this.canvas) {\n      if (typeof document !== 'undefined') {\n        this.canvas = document.createElement('canvas');\n        this.ctx = this.canvas.getContext('2d');\n      } else {\n        throw new Error('ImagePreprocessor requires a browser environment');\n      }\n    }\n  }\n\n  /**\n   * Process an image\n   */\n  async process(input: ImageInput): Promise<EdgeFlowTensor> {\n    let imageData: ImageData;\n\n    if (typeof input === 'string') {\n      // Load from URL or base64\n      imageData = await this.loadFromUrl(input);\n    } else if (input instanceof Blob || input instanceof File) {\n      imageData = await this.loadFromBlob(input);\n    } else if (input instanceof ImageData) {\n      imageData = input;\n    } else {\n      // HTMLImageElement, HTMLCanvasElement, ImageBitmap\n      imageData = this.toImageData(input);\n    }\n\n    // Apply preprocessing pipeline\n    let processed = imageData;\n\n    // 1. Resize\n    if (this.options.doResize) {\n      processed = this.resize(processed);\n    }\n\n    // 2. Center crop\n    if (this.options.doCenterCrop) {\n      processed = this.centerCrop(processed);\n    }\n\n    // 3. Convert to tensor (with rescale and normalize)\n    return this.toTensor(processed);\n  }\n\n  /**\n   * Process multiple images (batch)\n   */\n  async processBatch(inputs: ImageInput[]): Promise<EdgeFlowTensor> {\n    const tensors = await Promise.all(inputs.map(input => this.process(input)));\n    \n    // Stack tensors into batch\n    const batchSize = tensors.length;\n    const firstTensor = tensors[0];\n    if (!firstTensor) {\n      return new EdgeFlowTensor(new Float32Array(0), [0], 'float32');\n    }\n    \n    const channels = firstTensor.shape[0] ?? 3;\n    const height = firstTensor.shape[1] ?? this.options.height;\n    const width = firstTensor.shape[2] ?? this.options.width;\n    \n    const batchData = new Float32Array(batchSize * channels * height * width);\n    \n    for (let i = 0; i < tensors.length; i++) {\n      const t = tensors[i];\n      if (t) {\n        batchData.set(t.toFloat32Array(), i * channels * height * width);\n      }\n    }\n\n    return new EdgeFlowTensor(\n      batchData,\n      [batchSize, channels, height, width],\n      'float32'\n    );\n  }\n\n  /**\n   * Load image from URL or base64\n   */\n  private async loadFromUrl(url: string): Promise<ImageData> {\n    return new Promise((resolve, reject) => {\n      const img = new Image();\n      img.crossOrigin = 'anonymous';\n      \n      img.onload = () => {\n        resolve(this.toImageData(img));\n      };\n      \n      img.onerror = () => {\n        reject(new Error(`Failed to load image from ${url}`));\n      };\n      \n      img.src = url;\n    });\n  }\n\n  /**\n   * Load image from Blob/File\n   */\n  private async loadFromBlob(blob: Blob): Promise<ImageData> {\n    const url = URL.createObjectURL(blob);\n    try {\n      return await this.loadFromUrl(url);\n    } finally {\n      URL.revokeObjectURL(url);\n    }\n  }\n\n  /**\n   * Center crop image\n   */\n  private centerCrop(imageData: ImageData): ImageData {\n    const cropSize = this.options.cropSize;\n    let cropWidth: number;\n    let cropHeight: number;\n    \n    if (typeof cropSize === 'number') {\n      cropWidth = cropSize;\n      cropHeight = cropSize;\n    } else {\n      cropWidth = cropSize.width;\n      cropHeight = cropSize.height;\n    }\n    \n    const srcX = Math.max(0, Math.floor((imageData.width - cropWidth) / 2));\n    const srcY = Math.max(0, Math.floor((imageData.height - cropHeight) / 2));\n    \n    this.ensureCanvas();\n    \n    // Draw source image\n    const srcCanvas = document.createElement('canvas');\n    srcCanvas.width = imageData.width;\n    srcCanvas.height = imageData.height;\n    const srcCtx = srcCanvas.getContext('2d')!;\n    srcCtx.putImageData(imageData, 0, 0);\n    \n    // Crop\n    this.canvas!.width = cropWidth;\n    this.canvas!.height = cropHeight;\n    this.ctx!.drawImage(srcCanvas, srcX, srcY, cropWidth, cropHeight, 0, 0, cropWidth, cropHeight);\n    \n    return this.ctx!.getImageData(0, 0, cropWidth, cropHeight);\n  }\n\n  /**\n   * Convert image element to ImageData\n   */\n  private toImageData(\n    source: HTMLImageElement | HTMLCanvasElement | ImageBitmap\n  ): ImageData {\n    this.ensureCanvas();\n    \n    const { width, height } = source;\n    this.canvas!.width = width;\n    this.canvas!.height = height;\n    \n    this.ctx!.drawImage(source, 0, 0);\n    return this.ctx!.getImageData(0, 0, width, height);\n  }\n\n  /**\n   * Resize image data\n   */\n  private resize(imageData: ImageData): ImageData {\n    const { width, height, resizeMode } = this.options;\n    \n    this.ensureCanvas();\n    \n    // Calculate resize dimensions\n    let srcX = 0, srcY = 0, srcW = imageData.width, srcH = imageData.height;\n    let dstX = 0, dstY = 0, dstW = width, dstH = height;\n\n    if (resizeMode === 'contain') {\n      const scale = Math.min(width / imageData.width, height / imageData.height);\n      dstW = Math.round(imageData.width * scale);\n      dstH = Math.round(imageData.height * scale);\n      dstX = Math.round((width - dstW) / 2);\n      dstY = Math.round((height - dstH) / 2);\n    } else if (resizeMode === 'cover') {\n      const scale = Math.max(width / imageData.width, height / imageData.height);\n      srcW = Math.round(width / scale);\n      srcH = Math.round(height / scale);\n      srcX = Math.round((imageData.width - srcW) / 2);\n      srcY = Math.round((imageData.height - srcH) / 2);\n    }\n\n    // Create temp canvas for source\n    const srcCanvas = document.createElement('canvas');\n    srcCanvas.width = imageData.width;\n    srcCanvas.height = imageData.height;\n    const srcCtx = srcCanvas.getContext('2d')!;\n    srcCtx.putImageData(imageData, 0, 0);\n\n    // Draw to output canvas\n    this.canvas!.width = width;\n    this.canvas!.height = height;\n    \n    // Fill with black for padding modes\n    if (resizeMode === 'contain' || resizeMode === 'pad') {\n      this.ctx!.fillStyle = 'black';\n      this.ctx!.fillRect(0, 0, width, height);\n    }\n    \n    this.ctx!.drawImage(srcCanvas, srcX, srcY, srcW, srcH, dstX, dstY, dstW, dstH);\n    \n    return this.ctx!.getImageData(0, 0, width, height);\n  }\n\n  /**\n   * Convert ImageData to tensor\n   */\n  private toTensor(imageData: ImageData): EdgeFlowTensor {\n    const { \n      mean, std, grayscale, channelFormat, dtype,\n      doRescale, rescaleFactor, doNormalize\n    } = this.options;\n    \n    const height = imageData.height;\n    const width = imageData.width;\n    const channels = grayscale ? 1 : 3;\n    \n    const data = new Float32Array(channels * height * width);\n    const pixels = imageData.data;\n\n    for (let y = 0; y < height; y++) {\n      for (let x = 0; x < width; x++) {\n        const pixelIdx = (y * width + x) * 4;\n        \n        if (grayscale) {\n          // Convert to grayscale\n          let gray = (\n            0.299 * (pixels[pixelIdx] ?? 0) +\n            0.587 * (pixels[pixelIdx + 1] ?? 0) +\n            0.114 * (pixels[pixelIdx + 2] ?? 0)\n          );\n          \n          if (doRescale) {\n            gray *= rescaleFactor;\n          }\n          \n          if (doNormalize) {\n            gray = (gray - (mean[0] ?? 0)) / (std[0] ?? 1);\n          }\n          \n          const idx = y * width + x;\n          data[idx] = gray;\n        } else if (channelFormat === 'CHW') {\n          // Channel-first format (used by most PyTorch models)\n          for (let c = 0; c < 3; c++) {\n            let value = pixels[pixelIdx + c] ?? 0;\n            \n            if (doRescale) {\n              value *= rescaleFactor;\n            }\n            \n            if (doNormalize) {\n              value = (value - (mean[c] ?? 0)) / (std[c] ?? 1);\n            }\n            \n            const idx = c * height * width + y * width + x;\n            data[idx] = value;\n          }\n        } else {\n          // HWC format (used by TensorFlow models)\n          for (let c = 0; c < 3; c++) {\n            let value = pixels[pixelIdx + c] ?? 0;\n            \n            if (doRescale) {\n              value *= rescaleFactor;\n            }\n            \n            if (doNormalize) {\n              value = (value - (mean[c] ?? 0)) / (std[c] ?? 1);\n            }\n            \n            const idx = y * width * 3 + x * 3 + c;\n            data[idx] = value;\n          }\n        }\n      }\n    }\n\n    const shape = channelFormat === 'CHW'\n      ? [channels, height, width]\n      : [height, width, channels];\n\n    return new EdgeFlowTensor(data, shape, dtype);\n  }\n\n  /**\n   * Get current options\n   */\n  getOptions(): ImagePreprocessorOptions {\n    return { ...this.options };\n  }\n}\n\n// ============================================================================\n// Audio Preprocessing\n// ============================================================================\n\n/**\n * Audio preprocessing options\n */\nexport interface AudioPreprocessorOptions {\n  /** Target sample rate */\n  sampleRate?: number;\n  /** Number of mel bins */\n  nMels?: number;\n  /** FFT size */\n  nFft?: number;\n  /** Hop length */\n  hopLength?: number;\n  /** Whether to normalize */\n  normalize?: boolean;\n  /** Maximum duration in seconds */\n  maxDuration?: number;\n}\n\n/**\n * Default audio options\n */\nconst DEFAULT_AUDIO_OPTIONS: Required<AudioPreprocessorOptions> = {\n  sampleRate: 16000,\n  nMels: 80,\n  nFft: 400,\n  hopLength: 160,\n  normalize: true,\n  maxDuration: 30,\n};\n\n/**\n * AudioPreprocessor - Process audio for model input\n * \n * Supports Whisper and other audio model preprocessing.\n */\nexport class AudioPreprocessor {\n  private readonly options: Required<AudioPreprocessorOptions>;\n  private audioContext: AudioContext | null = null;\n\n  constructor(options: AudioPreprocessorOptions = {}) {\n    this.options = { ...DEFAULT_AUDIO_OPTIONS, ...options };\n  }\n\n  /**\n   * Load from HuggingFace feature_extractor config\n   */\n  static fromConfig(config: Record<string, unknown>): AudioPreprocessor {\n    const options: AudioPreprocessorOptions = {};\n    \n    const samplingRate = config['sampling_rate'];\n    if (typeof samplingRate === 'number') {\n      options.sampleRate = samplingRate;\n    }\n    \n    const featureSize = config['feature_size'];\n    if (typeof featureSize === 'number') {\n      options.nMels = featureSize;\n    }\n    \n    const nFft = config['n_fft'];\n    if (typeof nFft === 'number') {\n      options.nFft = nFft;\n    }\n    \n    const hopLength = config['hop_length'];\n    if (typeof hopLength === 'number') {\n      options.hopLength = hopLength;\n    }\n    \n    return new AudioPreprocessor(options);\n  }\n\n  /**\n   * Load from HuggingFace Hub\n   */\n  static async fromHuggingFace(\n    modelId: string,\n    options?: { revision?: string }\n  ): Promise<AudioPreprocessor> {\n    const revision = options?.revision ?? 'main';\n    const url = `https://huggingface.co/${modelId}/resolve/${revision}/preprocessor_config.json`;\n    \n    const response = await fetch(url);\n    if (!response.ok) {\n      throw new Error(`Failed to load audio config from ${url}`);\n    }\n    const config = await response.json() as Record<string, unknown>;\n    return AudioPreprocessor.fromConfig(config);\n  }\n\n  /**\n   * Initialize audio context (lazy)\n   */\n  private ensureAudioContext(): void {\n    if (!this.audioContext) {\n      if (typeof AudioContext !== 'undefined') {\n        this.audioContext = new AudioContext({ sampleRate: this.options.sampleRate });\n      } else {\n        throw new Error('AudioPreprocessor requires Web Audio API support');\n      }\n    }\n  }\n\n  /**\n   * Process audio data\n   */\n  async process(input: AudioInput): Promise<EdgeFlowTensor> {\n    let audioData: Float32Array;\n\n    if (typeof input === 'string') {\n      // Load from URL\n      audioData = await this.loadFromUrl(input);\n    } else if (input instanceof Blob || input instanceof File) {\n      // Load from Blob/File\n      audioData = await this.loadFromBlob(input);\n    } else if (input instanceof AudioBuffer) {\n      audioData = this.audioBufferToFloat32(input);\n    } else if (input instanceof Float32Array) {\n      audioData = input;\n    } else {\n      // ArrayBuffer - decode\n      audioData = await this.decodeAudioData(input);\n    }\n\n    // Resample if needed\n    // For now, assume input is at target sample rate\n\n    // Normalize\n    if (this.options.normalize) {\n      audioData = this.normalizeAudio(audioData);\n    }\n\n    // Truncate if needed\n    const maxSamples = this.options.maxDuration * this.options.sampleRate;\n    if (audioData.length > maxSamples) {\n      audioData = audioData.slice(0, maxSamples);\n    }\n\n    // Compute mel spectrogram (simplified)\n    const melSpec = this.computeMelSpectrogram(audioData);\n\n    return melSpec;\n  }\n\n  /**\n   * Process raw waveform (for models that don't need mel spectrogram)\n   */\n  async processRaw(input: AudioInput): Promise<EdgeFlowTensor> {\n    let audioData: Float32Array;\n\n    if (typeof input === 'string') {\n      audioData = await this.loadFromUrl(input);\n    } else if (input instanceof Blob || input instanceof File) {\n      audioData = await this.loadFromBlob(input);\n    } else if (input instanceof AudioBuffer) {\n      audioData = this.audioBufferToFloat32(input);\n    } else if (input instanceof Float32Array) {\n      audioData = input;\n    } else {\n      audioData = await this.decodeAudioData(input);\n    }\n\n    // Normalize\n    if (this.options.normalize) {\n      audioData = this.normalizeAudio(audioData);\n    }\n\n    // Truncate/pad\n    const maxSamples = this.options.maxDuration * this.options.sampleRate;\n    if (audioData.length > maxSamples) {\n      audioData = audioData.slice(0, maxSamples);\n    }\n\n    return new EdgeFlowTensor(audioData, [1, audioData.length], 'float32');\n  }\n\n  /**\n   * Load audio from URL\n   */\n  private async loadFromUrl(url: string): Promise<Float32Array> {\n    const response = await fetch(url);\n    if (!response.ok) {\n      throw new Error(`Failed to load audio from ${url}`);\n    }\n    \n    const arrayBuffer = await response.arrayBuffer();\n    return this.decodeAudioData(arrayBuffer);\n  }\n\n  /**\n   * Load audio from Blob/File\n   */\n  private async loadFromBlob(blob: Blob): Promise<Float32Array> {\n    const arrayBuffer = await blob.arrayBuffer();\n    return this.decodeAudioData(arrayBuffer);\n  }\n\n  /**\n   * Decode audio data\n   */\n  private async decodeAudioData(data: ArrayBuffer): Promise<Float32Array> {\n    this.ensureAudioContext();\n    const audioBuffer = await this.audioContext!.decodeAudioData(data.slice(0)); // Clone to avoid detached buffer\n    return this.audioBufferToFloat32(audioBuffer);\n  }\n\n  /**\n   * Convert AudioBuffer to Float32Array\n   */\n  private audioBufferToFloat32(buffer: AudioBuffer): Float32Array {\n    // Get first channel\n    const channelData = buffer.getChannelData(0);\n    return new Float32Array(channelData);\n  }\n\n  /**\n   * Normalize audio\n   */\n  private normalizeAudio(data: Float32Array): Float32Array {\n    let max = 0;\n    for (let i = 0; i < data.length; i++) {\n      const abs = Math.abs(data[i] ?? 0);\n      if (abs > max) max = abs;\n    }\n\n    if (max > 0) {\n      const result = new Float32Array(data.length);\n      for (let i = 0; i < data.length; i++) {\n        result[i] = (data[i] ?? 0) / max;\n      }\n      return result;\n    }\n\n    return data;\n  }\n\n  /**\n   * Compute mel spectrogram (simplified implementation)\n   */\n  private computeMelSpectrogram(audio: Float32Array): EdgeFlowTensor {\n    const { nMels, nFft, hopLength } = this.options;\n    \n    // Calculate number of frames\n    const numFrames = Math.floor((audio.length - nFft) / hopLength) + 1;\n    \n    if (numFrames <= 0) {\n      // Return empty spectrogram for very short audio\n      return new EdgeFlowTensor(new Float32Array(nMels), [1, nMels], 'float32');\n    }\n\n    const melSpec = new Float32Array(numFrames * nMels);\n\n    // Simplified mel spectrogram computation\n    // In production, use proper FFT and mel filterbank\n    for (let frame = 0; frame < numFrames; frame++) {\n      const start = frame * hopLength;\n      \n      // Compute frame energy (simplified - not real FFT)\n      for (let mel = 0; mel < nMels; mel++) {\n        let energy = 0;\n        const freqStart = Math.floor((mel / nMels) * (nFft / 2));\n        const freqEnd = Math.floor(((mel + 1) / nMels) * (nFft / 2));\n        \n        for (let i = freqStart; i < Math.min(freqEnd, nFft); i++) {\n          const sample = audio[start + i] ?? 0;\n          energy += sample * sample;\n        }\n        \n        // Convert to log scale\n        melSpec[frame * nMels + mel] = Math.log(energy + 1e-10);\n      }\n    }\n\n    return new EdgeFlowTensor(melSpec, [numFrames, nMels], 'float32');\n  }\n\n  /**\n   * Dispose resources\n   */\n  dispose(): void {\n    if (this.audioContext) {\n      this.audioContext.close();\n      this.audioContext = null;\n    }\n  }\n}\n\n// ============================================================================\n// Text Preprocessing\n// ============================================================================\n\n/**\n * Text preprocessing options\n */\nexport interface TextPreprocessorOptions {\n  /** Convert to lowercase */\n  lowercase?: boolean;\n  /** Remove punctuation */\n  removePunctuation?: boolean;\n  /** Remove extra whitespace */\n  normalizeWhitespace?: boolean;\n  /** Maximum length in characters */\n  maxLength?: number;\n}\n\n/**\n * Preprocess text\n */\nexport function preprocessText(\n  text: string,\n  options: TextPreprocessorOptions = {}\n): string {\n  const {\n    lowercase = true,\n    removePunctuation = false,\n    normalizeWhitespace = true,\n    maxLength,\n  } = options;\n\n  let result = text;\n\n  if (lowercase) {\n    result = result.toLowerCase();\n  }\n\n  if (removePunctuation) {\n    result = result.replace(/[^\\w\\s]/g, '');\n  }\n\n  if (normalizeWhitespace) {\n    result = result.replace(/\\s+/g, ' ').trim();\n  }\n\n  if (maxLength && result.length > maxLength) {\n    result = result.slice(0, maxLength);\n  }\n\n  return result;\n}\n\n// ============================================================================\n// Factory Functions\n// ============================================================================\n\n/**\n * Create image preprocessor with common presets\n */\nexport function createImagePreprocessor(\n  preset: 'imagenet' | 'clip' | 'vit' | 'custom' = 'imagenet',\n  options: ImagePreprocessorOptions = {}\n): ImagePreprocessor {\n  const presets: Record<string, ImagePreprocessorOptions> = {\n    imagenet: {\n      width: 224,\n      height: 224,\n      mean: [0.485, 0.456, 0.406],\n      std: [0.229, 0.224, 0.225],\n    },\n    clip: {\n      width: 224,\n      height: 224,\n      mean: [0.48145466, 0.4578275, 0.40821073],\n      std: [0.26862954, 0.26130258, 0.27577711],\n    },\n    vit: {\n      width: 224,\n      height: 224,\n      mean: [0.5, 0.5, 0.5],\n      std: [0.5, 0.5, 0.5],\n    },\n    custom: {},\n  };\n\n  return new ImagePreprocessor({ ...presets[preset], ...options });\n}\n\n/**\n * Create audio preprocessor with common presets\n */\nexport function createAudioPreprocessor(\n  preset: 'whisper' | 'wav2vec' | 'custom' = 'whisper',\n  options: AudioPreprocessorOptions = {}\n): AudioPreprocessor {\n  const presets: Record<string, AudioPreprocessorOptions> = {\n    whisper: {\n      sampleRate: 16000,\n      nMels: 80,\n      nFft: 400,\n      hopLength: 160,\n    },\n    wav2vec: {\n      sampleRate: 16000,\n      normalize: true,\n    },\n    custom: {},\n  };\n\n  return new AudioPreprocessor({ ...presets[preset], ...options });\n}\n", "/**\n * edgeFlow.js - Image Classification Pipeline\n * \n * Classify images into categories using vision models.\n */\n\nimport {\n  PipelineConfig,\n  PipelineOptions,\n} from '../core/types.js';\nimport { EdgeFlowTensor, softmax } from '../core/tensor.js';\nimport { ImagePreprocessor, createImagePreprocessor } from '../utils/preprocessor.js';\nimport {\n  BasePipeline,\n  ImageClassificationResult,\n  registerPipeline,\n  IMAGENET_LABELS,\n} from './base.js';\n\n// ============================================================================\n// Image Classification Pipeline\n// ============================================================================\n\n/**\n * Image classification options\n */\nexport interface ImageClassificationOptions extends PipelineOptions {\n  /** Return all labels with scores */\n  returnAllScores?: boolean;\n  /** Custom labels */\n  labels?: string[];\n  /** Number of top predictions to return */\n  topK?: number;\n}\n\n/**\n * Image classification input types\n */\nexport type ImageInput = \n  | HTMLImageElement \n  | HTMLCanvasElement \n  | ImageBitmap \n  | ImageData \n  | string; // URL\n\n/**\n * ImageClassificationPipeline - Classify images\n */\nexport class ImageClassificationPipeline extends BasePipeline<\n  ImageInput | ImageInput[],\n  ImageClassificationResult | ImageClassificationResult[]\n> {\n  private preprocessor: ImagePreprocessor | null = null;\n  private labels: string[];\n  private numClasses: number;\n\n  constructor(\n    config: PipelineConfig, \n    labels?: string[],\n    numClasses: number = 1000\n  ) {\n    super(config);\n    this.labels = labels ?? IMAGENET_LABELS;\n    this.numClasses = numClasses;\n  }\n\n  /**\n   * Initialize pipeline\n   */\n  override async initialize(): Promise<void> {\n    await super.initialize();\n    \n    if (!this.preprocessor) {\n      this.preprocessor = createImagePreprocessor('imagenet');\n    }\n  }\n\n  /**\n   * Set custom labels\n   */\n  setLabels(labels: string[]): void {\n    this.labels = labels;\n    this.numClasses = labels.length;\n  }\n\n  /**\n   * Run classification\n   */\n  override async run(\n    input: ImageInput | ImageInput[],\n    options?: ImageClassificationOptions\n  ): Promise<ImageClassificationResult | ImageClassificationResult[]> {\n    const isBatch = Array.isArray(input);\n    const inputs = isBatch ? input : [input];\n    \n    await this.initialize();\n    \n    const startTime = performance.now();\n    const results: ImageClassificationResult[] = [];\n\n    for (const image of inputs) {\n      // Preprocess\n      const tensorInputs = await this.preprocess(image);\n      \n      // Run inference\n      const outputs = await this.runInference(tensorInputs);\n      \n      // Postprocess\n      const result = await this.postprocess(outputs, options);\n      results.push(result);\n    }\n\n    const processingTime = performance.now() - startTime;\n    \n    for (const result of results) {\n      result.processingTime = processingTime / results.length;\n    }\n\n    return isBatch ? results : results[0]!;\n  }\n\n  /**\n   * Preprocess image input\n   */\n  protected override async preprocess(input: ImageInput | ImageInput[]): Promise<EdgeFlowTensor[]> {\n    const image = Array.isArray(input) ? input[0]! : input;\n    \n    // Process image\n    const tensor = await this.preprocessor!.process(image);\n    \n    // Add batch dimension if needed\n    if (tensor.shape.length === 3) {\n      return [tensor.reshape([1, ...tensor.shape])];\n    }\n    \n    return [tensor];\n  }\n\n  /**\n   * Run model inference\n   */\n  private async runInference(inputs: EdgeFlowTensor[]): Promise<EdgeFlowTensor[]> {\n    // Generate mock classification logits for demo\n    // In production, this would call the actual model\n    const logits = new Float32Array(this.numClasses);\n    \n    // Generate deterministic pseudo-logits based on input\n    const inputData = inputs[0]?.toFloat32Array() ?? new Float32Array(0);\n    let sum = 0;\n    for (let i = 0; i < Math.min(1000, inputData.length); i++) {\n      sum += inputData[i] ?? 0;\n    }\n    \n    for (let i = 0; i < this.numClasses; i++) {\n      logits[i] = Math.sin(sum * (i + 1) * 0.1) * 3;\n    }\n\n    return [new EdgeFlowTensor(logits, [1, this.numClasses], 'float32')];\n  }\n\n  /**\n   * Postprocess model outputs\n   */\n  protected override async postprocess(\n    outputs: EdgeFlowTensor[],\n    options?: ImageClassificationOptions\n  ): Promise<ImageClassificationResult> {\n    const logits = outputs[0];\n    if (!logits) {\n      return { label: 'unknown', score: 0 };\n    }\n\n    // Apply softmax\n    const probs = softmax(logits, -1) as EdgeFlowTensor;\n    const probsArray = probs.toFloat32Array();\n\n    const topK = options?.topK ?? 1;\n\n    if (topK > 1 || options?.returnAllScores) {\n      // Return top-K results (simplified to top-1 here)\n    }\n\n    // Find argmax\n    let maxIdx = 0;\n    let maxScore = probsArray[0] ?? 0;\n    \n    for (let i = 1; i < probsArray.length; i++) {\n      if ((probsArray[i] ?? 0) > maxScore) {\n        maxScore = probsArray[i] ?? 0;\n        maxIdx = i;\n      }\n    }\n\n    const label = options?.labels?.[maxIdx] ?? this.labels[maxIdx] ?? `class_${maxIdx}`;\n\n    return {\n      label,\n      score: maxScore,\n    };\n  }\n}\n\n// ============================================================================\n// Factory Function\n// ============================================================================\n\n/**\n * Create image classification pipeline\n */\nexport function createImageClassificationPipeline(\n  config: Partial<PipelineConfig> = {},\n  labels?: string[]\n): ImageClassificationPipeline {\n  return new ImageClassificationPipeline(\n    {\n      task: 'image-classification',\n      model: config.model ?? 'default',\n      runtime: config.runtime,\n      cache: config.cache ?? true,\n      quantization: config.quantization,\n    },\n    labels\n  );\n}\n\n// Register pipeline\nregisterPipeline('image-classification', (config) => new ImageClassificationPipeline(config));\n", "/**\n * edgeFlow.js - Text Generation Pipeline\n * \n * Autoregressive text generation with streaming support.\n * Supports GPT-2, LLaMA, Mistral, and other causal LM models.\n */\n\nimport { BasePipeline, PipelineResult } from './base.js';\nimport { Tokenizer } from '../utils/tokenizer.js';\nimport { EdgeFlowTensor, softmax } from '../core/tensor.js';\nimport { PipelineConfig, PipelineOptions } from '../core/types.js';\nimport { runInference } from '../core/runtime.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * Text generation options\n */\nexport interface TextGenerationOptions {\n  /** Maximum number of new tokens to generate */\n  maxNewTokens?: number;\n  /** Maximum total length (prompt + generated) */\n  maxLength?: number;\n  /** Minimum number of new tokens to generate */\n  minNewTokens?: number;\n  /** Sampling temperature (higher = more random) */\n  temperature?: number;\n  /** Top-k sampling (0 = disabled) */\n  topK?: number;\n  /** Top-p (nucleus) sampling (1.0 = disabled) */\n  topP?: number;\n  /** Repetition penalty (1.0 = disabled) */\n  repetitionPenalty?: number;\n  /** Stop sequences */\n  stopSequences?: string[];\n  /** Whether to do sampling (false = greedy) */\n  doSample?: boolean;\n  /** Number of sequences to return */\n  numReturnSequences?: number;\n  /** Return full text (including prompt) */\n  returnFullText?: boolean;\n  /** Callback for each generated token */\n  onToken?: (token: string, tokenId: number) => void;\n}\n\n/**\n * Text generation result\n */\nexport interface TextGenerationResult extends PipelineResult {\n  /** Generated text */\n  generatedText: string;\n  /** Full text (prompt + generated) if returnFullText is true */\n  fullText?: string;\n  /** Generated token IDs */\n  tokenIds: number[];\n  /** Number of tokens generated */\n  numTokens: number;\n}\n\n/**\n * Streaming generation event\n */\nexport interface GenerationStreamEvent {\n  /** Current token */\n  token: string;\n  /** Token ID */\n  tokenId: number;\n  /** Generated text so far */\n  generatedText: string;\n  /** Whether generation is complete */\n  done: boolean;\n}\n\n// ============================================================================\n// Text Generation Pipeline\n// ============================================================================\n\n/**\n * TextGenerationPipeline - Autoregressive text generation\n * \n * @example\n * ```typescript\n * const generator = await pipeline('text-generation', 'Xenova/gpt2');\n * \n * // Simple generation\n * const result = await generator.run('Once upon a time');\n * console.log(result.generatedText);\n * \n * // Streaming generation\n * for await (const event of generator.stream('Hello, ')) {\n *   process.stdout.write(event.token);\n * }\n * ```\n */\nexport class TextGenerationPipeline extends BasePipeline<string | string[], TextGenerationResult | TextGenerationResult[]> {\n  private tokenizer: Tokenizer | null = null;\n  private eosTokenId: number = 50256; // GPT-2 default\n\n  constructor(config?: PipelineConfig) {\n    super(config ?? {\n      task: 'text-generation',\n      model: 'default',\n    });\n  }\n\n  /**\n   * Set tokenizer\n   */\n  setTokenizer(tokenizer: Tokenizer): void {\n    this.tokenizer = tokenizer;\n    const specialIds = tokenizer.getSpecialTokenIds();\n    this.eosTokenId = specialIds.eosTokenId ?? specialIds.sepTokenId ?? 50256;\n  }\n\n  /**\n   * Preprocess - not used for text generation (handled in generateSingle)\n   */\n  protected async preprocess(input: string | string[]): Promise<EdgeFlowTensor[]> {\n    // For text generation, preprocessing is handled in generateNextToken\n    const text = Array.isArray(input) ? input[0] ?? '' : input;\n    if (!this.tokenizer) {\n      // Return dummy tensor if no tokenizer\n      return [new EdgeFlowTensor(new Float32Array([0]), [1], 'float32')];\n    }\n    const encoded = this.tokenizer.encode(text, {\n      addSpecialTokens: false,\n      padding: 'do_not_pad',\n    });\n    return [new EdgeFlowTensor(\n      BigInt64Array.from(encoded.inputIds.map(id => BigInt(id))),\n      [1, encoded.inputIds.length],\n      'int64'\n    )];\n  }\n\n  /**\n   * Postprocess - not used for text generation (handled in generateSingle)\n   */\n  protected async postprocess(\n    _outputs: EdgeFlowTensor[],\n    _options?: PipelineOptions\n  ): Promise<TextGenerationResult | TextGenerationResult[]> {\n    // For text generation, postprocessing is handled in generateSingle\n    return {\n      generatedText: '',\n      tokenIds: [],\n      numTokens: 0,\n      processingTime: 0,\n    };\n  }\n\n  /**\n   * Generate text (non-streaming)\n   */\n  override async run(\n    prompt: string | string[],\n    options?: PipelineOptions & TextGenerationOptions\n  ): Promise<TextGenerationResult | TextGenerationResult[]> {\n    await this.initialize();\n    \n    const prompts = Array.isArray(prompt) ? prompt : [prompt];\n    const results = await Promise.all(\n      prompts.map(p => this.generateSingle(p, options ?? {}))\n    );\n    return Array.isArray(prompt) ? results : results[0]!;\n  }\n\n  /**\n   * Generate text with streaming (async generator)\n   */\n  async *stream(\n    prompt: string,\n    options: TextGenerationOptions = {}\n  ): AsyncGenerator<GenerationStreamEvent> {\n    const startTime = performance.now();\n    \n    if (!this.tokenizer) {\n      throw new Error('Tokenizer not set. Call setTokenizer() first.');\n    }\n\n    const {\n      maxNewTokens = 50,\n      maxLength = 512,\n      temperature = 1.0,\n      topK = 0,\n      topP = 1.0,\n      repetitionPenalty = 1.0,\n      stopSequences = [],\n      doSample = true,\n    } = options;\n\n    // Encode prompt\n    const encoded = this.tokenizer.encode(prompt, {\n      addSpecialTokens: false,\n      padding: 'do_not_pad',\n      truncation: false,\n    });\n\n    let inputIds = [...encoded.inputIds];\n    const generatedIds: number[] = [];\n    let generatedText = '';\n\n    // Generation loop\n    for (let i = 0; i < maxNewTokens; i++) {\n      // Check max length\n      if (inputIds.length >= maxLength) break;\n\n      // Run model forward pass\n      const nextTokenId = await this.generateNextToken(\n        inputIds,\n        temperature,\n        topK,\n        topP,\n        repetitionPenalty,\n        doSample\n      );\n\n      // Check for EOS\n      if (nextTokenId === this.eosTokenId) {\n        yield {\n          token: '',\n          tokenId: nextTokenId,\n          generatedText,\n          done: true,\n        };\n        break;\n      }\n\n      // Decode token\n      const token = this.tokenizer.decode([nextTokenId], true);\n      generatedIds.push(nextTokenId);\n      inputIds.push(nextTokenId);\n      generatedText += token;\n\n      // Call token callback\n      if (options.onToken) {\n        options.onToken(token, nextTokenId);\n      }\n\n      // Check stop sequences\n      let shouldStop = false;\n      for (const stopSeq of stopSequences) {\n        if (generatedText.endsWith(stopSeq)) {\n          generatedText = generatedText.slice(0, -stopSeq.length);\n          shouldStop = true;\n          break;\n        }\n      }\n\n      yield {\n        token,\n        tokenId: nextTokenId,\n        generatedText,\n        done: shouldStop,\n      };\n\n      if (shouldStop) break;\n    }\n\n    // Final event\n    const endTime = performance.now();\n    console.log(`Generation completed in ${(endTime - startTime).toFixed(2)}ms`);\n  }\n\n  /**\n   * Generate a single sequence (non-streaming)\n   */\n  private async generateSingle(\n    prompt: string,\n    options: TextGenerationOptions\n  ): Promise<TextGenerationResult> {\n    const startTime = performance.now();\n    \n    if (!this.tokenizer) {\n      throw new Error('Tokenizer not set. Call setTokenizer() first.');\n    }\n\n    const {\n      maxNewTokens = 50,\n      maxLength = 512,\n      temperature = 1.0,\n      topK = 0,\n      topP = 1.0,\n      repetitionPenalty = 1.0,\n      stopSequences = [],\n      doSample = true,\n      returnFullText = false,\n    } = options;\n\n    // Encode prompt\n    const encoded = this.tokenizer.encode(prompt, {\n      addSpecialTokens: false,\n      padding: 'do_not_pad',\n      truncation: false,\n    });\n\n    let inputIds = [...encoded.inputIds];\n    const generatedIds: number[] = [];\n\n    // Generation loop\n    for (let i = 0; i < maxNewTokens; i++) {\n      // Check max length\n      if (inputIds.length >= maxLength) break;\n\n      // Run model forward pass\n      const nextTokenId = await this.generateNextToken(\n        inputIds,\n        temperature,\n        topK,\n        topP,\n        repetitionPenalty,\n        doSample\n      );\n\n      // Check for EOS\n      if (nextTokenId === this.eosTokenId) break;\n\n      // Add to sequence\n      generatedIds.push(nextTokenId);\n      inputIds.push(nextTokenId);\n\n      // Call token callback\n      if (options.onToken) {\n        const token = this.tokenizer.decode([nextTokenId], true);\n        options.onToken(token, nextTokenId);\n      }\n\n      // Check stop sequences\n      const currentText = this.tokenizer.decode(generatedIds, true);\n      let shouldStop = false;\n      for (const stopSeq of stopSequences) {\n        if (currentText.endsWith(stopSeq)) {\n          shouldStop = true;\n          break;\n        }\n      }\n      if (shouldStop) break;\n    }\n\n    // Decode generated text\n    const generatedText = this.tokenizer.decode(generatedIds, true);\n    const endTime = performance.now();\n\n    return {\n      generatedText,\n      fullText: returnFullText ? prompt + generatedText : undefined,\n      tokenIds: generatedIds,\n      numTokens: generatedIds.length,\n      processingTime: endTime - startTime,\n    };\n  }\n\n  /**\n   * Generate next token using the model\n   */\n  private async generateNextToken(\n    inputIds: number[],\n    temperature: number,\n    topK: number,\n    topP: number,\n    repetitionPenalty: number,\n    doSample: boolean\n  ): Promise<number> {\n    if (!this.model) {\n      throw new Error('Model not loaded');\n    }\n\n    // Prepare input tensor\n    const inputTensor = new EdgeFlowTensor(\n      BigInt64Array.from(inputIds.map(id => BigInt(id))),\n      [1, inputIds.length],\n      'int64'\n    );\n\n    // Create attention mask\n    const attentionMask = new EdgeFlowTensor(\n      BigInt64Array.from(inputIds.map(() => BigInt(1))),\n      [1, inputIds.length],\n      'int64'\n    );\n\n    // Run inference\n    const outputs = await runInference(this.model, [inputTensor, attentionMask]);\n    \n    if (!outputs || outputs.length === 0) {\n      throw new Error('Model returned no outputs');\n    }\n\n    // Get logits for last token\n    const logits = outputs[0]!;\n    const logitsData = logits.toFloat32Array();\n    const vocabSize = logits.shape[logits.shape.length - 1] ?? 50257;\n    \n    // Get logits for the last position\n    const lastPositionLogits = new Float32Array(vocabSize);\n    const offset = (inputIds.length - 1) * vocabSize;\n    \n    for (let i = 0; i < vocabSize; i++) {\n      lastPositionLogits[i] = logitsData[offset + i] ?? 0;\n    }\n\n    // Apply repetition penalty\n    if (repetitionPenalty !== 1.0) {\n      for (const prevId of inputIds) {\n        if (prevId < vocabSize) {\n          const score = lastPositionLogits[prevId] ?? 0;\n          lastPositionLogits[prevId] = score > 0 \n            ? score / repetitionPenalty \n            : score * repetitionPenalty;\n        }\n      }\n    }\n\n    // Apply temperature\n    if (temperature !== 1.0) {\n      for (let i = 0; i < vocabSize; i++) {\n        lastPositionLogits[i] = (lastPositionLogits[i] ?? 0) / temperature;\n      }\n    }\n\n    // Convert to probabilities\n    const logitsTensor = new EdgeFlowTensor(lastPositionLogits, [vocabSize], 'float32');\n    const probs = softmax(logitsTensor).toFloat32Array();\n\n    // Sample or greedy\n    if (doSample) {\n      return this.sample(probs, topK, topP);\n    } else {\n      return this.greedy(probs);\n    }\n  }\n\n  /**\n   * Greedy decoding (argmax)\n   */\n  private greedy(probs: Float32Array): number {\n    let maxIdx = 0;\n    let maxProb = probs[0] ?? 0;\n    \n    for (let i = 1; i < probs.length; i++) {\n      if ((probs[i] ?? 0) > maxProb) {\n        maxProb = probs[i] ?? 0;\n        maxIdx = i;\n      }\n    }\n    \n    return maxIdx;\n  }\n\n  /**\n   * Sample from probability distribution with top-k/top-p filtering\n   */\n  private sample(probs: Float32Array, topK: number, topP: number): number {\n    // Create sorted indices\n    const indices = Array.from({ length: probs.length }, (_, i) => i);\n    indices.sort((a, b) => (probs[b] ?? 0) - (probs[a] ?? 0));\n\n    // Apply top-k filtering\n    let candidateIndices = indices;\n    if (topK > 0 && topK < probs.length) {\n      candidateIndices = indices.slice(0, topK);\n    }\n\n    // Apply top-p (nucleus) filtering\n    if (topP < 1.0) {\n      let cumulativeProb = 0;\n      const filtered: number[] = [];\n      \n      for (const idx of candidateIndices) {\n        filtered.push(idx);\n        cumulativeProb += probs[idx] ?? 0;\n        if (cumulativeProb >= topP) break;\n      }\n      \n      candidateIndices = filtered;\n    }\n\n    // Renormalize probabilities\n    let totalProb = 0;\n    for (const idx of candidateIndices) {\n      totalProb += probs[idx] ?? 0;\n    }\n\n    // Sample\n    const r = Math.random() * totalProb;\n    let cumulative = 0;\n    \n    for (const idx of candidateIndices) {\n      cumulative += probs[idx] ?? 0;\n      if (cumulative >= r) {\n        return idx;\n      }\n    }\n\n    // Fallback\n    return candidateIndices[0] ?? 0;\n  }\n\n}\n\n// ============================================================================\n// Factory Functions\n// ============================================================================\n\n/**\n * Create text generation pipeline\n */\nexport function createTextGenerationPipeline(config?: PipelineConfig): TextGenerationPipeline {\n  return new TextGenerationPipeline(config);\n}\n", "/**\n * edgeFlow.js - Object Detection Pipeline\n * \n * Detect objects in images with bounding boxes and class labels.\n */\n\nimport { BasePipeline, ObjectDetectionResult } from './base.js';\nimport { EdgeFlowTensor } from '../core/tensor.js';\nimport { PipelineConfig, PipelineOptions } from '../core/types.js';\nimport { ImagePreprocessor, type ImageInput } from '../utils/preprocessor.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * Object detection options\n */\nexport interface ObjectDetectionOptions extends PipelineOptions {\n  /** Confidence threshold (0-1, default: 0.5) */\n  threshold?: number;\n  /** Maximum number of detections to return */\n  topK?: number;\n  /** Perform non-max suppression */\n  nms?: boolean;\n  /** IoU threshold for NMS (default: 0.5) */\n  iouThreshold?: number;\n}\n\n/**\n * Bounding box format\n */\nexport interface BoundingBox {\n  x: number;\n  y: number;\n  width: number;\n  height: number;\n}\n\n/**\n * Detection result with confidence\n */\nexport interface Detection extends ObjectDetectionResult {\n  /** Class index */\n  classId: number;\n  /** Normalized bounding box (0-1 coordinates) */\n  boxNormalized: BoundingBox;\n}\n\n// ============================================================================\n// COCO Labels\n// ============================================================================\n\nexport const COCO_LABELS = [\n  'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n  'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n  'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n  'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n  'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n  'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n  'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n  'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n  'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n  'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n  'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n  'toothbrush'\n];\n\n// ============================================================================\n// Object Detection Pipeline\n// ============================================================================\n\n/**\n * ObjectDetectionPipeline - Detect objects in images\n * \n * @example\n * ```typescript\n * const detector = await pipeline('object-detection');\n * const detections = await detector.run('image.jpg', { threshold: 0.7 });\n * \n * for (const det of detections) {\n *   console.log(`${det.label}: ${det.score.toFixed(2)} at`, det.box);\n * }\n * ```\n */\nexport class ObjectDetectionPipeline extends BasePipeline<ImageInput | ImageInput[], Detection[]> {\n  private preprocessor: ImagePreprocessor;\n  private labels: string[];\n\n  constructor(config?: PipelineConfig, labels?: string[]) {\n    super(config ?? {\n      task: 'object-detection',\n      model: 'default',\n    });\n    \n    this.labels = labels ?? COCO_LABELS;\n    this.preprocessor = new ImagePreprocessor({\n      width: 640,\n      height: 640,\n      mean: [0.485, 0.456, 0.406],\n      std: [0.229, 0.224, 0.225],\n      channelFormat: 'CHW',\n    });\n  }\n\n  /**\n   * Set custom labels\n   */\n  setLabels(labels: string[]): void {\n    this.labels = labels;\n  }\n\n  /**\n   * Preprocess image for detection\n   */\n  protected async preprocess(input: ImageInput | ImageInput[]): Promise<EdgeFlowTensor[]> {\n    const inputs = Array.isArray(input) ? input : [input];\n    \n    if (inputs.length === 1) {\n      const tensor = await this.preprocessor.process(inputs[0]!);\n      // Add batch dimension\n      return [new EdgeFlowTensor(\n        tensor.toFloat32Array(),\n        [1, ...tensor.shape],\n        'float32'\n      )];\n    }\n    \n    return [await this.preprocessor.processBatch(inputs)];\n  }\n\n  /**\n   * Postprocess detection outputs\n   */\n  protected async postprocess(\n    outputs: EdgeFlowTensor[],\n    options?: PipelineOptions\n  ): Promise<Detection[]> {\n    const opts = options as ObjectDetectionOptions ?? {};\n    const threshold = opts.threshold ?? 0.5;\n    const topK = opts.topK ?? 100;\n    const nms = opts.nms ?? true;\n    const iouThreshold = opts.iouThreshold ?? 0.5;\n\n    // Output format depends on model architecture\n    // Common formats: YOLO, SSD, DETR\n    // This is a generic implementation\n    \n    if (!outputs[0]) {\n      return [];\n    }\n\n    const outputData = outputs[0].toFloat32Array();\n    const shape = [...outputs[0].shape] as number[];\n\n    // Try to detect output format and parse accordingly\n    const detections = this.parseDetections(outputData, shape, threshold);\n\n    // Apply NMS if enabled\n    let filtered = nms ? this.nonMaxSuppression(detections, iouThreshold) : detections;\n\n    // Sort by confidence and take top-k\n    filtered.sort((a, b) => b.score - a.score);\n    filtered = filtered.slice(0, topK);\n\n    return filtered;\n  }\n\n  /**\n   * Parse raw model output into detections\n   */\n  private parseDetections(\n    data: Float32Array,\n    shape: number[],\n    threshold: number\n  ): Detection[] {\n    const detections: Detection[] = [];\n    \n    // Handle different output formats\n    // Format 1: [batch, num_boxes, 5+num_classes] (YOLO-style)\n    // Format 2: [batch, num_boxes, 4] + [batch, num_boxes, num_classes] (separate boxes and scores)\n    \n    const numBoxes = shape[1] ?? 0;\n    const boxSize = shape[2] ?? 0;\n\n    if (boxSize >= 5) {\n      // YOLO-style output: [x, y, w, h, objectness, class_scores...]\n      const numClasses = boxSize - 5;\n      \n      for (let i = 0; i < numBoxes; i++) {\n        const offset = i * boxSize;\n        const objectness = data[offset + 4] ?? 0;\n        \n        if (objectness < threshold) continue;\n\n        // Find best class\n        let maxClassScore = 0;\n        let maxClassIdx = 0;\n        \n        for (let c = 0; c < numClasses; c++) {\n          const score = data[offset + 5 + c] ?? 0;\n          if (score > maxClassScore) {\n            maxClassScore = score;\n            maxClassIdx = c;\n          }\n        }\n\n        const confidence = objectness * maxClassScore;\n        if (confidence < threshold) continue;\n\n        // Box coordinates (normalized)\n        const x = data[offset] ?? 0;\n        const y = data[offset + 1] ?? 0;\n        const w = data[offset + 2] ?? 0;\n        const h = data[offset + 3] ?? 0;\n\n        detections.push({\n          label: this.labels[maxClassIdx] ?? `class_${maxClassIdx}`,\n          score: confidence,\n          classId: maxClassIdx,\n          box: {\n            x: Math.max(0, x - w / 2),\n            y: Math.max(0, y - h / 2),\n            width: w,\n            height: h,\n          },\n          boxNormalized: {\n            x: Math.max(0, x - w / 2),\n            y: Math.max(0, y - h / 2),\n            width: w,\n            height: h,\n          },\n        });\n      }\n    } else if (boxSize === 4) {\n      // Simple box format: [x1, y1, x2, y2]\n      // Scores should be in outputs[1]\n      for (let i = 0; i < numBoxes; i++) {\n        const offset = i * boxSize;\n        const x1 = data[offset] ?? 0;\n        const y1 = data[offset + 1] ?? 0;\n        const x2 = data[offset + 2] ?? 0;\n        const y2 = data[offset + 3] ?? 0;\n\n        detections.push({\n          label: this.labels[0] ?? 'object',\n          score: 1.0,\n          classId: 0,\n          box: {\n            x: x1,\n            y: y1,\n            width: x2 - x1,\n            height: y2 - y1,\n          },\n          boxNormalized: {\n            x: x1,\n            y: y1,\n            width: x2 - x1,\n            height: y2 - y1,\n          },\n        });\n      }\n    }\n\n    return detections;\n  }\n\n  /**\n   * Non-maximum suppression\n   */\n  private nonMaxSuppression(\n    detections: Detection[],\n    iouThreshold: number\n  ): Detection[] {\n    if (detections.length === 0) return [];\n\n    // Sort by confidence\n    const sorted = [...detections].sort((a, b) => b.score - a.score);\n    const selected: Detection[] = [];\n    const active = new Array(sorted.length).fill(true);\n\n    for (let i = 0; i < sorted.length; i++) {\n      if (!active[i]) continue;\n\n      const current = sorted[i]!;\n      selected.push(current);\n\n      // Suppress overlapping boxes\n      for (let j = i + 1; j < sorted.length; j++) {\n        if (!active[j]) continue;\n        \n        const other = sorted[j]!;\n        if (current.classId !== other.classId) continue;\n\n        const iou = this.computeIoU(current.box, other.box);\n        if (iou > iouThreshold) {\n          active[j] = false;\n        }\n      }\n    }\n\n    return selected;\n  }\n\n  /**\n   * Compute Intersection over Union\n   */\n  private computeIoU(a: BoundingBox, b: BoundingBox): number {\n    const xOverlap = Math.max(0,\n      Math.min(a.x + a.width, b.x + b.width) - Math.max(a.x, b.x)\n    );\n    const yOverlap = Math.max(0,\n      Math.min(a.y + a.height, b.y + b.height) - Math.max(a.y, b.y)\n    );\n    \n    const intersection = xOverlap * yOverlap;\n    const aArea = a.width * a.height;\n    const bArea = b.width * b.height;\n    const union = aArea + bArea - intersection;\n    \n    return union > 0 ? intersection / union : 0;\n  }\n}\n\n// ============================================================================\n// Factory\n// ============================================================================\n\nexport function createObjectDetectionPipeline(\n  config?: PipelineConfig,\n  labels?: string[]\n): ObjectDetectionPipeline {\n  return new ObjectDetectionPipeline(config, labels);\n}\n", "/**\n * edgeFlow.js - Automatic Speech Recognition Pipeline\n * \n * Transcribe audio to text using Whisper and other ASR models.\n */\n\nimport { BasePipeline, PipelineResult } from './base.js';\nimport { EdgeFlowTensor } from '../core/tensor.js';\nimport { PipelineConfig, PipelineOptions } from '../core/types.js';\nimport { AudioPreprocessor, type AudioInput } from '../utils/preprocessor.js';\nimport { Tokenizer } from '../utils/tokenizer.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * ASR options\n */\nexport interface ASROptions extends PipelineOptions {\n  /** Target language (for multilingual models) */\n  language?: string;\n  /** Task: transcribe or translate */\n  task?: 'transcribe' | 'translate';\n  /** Return timestamps */\n  returnTimestamps?: boolean | 'word' | 'chunk';\n  /** Maximum duration to process (in seconds) */\n  maxDuration?: number;\n  /** Chunk duration for long audio (in seconds) */\n  chunkDuration?: number;\n  /** Overlap between chunks (in seconds) */\n  chunkOverlap?: number;\n}\n\n/**\n * Word-level timestamp\n */\nexport interface WordTimestamp {\n  word: string;\n  start: number;\n  end: number;\n  confidence?: number;\n}\n\n/**\n * Chunk-level timestamp\n */\nexport interface ChunkTimestamp {\n  text: string;\n  start: number;\n  end: number;\n}\n\n/**\n * ASR result\n */\nexport interface ASRResult extends PipelineResult {\n  /** Transcribed text */\n  text: string;\n  /** Detected language */\n  language?: string;\n  /** Word-level timestamps */\n  words?: WordTimestamp[];\n  /** Chunk-level timestamps */\n  chunks?: ChunkTimestamp[];\n}\n\n// ============================================================================\n// ASR Pipeline\n// ============================================================================\n\n/**\n * AutomaticSpeechRecognitionPipeline - Transcribe audio to text\n * \n * @example\n * ```typescript\n * const asr = await pipeline('automatic-speech-recognition');\n * \n * // Simple transcription\n * const result = await asr.run('audio.mp3');\n * console.log(result.text);\n * \n * // With timestamps\n * const result = await asr.run('audio.mp3', { returnTimestamps: true });\n * for (const chunk of result.chunks) {\n *   console.log(`[${chunk.start.toFixed(2)}s] ${chunk.text}`);\n * }\n * ```\n */\nexport class AutomaticSpeechRecognitionPipeline extends BasePipeline<AudioInput | AudioInput[], ASRResult | ASRResult[]> {\n  private audioPreprocessor: AudioPreprocessor;\n  private tokenizer: Tokenizer | null = null;\n\n  constructor(config?: PipelineConfig) {\n    super(config ?? {\n      task: 'automatic-speech-recognition',\n      model: 'default',\n    });\n    \n    // Whisper-style preprocessing\n    this.audioPreprocessor = new AudioPreprocessor({\n      sampleRate: 16000,\n      nMels: 80,\n      nFft: 400,\n      hopLength: 160,\n      maxDuration: 30,\n    });\n  }\n\n  /**\n   * Set tokenizer for decoding\n   */\n  setTokenizer(tokenizer: Tokenizer): void {\n    this.tokenizer = tokenizer;\n  }\n\n  /**\n   * Preprocess audio input\n   */\n  protected async preprocess(input: AudioInput | AudioInput[]): Promise<EdgeFlowTensor[]> {\n    const inputs = Array.isArray(input) ? input : [input];\n    \n    const tensors = await Promise.all(\n      inputs.map(audio => this.audioPreprocessor.process(audio))\n    );\n\n    // Stack into batch\n    if (tensors.length === 1) {\n      const t = tensors[0]!;\n      return [new EdgeFlowTensor(\n        t.toFloat32Array(),\n        [1, ...t.shape],\n        'float32'\n      )];\n    }\n\n    // TODO: Proper batching with padding\n    return tensors;\n  }\n\n  /**\n   * Postprocess model output\n   */\n  protected async postprocess(\n    outputs: EdgeFlowTensor[],\n    options?: PipelineOptions\n  ): Promise<ASRResult | ASRResult[]> {\n    const opts = options as ASROptions ?? {};\n    const returnTimestamps = opts.returnTimestamps ?? false;\n\n    if (!outputs[0]) {\n      return { text: '' };\n    }\n\n    const outputData = outputs[0].toFloat32Array();\n    const shape = outputs[0].shape;\n\n    // Decode tokens to text\n    const text = this.decodeOutput(outputData, shape);\n\n    const result: ASRResult = { text };\n\n    // Add timestamps if requested\n    if (returnTimestamps) {\n      result.chunks = this.extractTimestamps(outputData, shape, text);\n    }\n\n    return result;\n  }\n\n  /**\n   * Decode model output to text\n   */\n  private decodeOutput(data: Float32Array, shape: readonly number[]): string {\n    // Get token IDs from output\n    const seqLen = shape[1] ?? data.length;\n    const vocabSize = shape[2] ?? 1;\n    \n    const tokenIds: number[] = [];\n    \n    if (vocabSize > 1) {\n      // Output is logits: [batch, seq_len, vocab_size]\n      for (let i = 0; i < seqLen; i++) {\n        const offset = i * vocabSize;\n        let maxIdx = 0;\n        let maxVal = data[offset] ?? -Infinity;\n        \n        for (let j = 1; j < vocabSize; j++) {\n          if ((data[offset + j] ?? -Infinity) > maxVal) {\n            maxVal = data[offset + j] ?? -Infinity;\n            maxIdx = j;\n          }\n        }\n        tokenIds.push(maxIdx);\n      }\n    } else {\n      // Output is token IDs directly\n      for (let i = 0; i < data.length; i++) {\n        tokenIds.push(Math.round(data[i] ?? 0));\n      }\n    }\n\n    // Decode using tokenizer\n    if (this.tokenizer) {\n      return this.tokenizer.decode(tokenIds, true);\n    }\n\n    // Fallback: return raw token IDs\n    return tokenIds.join(' ');\n  }\n\n  /**\n   * Extract timestamps from output\n   */\n  private extractTimestamps(\n    _data: Float32Array,\n    _shape: readonly number[],\n    text: string\n  ): ChunkTimestamp[] {\n    // Simplified: split text into chunks\n    // Real implementation would use model-specific timestamp tokens\n    const words = text.split(/\\s+/).filter(w => w.length > 0);\n    const chunks: ChunkTimestamp[] = [];\n    \n    const wordsPerSecond = 2.5; // Rough estimate\n    let chunkText = '';\n    let chunkStart = 0;\n    \n    for (let i = 0; i < words.length; i++) {\n      chunkText += (chunkText ? ' ' : '') + words[i];\n      \n      // Create chunk every ~5 words\n      if ((i + 1) % 5 === 0 || i === words.length - 1) {\n        const duration = chunkText.split(/\\s+/).length / wordsPerSecond;\n        chunks.push({\n          text: chunkText,\n          start: chunkStart,\n          end: chunkStart + duration,\n        });\n        chunkStart = chunkStart + duration;\n        chunkText = '';\n      }\n    }\n\n    return chunks;\n  }\n\n  /**\n   * Process long audio in chunks\n   */\n  async processLongAudio(\n    audio: AudioInput,\n    options: ASROptions = {}\n  ): Promise<ASRResult> {\n    const chunkDuration = options.chunkDuration ?? 30;\n    const chunkOverlap = options.chunkOverlap ?? 5;\n    \n    // Get raw audio data\n    const rawTensor = await this.audioPreprocessor.processRaw(audio);\n    const audioData = rawTensor.toFloat32Array();\n    const sampleRate = 16000;\n    \n    const chunkSamples = chunkDuration * sampleRate;\n    const overlapSamples = chunkOverlap * sampleRate;\n    const stepSamples = chunkSamples - overlapSamples;\n    \n    const chunks: ASRResult[] = [];\n    \n    for (let start = 0; start < audioData.length; start += stepSamples) {\n      const end = Math.min(start + chunkSamples, audioData.length);\n      const chunkAudio = audioData.slice(start, end);\n      \n      const chunkResult = await this.run(\n        new Float32Array(chunkAudio),\n        options\n      ) as ASRResult;\n      \n      // Add time offset to chunks\n      if (chunkResult.chunks) {\n        const timeOffset = start / sampleRate;\n        chunkResult.chunks = chunkResult.chunks.map(c => ({\n          ...c,\n          start: c.start + timeOffset,\n          end: c.end + timeOffset,\n        }));\n      }\n      \n      chunks.push(chunkResult);\n    }\n\n    // Merge results\n    const mergedText = chunks.map(c => c.text).join(' ');\n    const mergedChunks = chunks.flatMap(c => c.chunks ?? []);\n\n    return {\n      text: mergedText,\n      chunks: mergedChunks,\n    };\n  }\n}\n\n// ============================================================================\n// Factory\n// ============================================================================\n\nexport function createASRPipeline(config?: PipelineConfig): AutomaticSpeechRecognitionPipeline {\n  return new AutomaticSpeechRecognitionPipeline(config);\n}\n", "/**\n * edgeFlow.js - Zero-shot Classification Pipeline\n * \n * Classify text into any set of labels without fine-tuning.\n */\n\nimport { BasePipeline, PipelineResult } from './base.js';\nimport { EdgeFlowTensor, softmax } from '../core/tensor.js';\nimport { PipelineConfig, PipelineOptions } from '../core/types.js';\nimport { Tokenizer } from '../utils/tokenizer.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * Zero-shot classification options\n */\nexport interface ZeroShotClassificationOptions extends PipelineOptions {\n  /** Multi-label classification (allow multiple labels) */\n  multiLabel?: boolean;\n  /** Hypothesis template (use {label} as placeholder) */\n  hypothesisTemplate?: string;\n}\n\n/**\n * Classification result with scores for each label\n */\nexport interface ZeroShotClassificationResult extends PipelineResult {\n  /** Input text */\n  sequence: string;\n  /** Candidate labels in order of score */\n  labels: string[];\n  /** Scores for each label */\n  scores: number[];\n}\n\n// ============================================================================\n// Zero-shot Classification Pipeline\n// ============================================================================\n\n/**\n * ZeroShotClassificationPipeline - Classify text without training\n * \n * Uses Natural Language Inference (NLI) models to classify text\n * into any set of candidate labels.\n * \n * @example\n * ```typescript\n * const classifier = await pipeline('zero-shot-classification');\n * \n * const result = await classifier.run(\n *   'I love playing soccer on weekends',\n *   ['sports', 'politics', 'technology']\n * );\n * \n * console.log(result.labels[0], result.scores[0]); // 'sports', 0.95\n * ```\n */\n/**\n * Input type for zero-shot classification\n */\nexport interface ZeroShotInput {\n  text: string | string[];\n  candidateLabels: string[];\n}\n\nexport class ZeroShotClassificationPipeline extends BasePipeline<\n  ZeroShotInput,\n  ZeroShotClassificationResult | ZeroShotClassificationResult[]\n> {\n  private tokenizer: Tokenizer | null = null;\n  private hypothesisTemplate: string = 'This text is about {label}.';\n\n  constructor(config?: PipelineConfig) {\n    super(config ?? {\n      task: 'zero-shot-classification',\n      model: 'default',\n    });\n  }\n\n  /**\n   * Set tokenizer\n   */\n  setTokenizer(tokenizer: Tokenizer): void {\n    this.tokenizer = tokenizer;\n  }\n\n  /**\n   * Run classification (convenience method with separate arguments)\n   */\n  async classify(\n    text: string | string[],\n    candidateLabels: string[],\n    options?: ZeroShotClassificationOptions\n  ): Promise<ZeroShotClassificationResult | ZeroShotClassificationResult[]> {\n    return this.run({ text, candidateLabels }, options);\n  }\n\n  /**\n   * Run classification\n   */\n  override async run(\n    input: ZeroShotInput,\n    options?: PipelineOptions\n  ): Promise<ZeroShotClassificationResult | ZeroShotClassificationResult[]> {\n    await this.initialize();\n    \n    const { text, candidateLabels } = input;\n    const opts = options as ZeroShotClassificationOptions ?? {};\n    const texts = Array.isArray(text) ? text : [text];\n    const template = opts.hypothesisTemplate ?? this.hypothesisTemplate;\n    const multiLabel = opts.multiLabel ?? false;\n    \n    const results = await Promise.all(\n      texts.map(t => this.classifySingle(t, candidateLabels, template, multiLabel))\n    );\n    \n    return Array.isArray(text) ? results : results[0]!;\n  }\n\n  /**\n   * Classify a single text\n   */\n  private async classifySingle(\n    text: string,\n    candidateLabels: string[],\n    template: string,\n    multiLabel: boolean\n  ): Promise<ZeroShotClassificationResult> {\n    const startTime = performance.now();\n    \n    // Create hypothesis for each label\n    const hypotheses = candidateLabels.map(label =>\n      template.replace('{label}', label)\n    );\n\n    // Score each hypothesis\n    const scores: number[] = [];\n    \n    for (const hypothesis of hypotheses) {\n      const score = await this.scoreHypothesis(text, hypothesis);\n      scores.push(score);\n    }\n\n    // Normalize scores\n    let normalizedScores: number[];\n    \n    if (multiLabel) {\n      // Sigmoid for independent probabilities\n      normalizedScores = scores.map(s => 1 / (1 + Math.exp(-s)));\n    } else {\n      // Softmax for mutually exclusive labels\n      const tensor = new EdgeFlowTensor(new Float32Array(scores), [scores.length], 'float32');\n      normalizedScores = Array.from(softmax(tensor).toFloat32Array());\n    }\n\n    // Sort by score\n    const indexed = candidateLabels.map((label, i) => ({\n      label,\n      score: normalizedScores[i] ?? 0,\n    }));\n    indexed.sort((a, b) => b.score - a.score);\n\n    return {\n      sequence: text,\n      labels: indexed.map(i => i.label),\n      scores: indexed.map(i => i.score),\n      processingTime: performance.now() - startTime,\n    };\n  }\n\n  /**\n   * Score a single hypothesis using NLI\n   */\n  private async scoreHypothesis(premise: string, hypothesis: string): Promise<number> {\n    if (!this.tokenizer) {\n      throw new Error('Tokenizer not set. Call setTokenizer() first.');\n    }\n\n    // Encode premise-hypothesis pair (for future model integration)\n    this.tokenizer.encode(premise, {\n      textPair: hypothesis,\n      addSpecialTokens: true,\n      maxLength: 512,\n      truncation: true,\n      returnAttentionMask: true,\n      returnTokenTypeIds: true,\n    });\n\n    // For NLI models, output is typically [contradiction, neutral, entailment]\n    // Return the entailment score\n    \n    // Simplified: return random score for now (real implementation needs model output)\n    return Math.random();\n  }\n\n  /**\n   * Preprocess - not directly used (handled in scoreHypothesis)\n   */\n  protected async preprocess(\n    input: ZeroShotInput\n  ): Promise<EdgeFlowTensor[]> {\n    const { text, candidateLabels } = input;\n    \n    // Encode first text-label pair for shape reference\n    const firstText = Array.isArray(text) ? text[0] ?? '' : text;\n    const firstLabel = candidateLabels[0] ?? '';\n    \n    if (!this.tokenizer) {\n      return [new EdgeFlowTensor(new Float32Array([0]), [1], 'float32')];\n    }\n\n    const encoded = this.tokenizer.encode(firstText, {\n      textPair: this.hypothesisTemplate.replace('{label}', firstLabel),\n      addSpecialTokens: true,\n      maxLength: 512,\n    });\n\n    return [new EdgeFlowTensor(\n      BigInt64Array.from(encoded.inputIds.map(id => BigInt(id))),\n      [1, encoded.inputIds.length],\n      'int64'\n    )];\n  }\n\n  /**\n   * Postprocess - not directly used\n   */\n  protected async postprocess(\n    _outputs: EdgeFlowTensor[],\n    _options?: PipelineOptions\n  ): Promise<ZeroShotClassificationResult | ZeroShotClassificationResult[]> {\n    return {\n      sequence: '',\n      labels: [],\n      scores: [],\n    };\n  }\n}\n\n// ============================================================================\n// Factory\n// ============================================================================\n\nexport function createZeroShotClassificationPipeline(\n  config?: PipelineConfig\n): ZeroShotClassificationPipeline {\n  return new ZeroShotClassificationPipeline(config);\n}\n", "/**\n * edgeFlow.js - Question Answering Pipeline\n * \n * Extract answers from context given a question.\n */\n\nimport { BasePipeline, PipelineResult } from './base.js';\nimport { EdgeFlowTensor, softmax } from '../core/tensor.js';\nimport { PipelineConfig, PipelineOptions } from '../core/types.js';\nimport { Tokenizer } from '../utils/tokenizer.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * Question answering input\n */\nexport interface QAInput {\n  question: string;\n  context: string;\n}\n\n/**\n * Question answering options\n */\nexport interface QuestionAnsweringOptions extends PipelineOptions {\n  /** Maximum answer length in tokens */\n  maxAnswerLength?: number;\n  /** Maximum question length */\n  maxQuestionLength?: number;\n  /** Top-k answers to return */\n  topK?: number;\n  /** Minimum confidence threshold */\n  threshold?: number;\n  /** Handle impossible questions */\n  handleImpossible?: boolean;\n}\n\n/**\n * Question answering result\n */\nexport interface QuestionAnsweringResult extends PipelineResult {\n  /** Answer text */\n  answer: string;\n  /** Confidence score */\n  score: number;\n  /** Start character index in context */\n  start: number;\n  /** End character index in context */\n  end: number;\n}\n\n// ============================================================================\n// Question Answering Pipeline\n// ============================================================================\n\n/**\n * QuestionAnsweringPipeline - Extractive QA\n * \n * @example\n * ```typescript\n * const qa = await pipeline('question-answering');\n * \n * const result = await qa.run({\n *   question: 'What is the capital of France?',\n *   context: 'Paris is the capital and largest city of France.'\n * });\n * \n * console.log(result.answer); // 'Paris'\n * ```\n */\nexport class QuestionAnsweringPipeline extends BasePipeline<\n  QAInput | QAInput[],\n  QuestionAnsweringResult | QuestionAnsweringResult[]\n> {\n  private tokenizer: Tokenizer | null = null;\n\n  constructor(config?: PipelineConfig) {\n    super(config ?? {\n      task: 'question-answering',\n      model: 'default',\n    });\n  }\n\n  /**\n   * Set tokenizer\n   */\n  setTokenizer(tokenizer: Tokenizer): void {\n    this.tokenizer = tokenizer;\n  }\n\n  /**\n   * Run question answering\n   */\n  override async run(\n    input: QAInput | QAInput[],\n    options?: QuestionAnsweringOptions\n  ): Promise<QuestionAnsweringResult | QuestionAnsweringResult[]> {\n    await this.initialize();\n    \n    const inputs = Array.isArray(input) ? input : [input];\n    const results = await Promise.all(\n      inputs.map(i => this.answerQuestion(i, options ?? {}))\n    );\n    \n    return Array.isArray(input) ? results : results[0]!;\n  }\n\n  /**\n   * Answer a single question\n   */\n  private async answerQuestion(\n    input: QAInput,\n    options: QuestionAnsweringOptions\n  ): Promise<QuestionAnsweringResult> {\n    const startTime = performance.now();\n    \n    if (!this.tokenizer) {\n      throw new Error('Tokenizer not set. Call setTokenizer() first.');\n    }\n\n    const { question, context } = input;\n    const {\n      maxAnswerLength = 30,\n    } = options;\n\n    // Encode question and context\n    const encoded = this.tokenizer.encode(question, {\n      textPair: context,\n      addSpecialTokens: true,\n      maxLength: 512,\n      truncation: true,\n      returnAttentionMask: true,\n      returnTokenTypeIds: true,\n    });\n    \n    // Simplified: find answer in context\n    const answer = this.findBestAnswer(\n      context,\n      question,\n      encoded.inputIds,\n      maxAnswerLength\n    );\n\n    return {\n      answer: answer.text,\n      score: answer.score,\n      start: answer.start,\n      end: answer.end,\n      processingTime: performance.now() - startTime,\n    };\n  }\n\n  /**\n   * Find best answer span\n   */\n  private findBestAnswer(\n    context: string,\n    question: string,\n    _tokenIds: number[],\n    maxLength: number\n  ): { text: string; score: number; start: number; end: number } {\n    // Simplified answer extraction\n    // Real implementation would use model's start/end logits\n    \n    // Find common words between question and context\n    const questionWords = question.toLowerCase().split(/\\s+/);\n    const contextSentences = context.split(/[.!?]+/).filter(s => s.trim());\n    \n    let bestSentence = '';\n    let bestScore = 0;\n    let bestStart = 0;\n    \n    for (const sentence of contextSentences) {\n      const words = sentence.toLowerCase().split(/\\s+/);\n      let score = 0;\n      \n      for (const qWord of questionWords) {\n        if (words.some(w => w.includes(qWord) || qWord.includes(w))) {\n          score += 1;\n        }\n      }\n      \n      if (score > bestScore) {\n        bestScore = score;\n        bestSentence = sentence.trim();\n        bestStart = context.indexOf(sentence.trim());\n      }\n    }\n\n    // Extract a shorter answer from the sentence\n    const words = bestSentence.split(/\\s+/);\n    if (words.length > maxLength) {\n      bestSentence = words.slice(0, maxLength).join(' ');\n    }\n\n    const normalizedScore = questionWords.length > 0\n      ? bestScore / questionWords.length\n      : 0;\n\n    return {\n      text: bestSentence || 'No answer found',\n      score: Math.min(normalizedScore, 1.0),\n      start: bestStart >= 0 ? bestStart : 0,\n      end: bestStart >= 0 ? bestStart + bestSentence.length : 0,\n    };\n  }\n\n  /**\n   * Preprocess QA input\n   */\n  protected async preprocess(input: QAInput | QAInput[]): Promise<EdgeFlowTensor[]> {\n    if (!this.tokenizer) {\n      return [new EdgeFlowTensor(new Float32Array([0]), [1], 'float32')];\n    }\n\n    const qaInput = Array.isArray(input) ? input[0]! : input;\n    \n    const encoded = this.tokenizer.encode(qaInput.question, {\n      textPair: qaInput.context,\n      addSpecialTokens: true,\n      maxLength: 512,\n      truncation: true,\n      returnAttentionMask: true,\n      returnTokenTypeIds: true,\n    });\n\n    return [\n      new EdgeFlowTensor(\n        BigInt64Array.from(encoded.inputIds.map(id => BigInt(id))),\n        [1, encoded.inputIds.length],\n        'int64'\n      ),\n      new EdgeFlowTensor(\n        BigInt64Array.from(encoded.attentionMask.map(m => BigInt(m))),\n        [1, encoded.attentionMask.length],\n        'int64'\n      ),\n    ];\n  }\n\n  /**\n   * Postprocess model output\n   */\n  protected async postprocess(\n    outputs: EdgeFlowTensor[],\n    _options?: PipelineOptions\n  ): Promise<QuestionAnsweringResult | QuestionAnsweringResult[]> {\n    // Extract start and end positions from model output\n    if (outputs.length < 2) {\n      return { answer: '', score: 0, start: 0, end: 0 };\n    }\n\n    const startLogits = outputs[0]!.toFloat32Array();\n    const endLogits = outputs[1]!.toFloat32Array();\n    const seqLen = startLogits.length;\n\n    // Apply softmax\n    const startProbs = softmax(new EdgeFlowTensor(startLogits, [seqLen], 'float32')).toFloat32Array();\n    const endProbs = softmax(new EdgeFlowTensor(endLogits, [seqLen], 'float32')).toFloat32Array();\n\n    // Find best start/end positions\n    let bestStart = 0;\n    let bestEnd = 0;\n    let bestScore = 0;\n\n    for (let start = 0; start < seqLen; start++) {\n      for (let end = start; end < Math.min(start + 30, seqLen); end++) {\n        const score = (startProbs[start] ?? 0) * (endProbs[end] ?? 0);\n        if (score > bestScore) {\n          bestScore = score;\n          bestStart = start;\n          bestEnd = end;\n        }\n      }\n    }\n\n    return {\n      answer: '', // Would need tokenizer to decode\n      score: bestScore,\n      start: bestStart,\n      end: bestEnd,\n    };\n  }\n}\n\n// ============================================================================\n// Factory\n// ============================================================================\n\nexport function createQuestionAnsweringPipeline(\n  config?: PipelineConfig\n): QuestionAnsweringPipeline {\n  return new QuestionAnsweringPipeline(config);\n}\n", "/**\n * edgeFlow.js - Pipeline Exports\n */\n\nimport {\n  PipelineConfig,\n  PipelineTask,\n  RuntimeType,\n  QuantizationType,\n} from '../core/types.js';\n\n// Base\nexport {\n  BasePipeline,\n  registerPipeline,\n  getPipelineFactory,\n  SENTIMENT_LABELS,\n  EMOTION_LABELS,\n  IMAGENET_LABELS,\n  type PipelineResult,\n  type TextClassificationResult,\n  type FeatureExtractionResult,\n  type ImageClassificationResult,\n  type ObjectDetectionResult,\n} from './base.js';\n\n// Text Classification\nexport {\n  TextClassificationPipeline,\n  SentimentAnalysisPipeline,\n  createTextClassificationPipeline,\n  createSentimentAnalysisPipeline,\n  type TextClassificationOptions,\n} from './text-classification.js';\n\n// Feature Extraction\nexport {\n  FeatureExtractionPipeline,\n  createFeatureExtractionPipeline,\n  type FeatureExtractionOptions,\n} from './feature-extraction.js';\n\n// Image Classification\nexport {\n  ImageClassificationPipeline,\n  createImageClassificationPipeline,\n  type ImageClassificationOptions,\n  type ImageInput,\n} from './image-classification.js';\n\n// Text Generation\nexport {\n  TextGenerationPipeline,\n  createTextGenerationPipeline,\n  type TextGenerationOptions,\n  type TextGenerationResult,\n  type GenerationStreamEvent,\n} from './text-generation.js';\n\n// Object Detection\nexport {\n  ObjectDetectionPipeline,\n  createObjectDetectionPipeline,\n  COCO_LABELS,\n  type ObjectDetectionOptions,\n  type Detection,\n  type BoundingBox,\n} from './object-detection.js';\n\n// Automatic Speech Recognition\nexport {\n  AutomaticSpeechRecognitionPipeline,\n  createASRPipeline,\n  type ASROptions,\n  type ASRResult,\n  type WordTimestamp,\n  type ChunkTimestamp,\n} from './automatic-speech-recognition.js';\n\n// Zero-shot Classification\nexport {\n  ZeroShotClassificationPipeline,\n  createZeroShotClassificationPipeline,\n  type ZeroShotClassificationOptions,\n  type ZeroShotClassificationResult,\n} from './zero-shot-classification.js';\n\n// Question Answering\nexport {\n  QuestionAnsweringPipeline,\n  createQuestionAnsweringPipeline,\n  type QuestionAnsweringOptions,\n  type QuestionAnsweringResult,\n  type QAInput,\n} from './question-answering.js';\n\n// ============================================================================\n// High-Level Pipeline Factory\n// ============================================================================\n\n/**\n * Pipeline options for the factory function\n */\nexport interface PipelineFactoryOptions {\n  /** Model ID or URL */\n  model?: string;\n  /** Runtime to use */\n  runtime?: RuntimeType;\n  /** Enable caching */\n  cache?: boolean;\n  /** Quantization type */\n  quantization?: QuantizationType;\n  /** Custom labels for classification */\n  labels?: string[];\n}\n\n/**\n * Supported pipeline task mapping\n */\ntype PipelineTaskMap = {\n  'text-classification': TextClassificationPipeline;\n  'sentiment-analysis': SentimentAnalysisPipeline;\n  'feature-extraction': FeatureExtractionPipeline;\n  'image-classification': ImageClassificationPipeline;\n  'text-generation': TextGenerationPipeline;\n  'object-detection': ObjectDetectionPipeline;\n  'automatic-speech-recognition': AutomaticSpeechRecognitionPipeline;\n  'zero-shot-classification': ZeroShotClassificationPipeline;\n  'question-answering': QuestionAnsweringPipeline;\n};\n\n// Import pipeline classes\nimport { TextClassificationPipeline, SentimentAnalysisPipeline } from './text-classification.js';\nimport { FeatureExtractionPipeline } from './feature-extraction.js';\nimport { ImageClassificationPipeline } from './image-classification.js';\nimport { TextGenerationPipeline } from './text-generation.js';\nimport { ObjectDetectionPipeline } from './object-detection.js';\nimport { AutomaticSpeechRecognitionPipeline } from './automatic-speech-recognition.js';\nimport { ZeroShotClassificationPipeline } from './zero-shot-classification.js';\nimport { QuestionAnsweringPipeline } from './question-answering.js';\n\n/**\n * Create a pipeline for a specific task\n * \n * @example\n * ```typescript\n * // Create a sentiment analysis pipeline\n * const sentiment = await pipeline('sentiment-analysis');\n * const result = await sentiment.run('I love this product!');\n * \n * // Create an image classifier with custom model\n * const classifier = await pipeline('image-classification', {\n *   model: 'https://example.com/model.bin',\n * });\n * ```\n */\nexport async function pipeline<T extends keyof PipelineTaskMap>(\n  task: T,\n  options?: PipelineFactoryOptions\n): Promise<PipelineTaskMap[T]> {\n  const config: PipelineConfig = {\n    task: task as PipelineTask,\n    model: options?.model ?? 'default',\n    runtime: options?.runtime,\n    cache: options?.cache ?? true,\n    quantization: options?.quantization,\n  };\n\n  type AllPipelines = TextClassificationPipeline | SentimentAnalysisPipeline | FeatureExtractionPipeline | ImageClassificationPipeline | TextGenerationPipeline | ObjectDetectionPipeline | AutomaticSpeechRecognitionPipeline | ZeroShotClassificationPipeline | QuestionAnsweringPipeline;\n  \n  let pipelineInstance: AllPipelines;\n\n  switch (task) {\n    case 'text-classification':\n      pipelineInstance = new TextClassificationPipeline(config, options?.labels);\n      break;\n    case 'sentiment-analysis':\n      pipelineInstance = new SentimentAnalysisPipeline(config);\n      break;\n    case 'feature-extraction':\n      pipelineInstance = new FeatureExtractionPipeline(config);\n      break;\n    case 'image-classification':\n      pipelineInstance = new ImageClassificationPipeline(config, options?.labels);\n      break;\n    case 'text-generation':\n      pipelineInstance = new TextGenerationPipeline(config);\n      break;\n    case 'object-detection':\n      pipelineInstance = new ObjectDetectionPipeline(config, options?.labels);\n      break;\n    case 'automatic-speech-recognition':\n      pipelineInstance = new AutomaticSpeechRecognitionPipeline(config);\n      break;\n    case 'zero-shot-classification':\n      pipelineInstance = new ZeroShotClassificationPipeline(config);\n      break;\n    case 'question-answering':\n      pipelineInstance = new QuestionAnsweringPipeline(config);\n      break;\n    default:\n      throw new Error(`Unknown pipeline task: ${task}`);\n  }\n\n  // Initialize the pipeline\n  await pipelineInstance.initialize();\n\n  return pipelineInstance as PipelineTaskMap[T];\n}\n\n/**\n * Create multiple pipelines at once\n */\nexport async function createPipelines<T extends (keyof PipelineTaskMap)[]>(\n  tasks: T,\n  options?: PipelineFactoryOptions\n): Promise<{ [K in T[number]]: PipelineTaskMap[K] }> {\n  const pipelines = await Promise.all(\n    tasks.map(task => pipeline(task, options))\n  );\n\n  const result: Partial<{ [K in T[number]]: PipelineTaskMap[K] }> = {};\n  \n  for (let i = 0; i < tasks.length; i++) {\n    const task = tasks[i]!;\n    result[task as T[number]] = pipelines[i] as PipelineTaskMap[T[number]];\n  }\n\n  return result as { [K in T[number]]: PipelineTaskMap[K] };\n}\n", "/**\n * edgeFlow.js - Utilities Exports\n */\n\n// Tokenizer\nexport {\n  Tokenizer,\n  createBasicTokenizer,\n  loadTokenizer,\n  loadTokenizerFromHub,\n  type TokenizerModel,\n  type TokenizerOptions,\n} from './tokenizer.js';\n\n// Preprocessor\nexport {\n  ImagePreprocessor,\n  AudioPreprocessor,\n  preprocessText,\n  createImagePreprocessor,\n  createAudioPreprocessor,\n  type ImagePreprocessorOptions,\n  type AudioPreprocessorOptions,\n  type TextPreprocessorOptions,\n} from './preprocessor.js';\n\n// Cache\nexport {\n  Cache,\n  InferenceCache,\n  ModelDownloadCache,\n  createCache,\n  type CacheStrategy,\n  type CacheOptions,\n  type CacheStats,\n} from './cache.js';\n\n// Model Loader (Preloading, Sharding, Resume, Caching)\nexport {\n  loadModelData,\n  preloadModel,\n  preloadModels,\n  isModelCached,\n  getCachedModel,\n  deleteCachedModel,\n  clearModelCache,\n  getModelCacheStats,\n  getPreloadStatus,\n  cancelPreload,\n  getPreloadedModel,\n  type DownloadProgress,\n  type ModelLoaderOptions,\n  type PreloadOptions,\n} from './model-loader.js';\n\n// HuggingFace Hub Integration\nexport {\n  fromHub,\n  fromTask,\n  downloadModel,\n  downloadFile,\n  downloadTokenizer,\n  downloadConfig,\n  modelExists,\n  getModelInfo,\n  getDefaultModel,\n  POPULAR_MODELS,\n  type HubOptions,\n  type HubDownloadProgress,\n  type ModelConfig,\n  type ModelBundle,\n  type PopularModelTask,\n} from './hub.js';\n", "/**\n * edgeFlow.js - Hugging Face Hub Integration\n * \n * Automatically download models, tokenizers, and configs from Hugging Face Hub.\n */\n\nimport { loadModelData, isModelCached, type DownloadProgress } from './model-loader.js';\nimport { Tokenizer } from './tokenizer.js';\nimport { EdgeFlowError, ErrorCodes } from '../core/types.js';\n\n// ============================================================================\n// Types\n// ============================================================================\n\n/**\n * Hub options\n */\nexport interface HubOptions {\n  /** HuggingFace API endpoint (default: https://huggingface.co) */\n  endpoint?: string;\n  /** Model revision/branch (default: main) */\n  revision?: string;\n  /** Subfolder within the repo */\n  subfolder?: string;\n  /** Enable caching */\n  cache?: boolean;\n  /** Force re-download */\n  forceDownload?: boolean;\n  /** Progress callback */\n  onProgress?: (progress: HubDownloadProgress) => void;\n  /** HuggingFace API token (for private repos) */\n  token?: string;\n}\n\n/**\n * Download progress for hub\n */\nexport interface HubDownloadProgress {\n  /** Current file being downloaded */\n  file: string;\n  /** File index (1-based) */\n  fileIndex: number;\n  /** Total files */\n  totalFiles: number;\n  /** File download progress */\n  fileProgress: DownloadProgress;\n  /** Overall progress (0-100) */\n  overallProgress: number;\n}\n\n/**\n * Model info from config.json\n */\nexport interface ModelConfig {\n  model_type?: string;\n  architectures?: string[];\n  hidden_size?: number;\n  num_attention_heads?: number;\n  num_hidden_layers?: number;\n  vocab_size?: number;\n  max_position_embeddings?: number;\n  type_vocab_size?: number;\n  id2label?: Record<string, string>;\n  label2id?: Record<string, number>;\n  [key: string]: unknown;\n}\n\n/**\n * Downloaded model bundle\n */\nexport interface ModelBundle {\n  /** Model ID */\n  modelId: string;\n  /** Model data (ArrayBuffer) */\n  modelData: ArrayBuffer;\n  /** Tokenizer instance */\n  tokenizer?: Tokenizer;\n  /** Model config */\n  config?: ModelConfig;\n  /** Model files info */\n  files: {\n    model?: string;\n    tokenizer?: string;\n    config?: string;\n  };\n}\n\n// ============================================================================\n// Constants\n// ============================================================================\n\nconst DEFAULT_ENDPOINT = 'https://huggingface.co';\nconst DEFAULT_REVISION = 'main';\n\n/**\n * Common ONNX model file patterns (in order of preference)\n */\nconst ONNX_MODEL_FILES = [\n  'model.onnx',\n  'model_quantized.onnx',\n  'model_int8.onnx',\n  'model_uint8.onnx',\n  'model_fp16.onnx',\n  'onnx/model.onnx',\n  'onnx/model_quantized.onnx',\n];\n\n// ============================================================================\n// Hub API\n// ============================================================================\n\n/**\n * Build URL for a file in a HuggingFace repo\n */\nfunction buildFileUrl(\n  modelId: string,\n  filename: string,\n  options: HubOptions = {}\n): string {\n  const endpoint = options.endpoint ?? DEFAULT_ENDPOINT;\n  const revision = options.revision ?? DEFAULT_REVISION;\n  const subfolder = options.subfolder ? `${options.subfolder}/` : '';\n  \n  return `${endpoint}/${modelId}/resolve/${revision}/${subfolder}${filename}`;\n}\n\n/**\n * Fetch with optional auth token\n */\nasync function fetchWithAuth(url: string, token?: string): Promise<Response> {\n  const headers: HeadersInit = {};\n  if (token) {\n    headers['Authorization'] = `Bearer ${token}`;\n  }\n  \n  const response = await fetch(url, { headers });\n  return response;\n}\n\n/**\n * Check if a file exists in a repo\n */\nasync function fileExists(\n  modelId: string,\n  filename: string,\n  options: HubOptions = {}\n): Promise<boolean> {\n  const url = buildFileUrl(modelId, filename, options);\n  \n  try {\n    const response = await fetchWithAuth(url, options.token);\n    // HuggingFace returns 302 redirect for existing files\n    return response.ok || response.status === 302;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Find the best ONNX model file in a repo\n */\nasync function findOnnxModel(\n  modelId: string,\n  options: HubOptions = {}\n): Promise<string | null> {\n  // Try common file patterns\n  for (const filename of ONNX_MODEL_FILES) {\n    if (await fileExists(modelId, filename, options)) {\n      return filename;\n    }\n  }\n  \n  return null;\n}\n\n/**\n * Download a file from HuggingFace Hub\n */\nexport async function downloadFile(\n  modelId: string,\n  filename: string,\n  options: HubOptions = {}\n): Promise<ArrayBuffer> {\n  const url = buildFileUrl(modelId, filename, options);\n  \n  // Use model loader for caching and resume support\n  return loadModelData(url, {\n    cache: options.cache ?? true,\n    forceDownload: options.forceDownload ?? false,\n    onProgress: options.onProgress ? (progress) => {\n      options.onProgress!({\n        file: filename,\n        fileIndex: 1,\n        totalFiles: 1,\n        fileProgress: progress,\n        overallProgress: progress.percent,\n      });\n    } : undefined,\n  });\n}\n\n/**\n * Download JSON file from HuggingFace Hub\n */\nexport async function downloadJson<T = unknown>(\n  modelId: string,\n  filename: string,\n  options: HubOptions = {}\n): Promise<T> {\n  const url = buildFileUrl(modelId, filename, options);\n  \n  // Check cache first\n  if (options.cache !== false && !options.forceDownload) {\n    const cached = await isModelCached(url);\n    if (cached) {\n      const data = await loadModelData(url, { cache: true });\n      const text = new TextDecoder().decode(data);\n      return JSON.parse(text) as T;\n    }\n  }\n  \n  // Fetch directly for small JSON files\n  const response = await fetchWithAuth(url, options.token);\n  \n  if (!response.ok) {\n    throw new EdgeFlowError(\n      `Failed to download ${filename} from ${modelId}: ${response.status}`,\n      ErrorCodes.MODEL_NOT_FOUND\n    );\n  }\n  \n  return response.json() as Promise<T>;\n}\n\n/**\n * Download tokenizer from HuggingFace Hub\n */\nexport async function downloadTokenizer(\n  modelId: string,\n  options: HubOptions = {}\n): Promise<Tokenizer> {\n  const url = buildFileUrl(modelId, 'tokenizer.json', options);\n  return Tokenizer.fromUrl(url);\n}\n\n/**\n * Download model config from HuggingFace Hub\n */\nexport async function downloadConfig(\n  modelId: string,\n  options: HubOptions = {}\n): Promise<ModelConfig> {\n  return downloadJson<ModelConfig>(modelId, 'config.json', options);\n}\n\n/**\n * Download complete model bundle (model + tokenizer + config)\n */\nexport async function downloadModel(\n  modelId: string,\n  options: HubOptions = {}\n): Promise<ModelBundle> {\n  const files: ModelBundle['files'] = {};\n  const totalSteps = 3; // model, tokenizer, config\n  let currentStep = 0;\n  \n  const reportProgress = (\n    file: string,\n    progress: DownloadProgress\n  ) => {\n    if (options.onProgress) {\n      const baseProgress = (currentStep / totalSteps) * 100;\n      const stepProgress = (progress.percent / totalSteps);\n      \n      options.onProgress({\n        file,\n        fileIndex: currentStep + 1,\n        totalFiles: totalSteps,\n        fileProgress: progress,\n        overallProgress: baseProgress + stepProgress,\n      });\n    }\n  };\n  \n  // 1. Find and download ONNX model\n  console.log(`\uD83D\uDD0D Finding ONNX model in ${modelId}...`);\n  const modelFile = await findOnnxModel(modelId, options);\n  \n  if (!modelFile) {\n    throw new EdgeFlowError(\n      `No ONNX model found in ${modelId}. Please ensure the model has an ONNX file.`,\n      ErrorCodes.MODEL_NOT_FOUND,\n      { modelId, triedFiles: ONNX_MODEL_FILES }\n    );\n  }\n  \n  files.model = modelFile;\n  console.log(`\uD83D\uDCE6 Downloading model: ${modelFile}`);\n  \n  const modelData = await downloadFile(modelId, modelFile, {\n    ...options,\n    onProgress: (p) => reportProgress(modelFile, p.fileProgress),\n  });\n  \n  currentStep = 1;\n  \n  // 2. Download tokenizer (optional)\n  let tokenizer: Tokenizer | undefined;\n  try {\n    console.log(`\uD83D\uDCDD Downloading tokenizer...`);\n    files.tokenizer = 'tokenizer.json';\n    tokenizer = await downloadTokenizer(modelId, options);\n    console.log(`\u2713 Tokenizer loaded`);\n  } catch (error) {\n    console.warn(`\u26A0\uFE0F No tokenizer found for ${modelId}`);\n  }\n  \n  currentStep = 2;\n  \n  // 3. Download config (optional)\n  let config: ModelConfig | undefined;\n  try {\n    console.log(`\u2699\uFE0F Downloading config...`);\n    files.config = 'config.json';\n    config = await downloadConfig(modelId, options);\n    console.log(`\u2713 Config loaded`);\n  } catch (error) {\n    console.warn(`\u26A0\uFE0F No config found for ${modelId}`);\n  }\n  \n  currentStep = 3;\n  \n  if (options.onProgress) {\n    options.onProgress({\n      file: 'complete',\n      fileIndex: totalSteps,\n      totalFiles: totalSteps,\n      fileProgress: { loaded: 1, total: 1, percent: 100, speed: 0, eta: 0 },\n      overallProgress: 100,\n    });\n  }\n  \n  console.log(`\u2705 Model bundle downloaded: ${modelId}`);\n  \n  return {\n    modelId,\n    modelData,\n    tokenizer,\n    config,\n    files,\n  };\n}\n\n// ============================================================================\n// High-level API\n// ============================================================================\n\n/**\n * Load a model from HuggingFace Hub\n * \n * @example\n * ```typescript\n * // Load a sentiment analysis model\n * const bundle = await fromHub('Xenova/distilbert-base-uncased-finetuned-sst-2-english');\n * \n * // Use with edgeFlow\n * const model = await loadModelFromBuffer(bundle.modelData);\n * const tokens = bundle.tokenizer.encode('I love this!');\n * ```\n */\nexport async function fromHub(\n  modelId: string,\n  options: HubOptions = {}\n): Promise<ModelBundle> {\n  return downloadModel(modelId, options);\n}\n\n/**\n * Check if a model exists on HuggingFace Hub\n */\nexport async function modelExists(\n  modelId: string,\n  options: HubOptions = {}\n): Promise<boolean> {\n  try {\n    // Try to find an ONNX model\n    const modelFile = await findOnnxModel(modelId, options);\n    return modelFile !== null;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get model info from HuggingFace Hub\n */\nexport async function getModelInfo(\n  modelId: string,\n  options: HubOptions = {}\n): Promise<{\n  hasOnnx: boolean;\n  onnxFile?: string;\n  hasTokenizer: boolean;\n  hasConfig: boolean;\n  config?: ModelConfig;\n}> {\n  const [onnxFile, hasTokenizer, config] = await Promise.all([\n    findOnnxModel(modelId, options),\n    fileExists(modelId, 'tokenizer.json', options),\n    downloadConfig(modelId, options).catch(() => undefined),\n  ]);\n  \n  return {\n    hasOnnx: onnxFile !== null,\n    onnxFile: onnxFile ?? undefined,\n    hasTokenizer,\n    hasConfig: config !== undefined,\n    config,\n  };\n}\n\n// ============================================================================\n// Popular Models Registry\n// ============================================================================\n\n/**\n * Pre-configured popular models\n */\nexport const POPULAR_MODELS = {\n  // Text Classification / Sentiment\n  'sentiment-analysis': 'Xenova/distilbert-base-uncased-finetuned-sst-2-english',\n  'text-classification': 'Xenova/distilbert-base-uncased-finetuned-sst-2-english',\n  \n  // Feature Extraction\n  'feature-extraction': 'Xenova/all-MiniLM-L6-v2',\n  'sentence-similarity': 'Xenova/all-MiniLM-L6-v2',\n  \n  // Question Answering\n  'question-answering': 'Xenova/distilbert-base-cased-distilled-squad',\n  \n  // Token Classification\n  'ner': 'Xenova/bert-base-NER',\n  'token-classification': 'Xenova/bert-base-NER',\n  \n  // Text Generation\n  'text-generation': 'Xenova/gpt2',\n  \n  // Translation\n  'translation-en-fr': 'Xenova/t5-small',\n  'translation-en-de': 'Xenova/t5-small',\n  \n  // Summarization\n  'summarization': 'Xenova/distilbart-cnn-6-6',\n  \n  // Fill Mask\n  'fill-mask': 'Xenova/bert-base-uncased',\n  \n  // Image Classification\n  'image-classification': 'Xenova/vit-base-patch16-224',\n  \n  // Object Detection\n  'object-detection': 'Xenova/detr-resnet-50',\n  \n  // Image Segmentation\n  'image-segmentation': 'Xenova/segformer-b0-finetuned-ade-512-512',\n  \n  // Zero-shot Classification\n  'zero-shot-classification': 'Xenova/mobilebert-uncased-mnli',\n  \n  // Speech Recognition\n  'automatic-speech-recognition': 'Xenova/whisper-tiny.en',\n  \n  // Text-to-Speech\n  'text-to-speech': 'Xenova/speecht5_tts',\n} as const;\n\nexport type PopularModelTask = keyof typeof POPULAR_MODELS;\n\n/**\n * Get the default model ID for a task\n */\nexport function getDefaultModel(task: PopularModelTask): string {\n  return POPULAR_MODELS[task];\n}\n\n/**\n * Load a model by task name\n * \n * @example\n * ```typescript\n * const bundle = await fromTask('sentiment-analysis');\n * ```\n */\nexport async function fromTask(\n  task: PopularModelTask,\n  options: HubOptions = {}\n): Promise<ModelBundle> {\n  const modelId = getDefaultModel(task);\n  return downloadModel(modelId, options);\n}\n", "/**\n * edgeFlow.js - Tools and Utilities\n * \n * Model optimization, quantization, and analysis tools.\n */\n\nimport { \n  LoadedModel,\n  QuantizationType,\n} from '../core/types.js';\n\n// ============================================================================\n// Quantization Tools\n// ============================================================================\n\n/**\n * Quantization options\n */\nexport interface QuantizationOptions {\n  /** Quantization method */\n  method: QuantizationType;\n  /** Calibration data for calibrated quantization */\n  calibrationData?: Float32Array[];\n  /** Whether to quantize weights only */\n  weightsOnly?: boolean;\n  /** Layers to exclude from quantization */\n  excludeLayers?: string[];\n}\n\n/**\n * Quantization result\n */\nexport interface QuantizationResult {\n  /** Quantized model data */\n  modelData: ArrayBuffer;\n  /** Original size in bytes */\n  originalSize: number;\n  /** Quantized size in bytes */\n  quantizedSize: number;\n  /** Compression ratio */\n  compressionRatio: number;\n  /** Quantization statistics */\n  stats: {\n    layersQuantized: number;\n    layersSkipped: number;\n  };\n}\n\n/**\n * Quantize a model\n * \n * @example\n * ```typescript\n * const quantized = await quantize(model, {\n *   method: 'int8',\n *   calibrationData: samples,\n * });\n * ```\n */\nexport async function quantize(\n  model: LoadedModel | ArrayBuffer,\n  options: QuantizationOptions\n): Promise<QuantizationResult> {\n  // Get model data\n  const modelData = model instanceof ArrayBuffer \n    ? model \n    : await getModelData(model);\n\n  const originalSize = modelData.byteLength;\n\n  // Apply quantization based on method\n  let quantizedData: ArrayBuffer;\n  let layersQuantized = 0;\n  let layersSkipped = 0;\n\n  switch (options.method) {\n    case 'int8':\n      ({ data: quantizedData, layersQuantized, layersSkipped } = \n        quantizeInt8(modelData, options));\n      break;\n    case 'uint8':\n      ({ data: quantizedData, layersQuantized, layersSkipped } = \n        quantizeUint8(modelData, options));\n      break;\n    case 'float16':\n      ({ data: quantizedData, layersQuantized, layersSkipped } = \n        quantizeFloat16(modelData, options));\n      break;\n    case 'int4':\n      ({ data: quantizedData, layersQuantized, layersSkipped } = \n        quantizeInt4(modelData, options));\n      break;\n    default:\n      quantizedData = modelData;\n  }\n\n  return {\n    modelData: quantizedData,\n    originalSize,\n    quantizedSize: quantizedData.byteLength,\n    compressionRatio: originalSize / quantizedData.byteLength,\n    stats: {\n      layersQuantized,\n      layersSkipped,\n    },\n  };\n}\n\n/**\n * Placeholder for getting model data\n */\nasync function getModelData(_model: LoadedModel): Promise<ArrayBuffer> {\n  // In production, this would extract the model weights\n  return new ArrayBuffer(0);\n}\n\n/**\n * INT8 quantization\n */\nfunction quantizeInt8(\n  data: ArrayBuffer,\n  _options: QuantizationOptions\n): { data: ArrayBuffer; layersQuantized: number; layersSkipped: number } {\n  // Simplified INT8 quantization\n  const input = new Float32Array(data);\n  const output = new Int8Array(input.length);\n  \n  // Find scale\n  let max = 0;\n  for (let i = 0; i < input.length; i++) {\n    const abs = Math.abs(input[i] ?? 0);\n    if (abs > max) max = abs;\n  }\n  const scale = max / 127;\n  \n  // Quantize\n  for (let i = 0; i < input.length; i++) {\n    output[i] = Math.round((input[i] ?? 0) / scale);\n  }\n  \n  return {\n    data: output.buffer,\n    layersQuantized: 1,\n    layersSkipped: 0,\n  };\n}\n\n/**\n * UINT8 quantization\n */\nfunction quantizeUint8(\n  data: ArrayBuffer,\n  _options: QuantizationOptions\n): { data: ArrayBuffer; layersQuantized: number; layersSkipped: number } {\n  const input = new Float32Array(data);\n  const output = new Uint8Array(input.length);\n  \n  // Find min/max\n  let min = Infinity, max = -Infinity;\n  for (let i = 0; i < input.length; i++) {\n    const val = input[i] ?? 0;\n    if (val < min) min = val;\n    if (val > max) max = val;\n  }\n  const scale = (max - min) / 255;\n  \n  // Quantize\n  for (let i = 0; i < input.length; i++) {\n    output[i] = Math.round(((input[i] ?? 0) - min) / scale);\n  }\n  \n  return {\n    data: output.buffer,\n    layersQuantized: 1,\n    layersSkipped: 0,\n  };\n}\n\n/**\n * Float16 quantization\n */\nfunction quantizeFloat16(\n  data: ArrayBuffer,\n  _options: QuantizationOptions\n): { data: ArrayBuffer; layersQuantized: number; layersSkipped: number } {\n  const input = new Float32Array(data);\n  const output = new Uint16Array(input.length);\n  \n  // Convert float32 to float16\n  for (let i = 0; i < input.length; i++) {\n    output[i] = float32ToFloat16(input[i] ?? 0);\n  }\n  \n  return {\n    data: output.buffer,\n    layersQuantized: 1,\n    layersSkipped: 0,\n  };\n}\n\n/**\n * INT4 quantization\n */\nfunction quantizeInt4(\n  data: ArrayBuffer,\n  _options: QuantizationOptions\n): { data: ArrayBuffer; layersQuantized: number; layersSkipped: number } {\n  const input = new Float32Array(data);\n  // Pack two INT4 values per byte\n  const output = new Uint8Array(Math.ceil(input.length / 2));\n  \n  // Find scale\n  let max = 0;\n  for (let i = 0; i < input.length; i++) {\n    const abs = Math.abs(input[i] ?? 0);\n    if (abs > max) max = abs;\n  }\n  const scale = max / 7; // INT4 range: -8 to 7\n  \n  // Quantize and pack\n  for (let i = 0; i < input.length; i += 2) {\n    const val1 = Math.round((input[i] ?? 0) / scale) + 8;\n    const val2 = Math.round((input[i + 1] ?? 0) / scale) + 8;\n    output[i / 2] = ((val1 & 0xF) << 4) | (val2 & 0xF);\n  }\n  \n  return {\n    data: output.buffer,\n    layersQuantized: 1,\n    layersSkipped: 0,\n  };\n}\n\n/**\n * Convert float32 to float16\n */\nfunction float32ToFloat16(value: number): number {\n  const floatView = new Float32Array(1);\n  const int32View = new Int32Array(floatView.buffer);\n  \n  floatView[0] = value;\n  const x = int32View[0] ?? 0;\n  \n  let bits = (x >> 16) & 0x8000; // sign\n  let m = (x >> 12) & 0x07ff;    // mantissa\n  const e = (x >> 23) & 0xff;    // exponent\n  \n  if (e < 103) {\n    // Too small, return zero\n    return bits;\n  }\n  \n  if (e > 142) {\n    // Too large, return infinity\n    bits |= 0x7c00;\n    bits |= ((e === 255) ? 0 : 1) && (x & 0x007fffff);\n    return bits;\n  }\n  \n  if (e < 113) {\n    // Denormalized\n    m |= 0x0800;\n    bits |= (m >> (114 - e)) + ((m >> (113 - e)) & 1);\n    return bits;\n  }\n  \n  bits |= ((e - 112) << 10) | (m >> 1);\n  bits += m & 1;\n  return bits;\n}\n\n// ============================================================================\n// Model Pruning\n// ============================================================================\n\n/**\n * Pruning options\n */\nexport interface PruningOptions {\n  /** Target sparsity (0-1) */\n  sparsity: number;\n  /** Pruning method */\n  method?: 'magnitude' | 'random' | 'structured';\n  /** Layers to exclude */\n  excludeLayers?: string[];\n}\n\n/**\n * Pruning result\n */\nexport interface PruningResult {\n  /** Pruned model data */\n  modelData: ArrayBuffer;\n  /** Achieved sparsity */\n  actualSparsity: number;\n  /** Number of parameters pruned */\n  parametersPruned: number;\n  /** Total parameters */\n  totalParameters: number;\n}\n\n/**\n * Prune model weights\n */\nexport async function prune(\n  model: LoadedModel | ArrayBuffer,\n  options: PruningOptions\n): Promise<PruningResult> {\n  const modelData = model instanceof ArrayBuffer \n    ? model \n    : await getModelData(model);\n\n  const weights = new Float32Array(modelData);\n  const total = weights.length;\n  \n  // Calculate threshold for magnitude pruning\n  const magnitudes = weights.map(Math.abs);\n  const sorted = [...magnitudes].sort((a, b) => a - b);\n  const thresholdIdx = Math.floor(options.sparsity * sorted.length);\n  const threshold = sorted[thresholdIdx] ?? 0;\n  \n  // Prune weights\n  let pruned = 0;\n  for (let i = 0; i < weights.length; i++) {\n    if (Math.abs(weights[i] ?? 0) < threshold) {\n      weights[i] = 0;\n      pruned++;\n    }\n  }\n\n  return {\n    modelData: weights.buffer,\n    actualSparsity: pruned / total,\n    parametersPruned: pruned,\n    totalParameters: total,\n  };\n}\n\n// ============================================================================\n// Model Analysis\n// ============================================================================\n\n/**\n * Model analysis result\n */\nexport interface ModelAnalysis {\n  /** Total number of parameters */\n  totalParameters: number;\n  /** Model size in bytes */\n  sizeBytes: number;\n  /** Layer information */\n  layers: Array<{\n    name: string;\n    type: string;\n    parameters: number;\n    inputShape: number[];\n    outputShape: number[];\n  }>;\n  /** Estimated FLOPs */\n  estimatedFlops: number;\n  /** Memory requirements */\n  memoryRequirements: {\n    weights: number;\n    activations: number;\n    total: number;\n  };\n}\n\n/**\n * Analyze a model\n */\nexport async function analyzeModel(\n  model: LoadedModel | ArrayBuffer\n): Promise<ModelAnalysis> {\n  // Simplified analysis\n  const size = model instanceof ArrayBuffer \n    ? model.byteLength \n    : model.metadata.sizeBytes;\n\n  const estimatedParams = Math.floor(size / 4); // Assume float32\n\n  return {\n    totalParameters: estimatedParams,\n    sizeBytes: size,\n    layers: [],\n    estimatedFlops: estimatedParams * 2, // Rough estimate\n    memoryRequirements: {\n      weights: size,\n      activations: size * 0.1, // Rough estimate\n      total: size * 1.1,\n    },\n  };\n}\n\n// ============================================================================\n// Benchmarking\n// ============================================================================\n\n/**\n * Benchmark options\n */\nexport interface BenchmarkOptions {\n  /** Number of warmup runs */\n  warmupRuns?: number;\n  /** Number of benchmark runs */\n  runs?: number;\n  /** Input shape */\n  inputShape?: number[];\n}\n\n/**\n * Benchmark result\n */\nexport interface BenchmarkResult {\n  /** Average inference time in ms */\n  avgTime: number;\n  /** Minimum inference time in ms */\n  minTime: number;\n  /** Maximum inference time in ms */\n  maxTime: number;\n  /** Standard deviation */\n  stdDev: number;\n  /** Throughput (inferences per second) */\n  throughput: number;\n  /** All run times */\n  times: number[];\n}\n\n/**\n * Benchmark model inference\n */\nexport async function benchmark(\n  runFn: () => Promise<void>,\n  options: BenchmarkOptions = {}\n): Promise<BenchmarkResult> {\n  const {\n    warmupRuns = 3,\n    runs = 10,\n  } = options;\n\n  // Warmup\n  for (let i = 0; i < warmupRuns; i++) {\n    await runFn();\n  }\n\n  // Benchmark\n  const times: number[] = [];\n  for (let i = 0; i < runs; i++) {\n    const start = performance.now();\n    await runFn();\n    times.push(performance.now() - start);\n  }\n\n  // Calculate statistics\n  const sum = times.reduce((a, b) => a + b, 0);\n  const avgTime = sum / times.length;\n  const minTime = Math.min(...times);\n  const maxTime = Math.max(...times);\n  \n  const squaredDiffs = times.map(t => Math.pow(t - avgTime, 2));\n  const avgSquaredDiff = squaredDiffs.reduce((a, b) => a + b, 0) / times.length;\n  const stdDev = Math.sqrt(avgSquaredDiff);\n\n  return {\n    avgTime,\n    minTime,\n    maxTime,\n    stdDev,\n    throughput: 1000 / avgTime,\n    times,\n  };\n}\n\n// ============================================================================\n// Export Utilities\n// ============================================================================\n\n/**\n * Export model to different formats\n */\nexport async function exportModel(\n  model: LoadedModel | ArrayBuffer,\n  format: 'onnx' | 'json' | 'binary'\n): Promise<ArrayBuffer | string> {\n  const modelData = model instanceof ArrayBuffer \n    ? model \n    : await getModelData(model);\n\n  switch (format) {\n    case 'json':\n      // Export as JSON (for small models)\n      const array = new Float32Array(modelData);\n      return JSON.stringify(Array.from(array));\n    case 'binary':\n    case 'onnx':\n    default:\n      return modelData;\n  }\n}\n", "/**\n * edgeFlow.js\n * \n * Lightweight, high-performance browser ML inference framework\n * with native concurrency support.\n * \n * @example\n * ```typescript\n * import { pipeline } from 'edgeflow';\n * \n * // Create a sentiment analysis pipeline\n * const sentiment = await pipeline('sentiment-analysis');\n * \n * // Run inference\n * const result = await sentiment.run('I love this product!');\n * console.log(result); // { label: 'positive', score: 0.98 }\n * \n * // Batch processing\n * const results = await sentiment.run([\n *   'This is amazing!',\n *   'This is terrible.'\n * ]);\n * \n * // Concurrent execution with different models\n * const classifier = await pipeline('text-classification');\n * const extractor = await pipeline('feature-extraction');\n * \n * const [classification, features] = await Promise.all([\n *   classifier.run('Sample text'),\n *   extractor.run('Sample text')\n * ]);\n * ```\n * \n * @packageDocumentation\n */\n\n// ============================================================================\n// Core Exports\n// ============================================================================\n\n// Types\nexport type {\n  // Tensor types\n  DataType,\n  TypedArray,\n  Shape,\n  Tensor,\n  \n  // Runtime types\n  RuntimeType,\n  RuntimeCapabilities,\n  Runtime,\n  \n  // Model types\n  ModelFormat,\n  QuantizationType,\n  ModelMetadata,\n  ModelIOSpec,\n  ModelLoadOptions,\n  LoadedModel,\n  \n  // Scheduler types\n  TaskPriority,\n  TaskStatus,\n  InferenceTask,\n  SchedulerOptions,\n  \n  // Memory types\n  MemoryStats,\n  MemoryPoolConfig,\n  \n  // Pipeline types\n  PipelineTask,\n  PipelineConfig,\n  PipelineOptions,\n  \n  // Tokenizer types\n  TokenizerConfig,\n  TokenizedOutput,\n  \n  // Event types\n  EventType,\n  EdgeFlowEvent,\n  EventListener,\n  \n  // Error types\n  ErrorCode,\n} from './core/types.js';\n\n// Error class\nexport { EdgeFlowError, ErrorCodes } from './core/types.js';\n\n// Tensor operations\nexport {\n  EdgeFlowTensor,\n  tensor,\n  zeros,\n  ones,\n  full,\n  random,\n  randn,\n  arange,\n  linspace,\n  eye,\n  add,\n  sub,\n  mul,\n  div,\n  matmul,\n  softmax,\n  relu,\n  sigmoid,\n  tanh,\n  sum,\n  mean,\n  argmax,\n  concat,\n} from './core/tensor.js';\n\n// Scheduler\nexport {\n  InferenceScheduler,\n  getScheduler,\n  setScheduler,\n  configureScheduler,\n} from './core/scheduler.js';\n\n// Memory management\nexport {\n  MemoryManager,\n  MemoryScope,\n  ModelCache,\n  withMemoryScope,\n  withMemoryScopeSync,\n  getMemoryManager,\n  getMemoryStats,\n  release,\n  gc,\n} from './core/memory.js';\n\n// Runtime management\nexport {\n  RuntimeManager,\n  LoadedModelImpl,\n  loadModel,\n  loadModelFromBuffer,\n  runInference,\n  runBatchInference,\n  getRuntimeManager,\n  registerRuntime,\n  getBestRuntime,\n  getAvailableRuntimes,\n} from './core/runtime.js';\n\n// ============================================================================\n// Backend Exports\n// ============================================================================\n\nexport {\n  WebGPURuntime,\n  createWebGPURuntime,\n  WebNNRuntime,\n  createWebNNRuntime,\n  WASMRuntime,\n  createWASMRuntime,\n  registerAllBackends,\n} from './backends/index.js';\n\n// ============================================================================\n// Pipeline Exports\n// ============================================================================\n\nexport {\n  // Factory function\n  pipeline,\n  createPipelines,\n  \n  // Base classes\n  BasePipeline,\n  registerPipeline,\n  getPipelineFactory,\n  \n  // Labels\n  SENTIMENT_LABELS,\n  EMOTION_LABELS,\n  IMAGENET_LABELS,\n  \n  // Result types\n  type PipelineResult,\n  type TextClassificationResult,\n  type FeatureExtractionResult,\n  type ImageClassificationResult,\n  type ObjectDetectionResult,\n  \n  // Pipelines\n  TextClassificationPipeline,\n  SentimentAnalysisPipeline,\n  FeatureExtractionPipeline,\n  ImageClassificationPipeline,\n  \n  // Factory functions\n  createTextClassificationPipeline,\n  createSentimentAnalysisPipeline,\n  createFeatureExtractionPipeline,\n  createImageClassificationPipeline,\n  \n  // Options types\n  type PipelineFactoryOptions,\n  type TextClassificationOptions,\n  type FeatureExtractionOptions,\n  type ImageClassificationOptions,\n  type ImageInput,\n} from './pipelines/index.js';\n\n// ============================================================================\n// Utility Exports\n// ============================================================================\n\nexport {\n  // Tokenizer\n  Tokenizer,\n  createBasicTokenizer,\n  loadTokenizer,\n  loadTokenizerFromHub,\n  type TokenizerModel,\n  type TokenizerOptions,\n  \n  // Preprocessor\n  ImagePreprocessor,\n  AudioPreprocessor,\n  preprocessText,\n  createImagePreprocessor,\n  createAudioPreprocessor,\n  type ImagePreprocessorOptions,\n  type AudioPreprocessorOptions,\n  type TextPreprocessorOptions,\n  \n  // Cache\n  Cache,\n  InferenceCache,\n  ModelDownloadCache,\n  createCache,\n  type CacheStrategy,\n  type CacheOptions,\n  type CacheStats,\n  \n  // Model Loader (Preloading, Sharding, Resume, Caching)\n  loadModelData,\n  preloadModel,\n  preloadModels,\n  isModelCached,\n  getCachedModel,\n  deleteCachedModel,\n  clearModelCache,\n  getModelCacheStats,\n  getPreloadStatus,\n  cancelPreload,\n  getPreloadedModel,\n  type DownloadProgress,\n  type ModelLoaderOptions,\n  type PreloadOptions,\n  \n  // HuggingFace Hub Integration\n  fromHub,\n  fromTask,\n  downloadModel,\n  downloadTokenizer,\n  downloadConfig,\n  modelExists,\n  getModelInfo,\n  getDefaultModel,\n  POPULAR_MODELS,\n  type HubOptions,\n  type HubDownloadProgress,\n  type ModelConfig,\n  type ModelBundle,\n  type PopularModelTask,\n} from './utils/index.js';\n\n// ============================================================================\n// Tools Exports\n// ============================================================================\n\nexport {\n  // Quantization\n  quantize,\n  type QuantizationOptions,\n  type QuantizationResult,\n  \n  // Pruning\n  prune,\n  type PruningOptions,\n  type PruningResult,\n  \n  // Analysis\n  analyzeModel,\n  type ModelAnalysis,\n  \n  // Benchmarking\n  benchmark,\n  type BenchmarkOptions,\n  type BenchmarkResult,\n  \n  // Export\n  exportModel,\n} from './tools/index.js';\n\n// ============================================================================\n// Convenience Functions\n// ============================================================================\n\n/**\n * Check if edgeFlow is supported in the current environment\n */\nexport async function isSupported(): Promise<boolean> {\n  const runtimes = await getAvailableRuntimes();\n  return Array.from(runtimes.values()).some(v => v);\n}\n\n/**\n * Get the best available runtime type\n */\nexport async function getBestRuntimeType(): Promise<RuntimeType | null> {\n  const runtimes = await getAvailableRuntimes();\n  \n  if (runtimes.get('webgpu')) return 'webgpu';\n  if (runtimes.get('webnn')) return 'webnn';\n  if (runtimes.get('wasm')) return 'wasm';\n  \n  return null;\n}\n\n/**\n * Preload models for faster subsequent loading\n */\nexport async function preload(\n  models: string[]\n): Promise<void> {\n  const cache = new ModelDownloadCache();\n  \n  await Promise.all(models.map(async (url) => {\n    if (!(await cache.get(url))) {\n      const response = await fetch(url);\n      if (response.ok) {\n        await cache.put(url, response);\n      }\n    }\n  }));\n}\n\n// ============================================================================\n// Version Info\n// ============================================================================\n\n/**\n * edgeFlow.js version\n */\nexport const VERSION = '0.1.0';\n\n/**\n * Get framework info\n */\nexport async function getInfo(): Promise<{\n  version: string;\n  runtimes: Record<RuntimeType, boolean>;\n  features: string[];\n}> {\n  const runtimes = await getAvailableRuntimes();\n  \n  return {\n    version: VERSION,\n    runtimes: {\n      webgpu: runtimes.get('webgpu') ?? false,\n      webnn: runtimes.get('webnn') ?? false,\n      wasm: runtimes.get('wasm') ?? false,\n      auto: true,\n    },\n    features: [\n      'concurrent-execution',\n      'batch-processing',\n      'memory-management',\n      'model-caching',\n      'quantization',\n    ],\n  };\n}\n\n// Re-export RuntimeType for convenience\nimport { RuntimeType } from './core/types.js';\nimport { getAvailableRuntimes } from './core/runtime.js';\nimport { ModelDownloadCache } from './utils/cache.js';\n"],
  "mappings": "gRAAA,IAAAA,GAAA,GAAAC,GAAAD,GAAA,mBAAAE,GAAA,oBAAAC,GAAA,sBAAAC,GAAA,mBAAAC,GAAA,uBAAAC,GAAA,qBAAAC,GAAA,sBAAAC,GAAA,kBAAAC,GAAA,kBAAAC,EAAA,iBAAAC,GAAA,kBAAAC,KAuXA,eAAeC,GAAsBC,EAAW,CAC9C,GAAI,CACF,IAAMC,EAAW,MAAM,MAAMD,EAAK,CAAE,OAAQ,MAAM,CAAE,EAC9CE,EAAeD,EAAS,QAAQ,IAAI,eAAe,EACnDE,EAAgBF,EAAS,QAAQ,IAAI,gBAAgB,EACrDG,EAAOH,EAAS,QAAQ,IAAI,MAAM,GAAK,OAE7C,MAAO,CACL,SAAUC,IAAiB,QAC3B,KAAMC,EAAgB,SAASA,EAAe,EAAE,EAAI,EACpD,KAAAC,EAEJ,MAAQ,CACN,MAAO,CAAE,SAAU,GAAO,KAAM,CAAC,CACnC,CACF,CAKA,eAAeC,GACbL,EACAM,EACAC,EACAC,EAAe,CAEf,IAAMC,EAAa,IAAI,gBACjBC,EAAY,WAAW,IAAMD,EAAW,MAAK,EAAID,CAAO,EAE9D,GAAI,CACF,IAAMP,EAAW,MAAM,MAAMD,EAAK,CAChC,QAAS,CAAE,MAAO,SAASM,CAAK,IAAIC,CAAG,EAAE,EACzC,OAAQE,EAAW,OACpB,EAED,GAAIR,EAAS,SAAW,KAAOA,EAAS,SAAW,IACjD,MAAM,IAAI,MAAM,QAAQA,EAAS,MAAM,KAAKA,EAAS,UAAU,EAAE,EAGnE,OAAO,MAAMA,EAAS,YAAW,CACnC,SACE,aAAaS,CAAS,CACxB,CACF,CAKA,eAAeC,GACbX,EACAY,EAA2B,CAE3B,GAAM,CACJ,UAAAC,EAAY,EAAI,KAAO,KACvB,oBAAAC,EAAsB,EACtB,QAAAN,EAAU,IACV,WAAAO,CAAU,EACRH,EAGE,CAAE,SAAUI,EAAe,KAAMC,EAAW,KAAAb,CAAI,EAAK,MAAML,GAAsBC,CAAG,EAG1F,GAAI,CAACgB,GAAiBC,EAAYJ,EAAY,EAC5C,OAAOK,GAAelB,EAAKQ,EAASO,CAAU,EAIhD,IAAII,EAAQ,MAAMC,EAAW,iBAAiBpB,CAAG,EAGjD,GAAI,CAACmB,GAAUf,GAAQe,EAAM,YAAcF,EAAY,CACrD,IAAMI,EAAY,KAAK,KAAKJ,EAAYJ,CAAS,EAC3CS,EAAuB,CAAA,EAE7B,QAASC,EAAI,EAAGA,EAAIF,EAAWE,IAAK,CAClC,IAAMjB,EAAQiB,EAAIV,EACZN,GAAM,KAAK,IAAID,EAAQO,EAAY,EAAGI,EAAY,CAAC,EACzDK,EAAO,KAAK,CAAE,MAAOC,EAAG,MAAAjB,EAAO,IAAAC,GAAK,WAAY,EAAK,CAAE,CACzD,CAEAY,EAAQ,CACN,IAAAnB,EACA,UAAAiB,EACA,eAAgB,EAChB,OAAAK,EACA,UAAW,KAAK,IAAG,GAIrB,MAAMF,EAAW,YAAYpB,CAAG,CAClC,CAGA,IAAMwB,EAAgBL,EAAM,OAAO,OAAOM,GAAK,CAACA,EAAE,UAAU,EACxDC,EAAiBP,EAAM,eAEvBQ,EADc,KAAK,IAAG,EAEtBC,EAAqBF,EAGnBG,EAAiB,IAAK,CAC1B,GAAI,CAACd,EAAY,OAEjB,IAAMe,EAAM,KAAK,IAAG,EACdC,GAAWD,EAAMH,GAAoB,IACrCK,EAAkBN,EAAiBE,EACnCK,EAAQF,EAAU,EAAIC,EAAkBD,EAAU,EAClDG,GAAYjB,EAAYS,EACxBS,GAAMF,EAAQ,EAAKC,GAAYD,EAAS,IAAO,EAErDlB,EAAW,CACT,OAAQW,EACR,MAAOT,EACP,QAAUS,EAAiBT,EAAa,IACxC,MAAAgB,EACA,IAAAE,GACA,aAAchB,EAAO,OAAO,OAAOM,IAAKA,GAAE,UAAU,EAAE,OACtD,YAAaN,EAAO,OAAO,OAC5B,EAEDQ,EAAmBG,EACnBF,EAAqBF,CACvB,EAGMU,EAAgB,CAAC,GAAGZ,CAAa,EACjCa,EAAa,IAAI,IAEvB,KAAOD,EAAc,OAAS,GAAKC,EAAW,KAAO,GAAG,CAEtD,KAAOD,EAAc,OAAS,GAAKC,EAAW,KAAOvB,GAAqB,CACxE,IAAMwB,EAAQF,EAAc,MAAK,EAE3BG,GAAmB,SAAW,CAClC,GAAI,CACF,IAAMC,EAAO,MAAMnC,GAAcL,EAAKsC,EAAM,MAAOA,EAAM,IAAK9B,CAAO,EACrE,MAAMY,EAAW,UAAUpB,EAAKsC,EAAM,MAAOE,CAAI,EAEjDF,EAAM,WAAa,GACnBZ,GAAkBc,EAAK,WAGvBrB,EAAO,eAAiBO,EACxB,MAAMN,EAAW,kBAAkBD,CAAM,EAEzCU,EAAc,CAChB,SACEQ,EAAW,OAAOC,EAAM,KAAK,CAC/B,CACF,GAAE,EAEFD,EAAW,IAAIC,EAAM,MAAOC,CAAe,CAC7C,CAGIF,EAAW,KAAO,GACpB,MAAM,QAAQ,KAAKA,EAAW,OAAM,CAAE,CAE1C,CAGA,IAAMf,EAAS,MAAMF,EAAW,UAAUpB,CAAG,EACvCyC,EAAS,IAAI,WAAWxB,CAAS,EACnCyB,EAAS,EAEb,QAAWJ,KAAShB,EAClBmB,EAAO,IAAI,IAAI,WAAWH,CAAK,EAAGI,CAAM,EACxCA,GAAUJ,EAAM,WAIlB,aAAMlB,EAAW,SAAS,CACxB,IAAApB,EACA,KAAMiB,EACN,KAAAb,EACA,SAAU,KAAK,IAAG,EAClB,OAAQkB,EAAO,OACf,SAAU,GACX,EACD,MAAMF,EAAW,oBAAoBpB,CAAG,EAEjCyC,EAAO,MAChB,CAKA,eAAevB,GACblB,EACAQ,EACAO,EAAiD,CAEjD,IAAMN,EAAa,IAAI,gBACjBC,EAAY,WAAW,IAAMD,EAAW,MAAK,EAAID,CAAO,EAE9D,GAAI,CACF,IAAMP,EAAW,MAAM,MAAMD,EAAK,CAAE,OAAQS,EAAW,MAAM,CAAE,EAE/D,GAAI,CAACR,EAAS,GACZ,MAAM,IAAI,MAAM,QAAQA,EAAS,MAAM,KAAKA,EAAS,UAAU,EAAE,EAGnE,IAAME,EAAgBF,EAAS,QAAQ,IAAI,gBAAgB,EACrD0C,EAAQxC,EAAgB,SAASA,EAAe,EAAE,EAAI,EAE5D,GAAI,CAACF,EAAS,MAAQ,CAACc,GAAc4B,IAAU,EAC7C,OAAO,MAAM1C,EAAS,YAAW,EAInC,IAAM2C,EAAS3C,EAAS,KAAK,UAAS,EAChCqB,EAAuB,CAAA,EACzBuB,EAAS,EACPC,EAAY,KAAK,IAAG,EAE1B,OAAa,CACX,GAAM,CAAE,KAAAC,EAAM,MAAAC,CAAK,EAAK,MAAMJ,EAAO,KAAI,EACzC,GAAIG,EAAM,MAEVzB,EAAO,KAAK0B,CAAK,EACjBH,GAAUG,EAAM,OAEhB,IAAMjB,GAAW,KAAK,IAAG,EAAKe,GAAa,IACrCb,EAAQF,EAAU,EAAIc,EAASd,EAAU,EACzCG,EAAYS,EAAQE,EACpBV,EAAMF,EAAQ,EAAKC,EAAYD,EAAS,IAAO,EAErDlB,EAAW,CACT,OAAA8B,EACA,MAAAF,EACA,QAAUE,EAASF,EAAS,IAC5B,MAAAV,EACA,IAAAE,EACD,CACH,CAGA,IAAMM,EAAS,IAAI,WAAWI,CAAM,EAChCH,EAAS,EACb,QAAWJ,KAAShB,EAClBmB,EAAO,IAAIH,EAAOI,CAAM,EACxBA,GAAUJ,EAAM,OAGlB,OAAOG,EAAO,MAChB,SACE,aAAa/B,CAAS,CACxB,CACF,CA+KA,eAAsBd,EACpBI,EACAY,EAA8B,CAAA,EAAE,CAEhC,GAAM,CACJ,MAAAqC,EAAQ,GACR,cAAAC,EAAgB,GAChB,UAAAC,EAAY,EAAI,EACdvC,EAGJ,GAAIqC,GAAS,CAACC,EAAe,CAC3B,IAAME,EAAS,MAAMhC,EAAW,SAASpB,CAAG,EAC5C,GAAIoD,EACF,eAAQ,IAAI,mCAA8BpD,CAAG,EAAE,EAC/CY,EAAQ,aAAa,CACnB,OAAQwC,EAAO,WACf,MAAOA,EAAO,WACd,QAAS,IACT,MAAO,EACP,IAAK,EACN,EACMA,CAEX,CAGA,IAAIZ,EAEJ,OAAIW,EACFX,EAAO,MAAM7B,GAAmBX,EAAKY,CAAO,EAE5C4B,EAAO,MAAMtB,GAAelB,EAAKY,EAAQ,SAAW,IAAOA,EAAQ,UAAU,EAI3EqC,IAEGE,IACH,MAAM/B,EAAW,UAAUpB,EAAK,EAAGwC,CAAI,EACvC,MAAMpB,EAAW,SAAS,CACxB,IAAApB,EACA,KAAMwC,EAAK,WACX,SAAU,KAAK,IAAG,EAClB,OAAQ,EACR,SAAU,GACX,IAIEA,CACT,CAKM,SAAU3C,GAAaG,EAAaY,EAA0B,CAAA,EAAE,CACpE,OAAOyC,GAAe,QAAQrD,EAAKY,CAAO,CAC5C,CAKM,SAAUd,GACdwD,EACA1C,EAA4C,CAAA,EAAE,CAE9C,OAAO,QAAQ,IACb0C,EAAK,IAAI,CAAC,CAAE,IAAAtD,EAAK,SAAAuD,CAAQ,IAAOF,GAAe,QAAQrD,EAAK,CAAE,GAAGY,EAAS,SAAA2C,CAAQ,CAAE,CAAC,CAAC,CAE1F,CAKA,eAAsB5D,GAAcK,EAAW,CAE7C,OADa,MAAMoB,EAAW,QAAQpB,CAAG,IAC5B,UAAY,EAC3B,CAKA,eAAsBT,GAAeS,EAAW,CAC9C,OAAOoB,EAAW,SAASpB,CAAG,CAChC,CAKA,eAAsBV,GAAkBU,EAAW,CACjD,OAAOoB,EAAW,YAAYpB,CAAG,CACnC,CAKA,eAAsBX,IAAe,CACnC,OAAO+B,EAAW,MAAK,CACzB,CAKA,eAAsB5B,IAAkB,CACtC,OAAO4B,EAAW,SAAQ,CAC5B,CAKM,SAAU3B,GAAiBO,EAAW,CAC1C,OAAOqD,GAAe,UAAUrD,CAAG,CACrC,CAKM,SAAUZ,GAAcY,EAAW,CACvCqD,GAAe,OAAOrD,CAAG,CAC3B,CAKA,eAAsBN,GAAkBM,EAAW,CACjD,OAAOqD,GAAe,IAAIrD,CAAG,CAC/B,CA95BA,IAsGMwD,GAEAC,EACAC,EACAC,EAKAC,GA+PAxC,EAqRAyC,GAmJAR,GAtxBNS,GAAAC,GAAA,kBAsGMP,GAAU,uBAEVC,EAAa,OACbC,EAAe,SACfC,EAAc,iBAKdC,GAAN,KAAgB,CAAhB,cACUI,EAAA,UAAyB,MACzBA,EAAA,iBAAyC,MAKzC,MAAM,QAAM,CAClB,OAAI,KAAK,GAAW,KAAK,GACrB,KAAK,UAAkB,KAAK,WAEhC,KAAK,UAAY,IAAI,QAAQ,CAACC,EAASC,IAAU,CAC/C,IAAMC,EAAU,UAAU,KAAKX,GAAS,CAAU,EAElDW,EAAQ,gBAAmBC,GAAS,CAClC,IAAMC,EAAMD,EAAM,OAA4B,OAGzCC,EAAG,iBAAiB,SAASZ,CAAU,GAC1CY,EAAG,kBAAkBZ,EAAY,CAAE,QAAS,KAAK,CAAE,EAIhDY,EAAG,iBAAiB,SAASX,CAAY,GACzBW,EAAG,kBAAkBX,EAAc,CAAE,QAAS,CAAC,MAAO,OAAO,CAAC,CAAE,EACxE,YAAY,MAAO,MAAO,CAAE,OAAQ,EAAK,CAAE,EAInDW,EAAG,iBAAiB,SAASV,CAAW,GAC3CU,EAAG,kBAAkBV,EAAa,CAAE,QAAS,KAAK,CAAE,CAExD,EAEAQ,EAAQ,UAAY,IAAK,CACvB,KAAK,GAAKA,EAAQ,OAClBF,EAAQ,KAAK,EAAE,CACjB,EAEAE,EAAQ,QAAU,IAAMD,EAAOC,EAAQ,KAAK,CAC9C,CAAC,EAEM,KAAK,UACd,CAKA,MAAM,QAAQnE,EAAW,CACvB,IAAMqE,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CAGrC,IAAMC,EAFKE,EAAG,YAAYZ,EAAY,UAAU,EAC/B,YAAYA,CAAU,EACjB,IAAIzD,CAAG,EAC7BmE,EAAQ,UAAY,IAAMF,EAAQE,EAAQ,QAAU,IAAI,EACxDA,EAAQ,QAAU,IAAMD,EAAOC,EAAQ,KAAK,CAC9C,CAAC,CACH,CAKA,MAAM,SAASG,EAAqB,CAClC,IAAMD,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CACrC,IAAMK,EAAKF,EAAG,YAAYZ,EAAY,WAAW,EACnCc,EAAG,YAAYd,CAAU,EACjC,IAAIa,CAAI,EACdC,EAAG,WAAa,IAAMN,EAAO,EAC7BM,EAAG,QAAU,IAAML,EAAOK,EAAG,KAAK,CACpC,CAAC,CACH,CAKA,MAAM,UAAUvE,EAAawE,EAAehC,EAAiB,CAC3D,IAAM6B,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CACrC,IAAMK,EAAKF,EAAG,YAAYX,EAAc,WAAW,EACrCa,EAAG,YAAYb,CAAY,EACnC,IAAI,CAAE,IAAA1D,EAAK,MAAAwE,EAAO,KAAAhC,CAAI,CAAE,EAC9B+B,EAAG,WAAa,IAAMN,EAAO,EAC7BM,EAAG,QAAU,IAAML,EAAOK,EAAG,KAAK,CACpC,CAAC,CACH,CAKA,MAAM,UAAUvE,EAAW,CACzB,IAAMqE,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CAIrC,IAAMC,EAHKE,EAAG,YAAYX,EAAc,UAAU,EACjC,YAAYA,CAAY,EACrB,MAAM,KAAK,EACT,OAAO1D,CAAG,EAEhCmE,EAAQ,UAAY,IAAK,CACvB,IAAMM,EAAUN,EAAQ,OAExBM,EAAQ,KAAK,CAACC,EAAGC,IAAMD,EAAE,MAAQC,EAAE,KAAK,EACxCV,EAAQQ,EAAQ,IAAIG,GAAKA,EAAE,IAAI,CAAC,CAClC,EACAT,EAAQ,QAAU,IAAMD,EAAOC,EAAQ,KAAK,CAC9C,CAAC,CACH,CAKA,MAAM,SAASnE,EAAW,CACxB,IAAMsE,EAAO,MAAM,KAAK,QAAQtE,CAAG,EACnC,GAAI,CAACsE,GAAQ,CAACA,EAAK,SAAU,OAAO,KAEpC,IAAMhD,EAAS,MAAM,KAAK,UAAUtB,CAAG,EACvC,GAAIsB,EAAO,SAAW,EAAG,OAAO,KAGhC,IAAML,EAAYK,EAAO,OAAO,CAACuD,EAAKvC,IAAUuC,EAAMvC,EAAM,WAAY,CAAC,EACnEG,EAAS,IAAI,WAAWxB,CAAS,EACnCyB,EAAS,EAEb,QAAWJ,KAAShB,EAClBmB,EAAO,IAAI,IAAI,WAAWH,CAAK,EAAGI,CAAM,EACxCA,GAAUJ,EAAM,WAGlB,OAAOG,EAAO,MAChB,CAKA,MAAM,kBAAkBtB,EAAoB,CAC1C,IAAMkD,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CACrC,IAAMK,EAAKF,EAAG,YAAYV,EAAa,WAAW,EACpCY,EAAG,YAAYZ,CAAW,EAClC,IAAIxC,CAAK,EACfoD,EAAG,WAAa,IAAMN,EAAO,EAC7BM,EAAG,QAAU,IAAML,EAAOK,EAAG,KAAK,CACpC,CAAC,CACH,CAKA,MAAM,iBAAiBvE,EAAW,CAChC,IAAMqE,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CAGrC,IAAMC,EAFKE,EAAG,YAAYV,EAAa,UAAU,EAChC,YAAYA,CAAW,EAClB,IAAI3D,CAAG,EAC7BmE,EAAQ,UAAY,IAAMF,EAAQE,EAAQ,QAAU,IAAI,EACxDA,EAAQ,QAAU,IAAMD,EAAOC,EAAQ,KAAK,CAC9C,CAAC,CACH,CAKA,MAAM,oBAAoBnE,EAAW,CACnC,IAAMqE,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CACrC,IAAMK,EAAKF,EAAG,YAAYV,EAAa,WAAW,EACpCY,EAAG,YAAYZ,CAAW,EAClC,OAAO3D,CAAG,EAChBuE,EAAG,WAAa,IAAMN,EAAO,EAC7BM,EAAG,QAAU,IAAML,EAAOK,EAAG,KAAK,CACpC,CAAC,CACH,CAKA,MAAM,YAAYvE,EAAW,CAC3B,IAAMqE,EAAK,MAAM,KAAK,OAAM,EAG5B,MAAM,IAAI,QAAc,CAACJ,EAASC,IAAU,CAC1C,IAAMK,EAAKF,EAAG,YAAYZ,EAAY,WAAW,EACnCc,EAAG,YAAYd,CAAU,EACjC,OAAOzD,CAAG,EAChBuE,EAAG,WAAa,IAAMN,EAAO,EAC7BM,EAAG,QAAU,IAAML,EAAOK,EAAG,KAAK,CACpC,CAAC,GAGc,MAAM,KAAK,UAAUvE,CAAG,GAC5B,OAAS,GAClB,MAAM,IAAI,QAAc,CAACiE,EAASC,IAAU,CAC1C,IAAMK,EAAKF,EAAG,YAAYX,EAAc,WAAW,EAG7CS,EAFQI,EAAG,YAAYb,CAAY,EACrB,MAAM,KAAK,EACT,WAAW,YAAY,KAAK1D,CAAG,CAAC,EAEtDmE,EAAQ,UAAaC,GAAS,CAC5B,IAAMU,EAAUV,EAAM,OAA0C,OAC5DU,IACFA,EAAO,OAAM,EACbA,EAAO,SAAQ,EAEnB,EAEAP,EAAG,WAAa,IAAMN,EAAO,EAC7BM,EAAG,QAAU,IAAML,EAAOK,EAAG,KAAK,CACpC,CAAC,EAIH,MAAM,KAAK,oBAAoBvE,CAAG,CACpC,CAKA,MAAM,OAAK,CACT,IAAMqE,EAAK,MAAM,KAAK,OAAM,EAEtBU,EAAS,CAACtB,EAAYC,EAAcC,CAAW,EACrD,QAAWqB,KAAaD,EACtB,MAAM,IAAI,QAAc,CAACd,EAASC,IAAU,CAC1C,IAAMK,EAAKF,EAAG,YAAYW,EAAW,WAAW,EAClCT,EAAG,YAAYS,CAAS,EAChC,MAAK,EACXT,EAAG,WAAa,IAAMN,EAAO,EAC7BM,EAAG,QAAU,IAAML,EAAOK,EAAG,KAAK,CACpC,CAAC,CAEL,CAKA,MAAM,UAAQ,CACZ,IAAMF,EAAK,MAAM,KAAK,OAAM,EAC5B,OAAO,IAAI,QAAQ,CAACJ,EAASC,IAAU,CAGrC,IAAMC,EAFKE,EAAG,YAAYZ,EAAY,UAAU,EAC/B,YAAYA,CAAU,EACjB,OAAM,EAE5BU,EAAQ,UAAY,IAAK,CACvB,IAAMc,EAAQd,EAAQ,OACtBF,EAAQ,CACN,OAAQgB,EAAM,OAAOC,GAAKA,EAAE,QAAQ,EAAE,OACtC,UAAWD,EAAM,OAAO,CAACJ,EAAKK,IAAML,GAAOK,EAAE,SAAWA,EAAE,KAAO,GAAI,CAAC,EACvE,CACH,EACAf,EAAQ,QAAU,IAAMD,EAAOC,EAAQ,KAAK,CAC9C,CAAC,CACH,GAII/C,EAAa,IAAIwC,GAqRjBC,GAAN,KAAoB,CAApB,cACUG,EAAA,aAAkC,IAAI,KACtCA,EAAA,aAAkB,CAAA,GAClBA,EAAA,qBAAgB,GAChBA,EAAA,mBAAc,GAKtB,QAAQhE,EAAaY,EAA0B,CAAA,EAAE,CAE/C,IAAMuE,EAAW,KAAK,MAAM,IAAInF,CAAG,EACnC,GAAImF,EACF,OAAOA,EAAS,QAIlB,IAAIlB,EACAC,EAEEkB,EAAU,IAAI,QAAqB,CAACC,EAAKC,IAAO,CACpDrB,EAAUoB,EACVnB,EAASoB,CACX,CAAC,EAEKC,EAAoB,CACxB,IAAAvF,EACA,SAAUY,EAAQ,UAAY,EAC9B,QAAAA,EACA,QAAAwE,EACA,QAAAnB,EACA,OAAAC,EACA,OAAQ,WAGV,KAAK,MAAM,IAAIlE,EAAKuF,CAAI,EAGxB,IAAMC,EAAc,KAAK,MAAM,UAAUC,GAAI,CAC3C,IAAMC,EAAI,KAAK,MAAM,IAAID,CAAC,EAC1B,OAAOC,GAAKA,EAAE,SAAWH,EAAK,QAChC,CAAC,EAED,OAAIC,IAAgB,GAClB,KAAK,MAAM,KAAKxF,CAAG,EAEnB,KAAK,MAAM,OAAOwF,EAAa,EAAGxF,CAAG,EAIvC,KAAK,aAAY,EAEVoF,CACT,CAKQ,MAAM,cAAY,CACxB,KAAO,KAAK,MAAM,OAAS,GAAK,KAAK,YAAc,KAAK,eAAe,CACrE,IAAMpF,EAAM,KAAK,MAAM,MAAK,EAC5B,GAAI,CAACA,EAAK,MAEV,IAAMuF,EAAO,KAAK,MAAM,IAAIvF,CAAG,EAC3B,CAACuF,GAAQA,EAAK,SAAW,YAE7B,KAAK,cACLA,EAAK,OAAS,UAEd,KAAK,aAAaA,CAAI,EAAE,QAAQ,IAAK,CACnC,KAAK,cACL,KAAK,aAAY,CACnB,CAAC,EACH,CACF,CAKQ,MAAM,aAAaA,EAAiB,CAC1C,GAAI,CACF,IAAM/C,EAAO,MAAM5C,EAAc2F,EAAK,IAAKA,EAAK,OAAO,EACvDA,EAAK,OAAS,WACdA,EAAK,QAAQ/C,CAAI,CACnB,OAASmD,EAAO,CACdJ,EAAK,OAAS,QACdA,EAAK,OAAOI,aAAiB,MAAQA,EAAQ,IAAI,MAAM,OAAOA,CAAK,CAAC,CAAC,CACvE,CACF,CAKA,YAAY3F,EAAW,CAErB,OADa,KAAK,MAAM,IAAIA,CAAG,GAClB,SAAW,UAC1B,CAKA,UAAUA,EAAW,CAEnB,OADa,KAAK,MAAM,IAAIA,CAAG,GAClB,QAAU,WACzB,CAKA,MAAM,IAAIA,EAAW,CACnB,IAAMuF,EAAO,KAAK,MAAM,IAAIvF,CAAG,EAC/B,OAAKuF,IAEDA,EAAK,SAAW,YAAcA,EAAK,SAAW,WACzCA,EAAK,QAHI,IAOpB,CAKA,OAAOvF,EAAW,CAChB,IAAMuF,EAAO,KAAK,MAAM,IAAIvF,CAAG,EAC3BuF,GAAQA,EAAK,SAAW,YAC1B,KAAK,MAAM,OAAOvF,CAAG,EACrB,KAAK,MAAQ,KAAK,MAAM,OAAOyF,GAAKA,IAAMzF,CAAG,EAC7CuF,EAAK,OAAO,IAAI,MAAM,mBAAmB,CAAC,EAE9C,CAKA,OAAK,CACH,OAAW,CAAC,CAAEA,CAAI,IAAK,KAAK,MACtBA,EAAK,SAAW,WAClBA,EAAK,OAAO,IAAI,MAAM,iBAAiB,CAAC,EAG5C,KAAK,MAAM,MAAK,EAChB,KAAK,MAAQ,CAAA,CACf,GAIIlC,GAAiB,IAAIQ,KC9XrB,IAAO+B,EAAP,cAA6B,KAAK,CACtC,YACEC,EACgBC,EACAC,EAAiC,CAEjD,MAAMF,CAAO,EAHGG,EAAA,aACAA,EAAA,gBADA,KAAA,KAAAF,EACA,KAAA,QAAAC,EAGhB,KAAK,KAAO,eACd,GAMWE,EAAa,CAExB,sBAAuB,wBACvB,oBAAqB,sBACrB,wBAAyB,0BAGzB,gBAAiB,kBACjB,kBAAmB,oBACnB,qBAAsB,uBACtB,iBAAkB,mBAGlB,iBAAkB,mBAClB,kBAAmB,oBACnB,oBAAqB,sBAGrB,cAAe,gBACf,qBAAsB,uBAGtB,sBAAuB,wBACvB,sBAAuB,wBACvB,gBAAiB,kBAGjB,uBAAwB,yBACxB,uBAAwB,yBAGxB,iBAAkB,mBAClB,gBAAiB,kBACjB,cAAe,iBCvbjB,IAAIC,GAAkB,EAKtB,SAASC,IAAgB,CACvB,MAAO,UAAU,EAAED,EAAe,IAAI,KAAK,IAAG,EAAG,SAAS,EAAE,CAAC,EAC/D,CAKA,SAASE,GAAyBC,EAAe,CAC/C,OAAQA,EAAO,CACb,IAAK,UACH,OAAO,aACT,IAAK,UAEH,OAAO,aACT,IAAK,QACH,OAAO,WACT,IAAK,QACH,OAAO,cACT,IAAK,QACL,IAAK,OACH,OAAO,WACT,IAAK,OACH,OAAO,UACT,QACE,MAAM,IAAIC,EACR,0BAA0BD,CAAK,GAC/BE,EAAW,iBACX,CAAE,MAAAF,CAAK,CAAE,CAEf,CACF,CAKA,SAASG,EAAcC,EAAY,CACjC,OAAIA,EAAM,SAAW,EAAU,EACxBA,EAAM,OAAO,CAACC,EAAKC,IAAQD,EAAMC,EAAK,CAAC,CAChD,CAKA,SAASC,GAAcH,EAAY,CACjC,QAASI,EAAI,EAAGA,EAAIJ,EAAM,OAAQI,IAAK,CACrC,IAAMF,EAAMF,EAAMI,CAAC,EACnB,GAAIF,IAAQ,QAAa,CAAC,OAAO,UAAUA,CAAG,GAAKA,EAAM,EACvD,MAAM,IAAIL,EACR,oCAAoCO,CAAC,KAAKF,CAAG,GAC7CJ,EAAW,iBACX,CAAE,MAAAE,EAAO,MAAOI,EAAG,UAAWF,CAAG,CAAE,CAGzC,CACF,CAKM,IAAOG,EAAP,MAAOC,CAAc,CAQzB,YACEC,EACAP,EACAJ,EAAkB,UAAS,CAVpBY,EAAA,WACAA,EAAA,cACAA,EAAA,cACAA,EAAA,aACDA,EAAA,cACAA,EAAA,mBAAuB,IAO7BL,GAAcH,CAAK,EAEnB,KAAK,GAAKN,GAAgB,EAC1B,KAAK,MAAQE,EACb,KAAK,MAAQ,OAAO,OAAO,CAAC,GAAGI,CAAK,CAAC,EACrC,KAAK,KAAOD,EAAc,KAAK,KAAK,EAGpC,IAAMU,EAAe,KAAK,KAC1B,GAAIF,EAAK,SAAWE,EAClB,MAAM,IAAIZ,EACR,gBAAgBU,EAAK,MAAM,0BAA0B,KAAK,UAAUP,CAAK,CAAC,cAAcS,CAAY,IACpGX,EAAW,sBACX,CAAE,WAAYS,EAAK,OAAQ,aAAAE,EAAc,MAAAT,CAAK,CAAE,EAKpD,GAAIO,aAAgB,MAAO,CACzB,IAAMG,EAAiBf,GAAyBC,CAAK,EAGrD,GAFA,KAAK,MAAQ,IAAIc,EAAeH,EAAK,MAAM,EAEvCX,IAAU,QAAS,CAErB,IAAMe,EAAa,KAAK,MACxB,QAASP,EAAI,EAAGA,EAAIG,EAAK,OAAQH,IAC/BO,EAAWP,CAAC,EAAI,OAAO,KAAK,MAAMG,EAAKH,CAAC,GAAK,CAAC,CAAC,CAEnD,KACE,SAAS,EAAI,EAAG,EAAIG,EAAK,OAAQ,IAC9B,KAAK,MAAuB,CAAC,EAAIA,EAAK,CAAC,GAAK,CAGnD,MACE,KAAK,MAAQA,CAEjB,CAEA,IAAI,MAAI,CACN,YAAK,cAAa,EACX,KAAK,KACd,CAEA,IAAI,YAAU,CACZ,OAAO,KAAK,WACd,CAKQ,eAAa,CACnB,GAAI,KAAK,YACP,MAAM,IAAIV,EACR,gCACAC,EAAW,gBACX,CAAE,SAAU,KAAK,EAAE,CAAE,CAG3B,CAKA,gBAAc,CAGZ,GAFA,KAAK,cAAa,EAEd,KAAK,iBAAiB,aACxB,OAAO,KAAK,MAGd,IAAMc,EAAS,IAAI,aAAa,KAAK,IAAI,EACzC,QAASR,EAAI,EAAGA,EAAI,KAAK,KAAMA,IAC7BQ,EAAOR,CAAC,EAAI,OAAO,KAAK,MAAMA,CAAC,GAAK,CAAC,EAEvC,OAAOQ,CACT,CAKA,SAAO,CAEL,GADA,KAAK,cAAa,EACd,KAAK,QAAU,QAAS,CAE1B,IAAMD,EAAa,KAAK,MAClBC,EAAmB,CAAA,EACzB,QAASR,EAAI,EAAGA,EAAIO,EAAW,OAAQP,IACrCQ,EAAO,KAAK,OAAOD,EAAWP,CAAC,CAAC,CAAC,EAEnC,OAAOQ,CACT,CACA,OAAO,MAAM,KAAK,KAAK,KAAqB,CAC9C,CAKA,OAAK,CACH,KAAK,cAAa,EAElB,IAAMF,EAAiB,KAAK,MAAM,YAC5BG,EAAa,IAAIH,EAAe,KAAK,KAAK,EAChD,OAAO,IAAIJ,EAAeO,EAAY,KAAK,MAAO,KAAK,KAAK,CAC9D,CAKA,SAAO,CACA,KAAK,cACR,KAAK,YAAc,GAEnB,OAAO,OAAO,KAAM,CAAE,MAAO,IAAI,CAAE,EAEvC,CAKA,OAAOC,EAAiB,CAGtB,GAFA,KAAK,cAAa,EAEdA,EAAQ,SAAW,KAAK,MAAM,OAChC,MAAM,IAAIjB,EACR,YAAY,KAAK,MAAM,MAAM,iBAAiBiB,EAAQ,MAAM,GAC5DhB,EAAW,iBACX,CAAE,gBAAiB,KAAK,MAAM,OAAQ,WAAYgB,EAAQ,MAAM,CAAE,EAItE,IAAIC,EAAY,EACZC,EAAS,EAEb,QAASZ,EAAI,KAAK,MAAM,OAAS,EAAGA,GAAK,EAAGA,IAAK,CAC/C,IAAMa,EAAMH,EAAQV,CAAC,GAAK,EACpBF,EAAM,KAAK,MAAME,CAAC,GAAK,EAE7B,GAAIa,EAAM,GAAKA,GAAOf,EACpB,MAAM,IAAIL,EACR,SAASoB,CAAG,gCAAgCb,CAAC,cAAcF,CAAG,GAC9DJ,EAAW,iBACX,CAAE,MAAOmB,EAAK,UAAWb,EAAG,KAAMF,CAAG,CAAE,EAI3Ca,GAAaE,EAAMD,EACnBA,GAAUd,CACZ,CAEA,OAAO,OAAO,KAAK,MAAMa,CAAS,GAAK,CAAC,CAC1C,CAKA,IAAIG,KAAkBJ,EAAiB,CAGrC,GAFA,KAAK,cAAa,EAEdA,EAAQ,SAAW,KAAK,MAAM,OAChC,MAAM,IAAIjB,EACR,YAAY,KAAK,MAAM,MAAM,iBAAiBiB,EAAQ,MAAM,GAC5DhB,EAAW,iBACX,CAAE,gBAAiB,KAAK,MAAM,OAAQ,WAAYgB,EAAQ,MAAM,CAAE,EAItE,IAAIC,EAAY,EACZC,EAAS,EAEb,QAASZ,EAAI,KAAK,MAAM,OAAS,EAAGA,GAAK,EAAGA,IAAK,CAC/C,IAAMa,EAAMH,EAAQV,CAAC,GAAK,EACpBF,EAAM,KAAK,MAAME,CAAC,GAAK,EAE7B,GAAIa,EAAM,GAAKA,GAAOf,EACpB,MAAM,IAAIL,EACR,SAASoB,CAAG,gCAAgCb,CAAC,cAAcF,CAAG,GAC9DJ,EAAW,iBACX,CAAE,MAAOmB,EAAK,UAAWb,EAAG,KAAMF,CAAG,CAAE,EAI3Ca,GAAaE,EAAMD,EACnBA,GAAUd,CACZ,CAEC,KAAK,MAAuBa,CAAS,EAAIG,CAC5C,CAKA,QAAQC,EAAe,CACrB,KAAK,cAAa,EAElB,IAAMC,EAAUrB,EAAcoB,CAAQ,EACtC,GAAIC,IAAY,KAAK,KACnB,MAAM,IAAIvB,EACR,iCAAiC,KAAK,IAAI,aAAa,KAAK,UAAUsB,CAAQ,CAAC,UAAUC,CAAO,IAChGtB,EAAW,sBACX,CAAE,YAAa,KAAK,KAAM,QAAAsB,EAAS,SAAAD,CAAQ,CAAE,EAIjD,IAAMT,EAAiB,KAAK,MAAM,YAC5BG,EAAa,IAAIH,EAAe,KAAK,KAAK,EAChD,OAAO,IAAIJ,EAAeO,EAAYM,EAAU,KAAK,KAAK,CAC5D,CAKA,WAAS,CAGP,GAFA,KAAK,cAAa,EAEd,KAAK,MAAM,SAAW,EACxB,MAAM,IAAItB,EACR,uDACAC,EAAW,gBACX,CAAE,MAAO,KAAK,KAAK,CAAE,EAIzB,GAAM,CAACuB,EAAMC,CAAI,EAAI,KAAK,MACpBV,EAAS,IAAI,aAAa,KAAK,IAAI,EAEzC,QAASR,EAAI,EAAGA,EAAIiB,EAAMjB,IACxB,QAASmB,EAAI,EAAGA,EAAID,EAAMC,IACxBX,EAAOW,EAAIF,EAAOjB,CAAC,EAAI,OAAO,KAAK,MAAMA,EAAIkB,EAAOC,CAAC,GAAK,CAAC,EAI/D,OAAO,IAAIjB,EAAeM,EAAQ,CAACU,EAAMD,CAAI,EAAG,KAAK,KAAK,CAC5D,CAKA,UAAQ,CACN,MAAO,iBAAiB,KAAK,MAAM,KAAK,IAAI,CAAC,YAAY,KAAK,KAAK,GACrE,GAUI,SAAUG,GACdjB,EACAP,EACAJ,EAAkB,UAAS,CAG3B,GAAI,MAAM,QAAQW,CAAI,GAAKA,EAAK,OAAS,GAAK,MAAM,QAAQA,EAAK,CAAC,CAAC,EAAG,CACpE,IAAMc,EAAOd,EAAK,OACZe,EAAQf,EAAK,CAAC,EAAe,OAC7BkB,EAAqB,CAAA,EAE3B,QAAWC,KAAOnB,EAAoB,CACpC,GAAImB,EAAI,SAAWJ,EACjB,MAAM,IAAIzB,EACR,gDACAC,EAAW,gBAAgB,EAG/B2B,EAAS,KAAK,GAAGC,CAAG,CACtB,CAEA,OAAO,IAAIrB,EAAeoB,EAAUzB,GAAS,CAACqB,EAAMC,CAAI,EAAG1B,CAAK,CAClE,CAGA,IAAM+B,EAAgB3B,GAAS,CAACO,EAAK,MAAM,EAC3C,OAAO,IAAIF,EAAeE,EAA+BoB,EAAe/B,CAAK,CAC/E,CAKM,SAAUgC,GAAM5B,EAAcJ,EAAkB,UAAS,CAC7D,IAAMiC,EAAO9B,EAAcC,CAAK,EAC1BU,EAAiBf,GAAyBC,CAAK,EAC/CW,EAAO,IAAIG,EAAemB,CAAI,EACpC,OAAO,IAAIxB,EAAeE,EAAMP,EAAOJ,CAAK,CAC9C,CAKM,SAAUkC,GAAK9B,EAAcJ,EAAkB,UAAS,CAC5D,IAAMiC,EAAO9B,EAAcC,CAAK,EAC1BU,EAAiBf,GAAyBC,CAAK,EAC/CW,EAAO,IAAIG,EAAemB,CAAI,EACpC,OAAAtB,EAAK,KAAK,CAAU,EACb,IAAIF,EAAeE,EAAMP,EAAOJ,CAAK,CAC9C,CAKM,SAAUmC,GACd/B,EACAkB,EACAtB,EAAkB,UAAS,CAE3B,IAAMiC,EAAO9B,EAAcC,CAAK,EAC1BU,EAAiBf,GAAyBC,CAAK,EAC/CW,EAAO,IAAIG,EAAemB,CAAI,EACpC,OAAAtB,EAAK,KAAKW,CAAc,EACjB,IAAIb,EAAeE,EAAMP,EAAOJ,CAAK,CAC9C,CAKM,SAAUoC,GAAOhC,EAAcJ,EAAkB,UAAS,CAC9D,IAAMiC,EAAO9B,EAAcC,CAAK,EAC1BO,EAAO,IAAI,aAAasB,CAAI,EAClC,QAASzB,EAAI,EAAGA,EAAIyB,EAAMzB,IACxBG,EAAKH,CAAC,EAAI,KAAK,OAAM,EAEvB,OAAO,IAAIC,EAAeE,EAAMP,EAAOJ,CAAK,CAC9C,CAKM,SAAUqC,GAAMjC,EAAcJ,EAAkB,UAAS,CAC7D,IAAMiC,EAAO9B,EAAcC,CAAK,EAC1BO,EAAO,IAAI,aAAasB,CAAI,EAGlC,QAASzB,EAAI,EAAGA,EAAIyB,EAAMzB,GAAK,EAAG,CAChC,IAAM8B,EAAK,KAAK,OAAM,EAChBC,EAAK,KAAK,OAAM,EAChBC,EAAI,KAAK,KAAK,GAAK,KAAK,IAAIF,CAAE,CAAC,EAC/BG,EAAQ,EAAI,KAAK,GAAKF,EAE5B5B,EAAKH,CAAC,EAAIgC,EAAI,KAAK,IAAIC,CAAK,EACxBjC,EAAI,EAAIyB,IACVtB,EAAKH,EAAI,CAAC,EAAIgC,EAAI,KAAK,IAAIC,CAAK,EAEpC,CAEA,OAAO,IAAIhC,EAAeE,EAAMP,EAAOJ,CAAK,CAC9C,CAKM,SAAU0C,GACdC,EACAC,EACAC,EAAe,EACf7C,EAAkB,UAAS,CAEvB4C,IAAS,SACXA,EAAOD,EACPA,EAAQ,GAGV,IAAMV,EAAO,KAAK,MAAMW,EAAOD,GAASE,CAAI,EACtClC,EAAO,IAAI,aAAasB,CAAI,EAElC,QAAS,EAAI,EAAG,EAAIA,EAAM,IACxBtB,EAAK,CAAC,EAAIgC,EAAQ,EAAIE,EAGxB,OAAO,IAAIpC,EAAeE,EAAM,CAACsB,CAAI,EAAGjC,CAAK,CAC/C,CAKM,SAAU8C,GACdH,EACAC,EACAG,EAAc,GACd/C,EAAkB,UAAS,CAE3B,IAAMW,EAAO,IAAI,aAAaoC,CAAG,EAC3BF,GAAQD,EAAOD,IAAUI,EAAM,GAErC,QAAS,EAAI,EAAG,EAAIA,EAAK,IACvBpC,EAAK,CAAC,EAAIgC,EAAQ,EAAIE,EAGxB,OAAO,IAAIpC,EAAeE,EAAM,CAACoC,CAAG,EAAG/C,CAAK,CAC9C,CAKM,SAAUgD,GAAIC,EAAWjD,EAAkB,UAAS,CACxD,IAAMW,EAAO,IAAI,aAAasC,EAAIA,CAAC,EAEnC,QAASzC,EAAI,EAAGA,EAAIyC,EAAGzC,IACrBG,EAAKH,EAAIyC,EAAIzC,CAAC,EAAI,EAGpB,OAAO,IAAIC,EAAeE,EAAM,CAACsC,EAAGA,CAAC,EAAGjD,CAAK,CAC/C,CASM,SAAUkD,GAAIC,EAAmBC,EAA0B,CAC/D,GAAI,OAAOA,GAAM,SAAU,CACzB,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EAC9B,QAAS3C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,GAAK4C,EAEhC,OAAO,IAAI3C,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAEA,GAAIA,EAAE,OAASC,EAAE,KACf,MAAM,IAAInD,EACR,sDACAC,EAAW,sBACX,CAAE,OAAQiD,EAAE,MAAO,OAAQC,EAAE,KAAK,CAAE,EAIxC,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EACxBG,EAAQF,EAAE,eAAc,EAE9B,QAAS5C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,IAAM8C,EAAM9C,CAAC,GAAK,GAG7C,OAAO,IAAIC,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAKM,SAAUI,GAAIJ,EAAmBC,EAA0B,CAC/D,GAAI,OAAOA,GAAM,SAAU,CACzB,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EAC9B,QAAS3C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,GAAK4C,EAEhC,OAAO,IAAI3C,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAEA,GAAIA,EAAE,OAASC,EAAE,KACf,MAAM,IAAInD,EACR,sDACAC,EAAW,sBACX,CAAE,OAAQiD,EAAE,MAAO,OAAQC,EAAE,KAAK,CAAE,EAIxC,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EACxBG,EAAQF,EAAE,eAAc,EAE9B,QAAS5C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,IAAM8C,EAAM9C,CAAC,GAAK,GAG7C,OAAO,IAAIC,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAKM,SAAUK,GAAIL,EAAmBC,EAA0B,CAC/D,GAAI,OAAOA,GAAM,SAAU,CACzB,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EAC9B,QAAS3C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,GAAK4C,EAEhC,OAAO,IAAI3C,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAEA,GAAIA,EAAE,OAASC,EAAE,KACf,MAAM,IAAInD,EACR,sDACAC,EAAW,sBACX,CAAE,OAAQiD,EAAE,MAAO,OAAQC,EAAE,KAAK,CAAE,EAIxC,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EACxBG,EAAQF,EAAE,eAAc,EAE9B,QAAS5C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,IAAM8C,EAAM9C,CAAC,GAAK,GAG7C,OAAO,IAAIC,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAKM,SAAUM,GAAIN,EAAmBC,EAA0B,CAC/D,GAAI,OAAOA,GAAM,SAAU,CACzB,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EAC9B,QAAS3C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,GAAK4C,EAEhC,OAAO,IAAI3C,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAEA,GAAIA,EAAE,OAASC,EAAE,KACf,MAAM,IAAInD,EACR,sDACAC,EAAW,sBACX,CAAE,OAAQiD,EAAE,MAAO,OAAQC,EAAE,KAAK,CAAE,EAIxC,IAAMpC,EAAS,IAAI,aAAamC,EAAE,IAAI,EAChCE,EAAQF,EAAE,eAAc,EACxBG,EAAQF,EAAE,eAAc,EAE9B,QAAS5C,EAAI,EAAGA,EAAI2C,EAAE,KAAM3C,IAC1BQ,EAAOR,CAAC,GAAK6C,EAAM7C,CAAC,GAAK,IAAM8C,EAAM9C,CAAC,GAAK,GAG7C,OAAO,IAAIC,EAAeO,EAAQmC,EAAE,MAAOA,EAAE,KAAK,CACpD,CAKM,SAAUO,GAAOP,EAAmBC,EAAiB,CACzD,GAAID,EAAE,MAAM,SAAW,GAAKC,EAAE,MAAM,SAAW,EAC7C,MAAM,IAAInD,EACR,6BACAC,EAAW,iBACX,CAAE,OAAQiD,EAAE,MAAO,OAAQC,EAAE,KAAK,CAAE,EAIxC,GAAM,CAACO,EAAGC,CAAE,EAAIT,EAAE,MACZ,CAACU,EAAIZ,CAAC,EAAIG,EAAE,MAElB,GAAIQ,IAAOC,EACT,MAAM,IAAI5D,EACR,uDAAuD0D,CAAC,IAAIC,CAAE,QAAQC,CAAE,IAAIZ,CAAC,IAC7E/C,EAAW,sBACX,CAAE,OAAQiD,EAAE,MAAO,OAAQC,EAAE,KAAK,CAAE,EAIxC,IAAMpC,EAAS,IAAI,aAAa2C,EAAIV,CAAC,EAC/BI,EAAQF,EAAE,eAAc,EACxBG,EAAQF,EAAE,eAAc,EAE9B,QAAS5C,EAAI,EAAGA,EAAImD,EAAGnD,IACrB,QAASmB,EAAI,EAAGA,EAAIsB,EAAGtB,IAAK,CAC1B,IAAImC,EAAM,EACV,QAASC,EAAI,EAAGA,EAAIH,EAAIG,IACtBD,IAAQT,EAAM7C,EAAIoD,EAAKG,CAAC,GAAK,IAAMT,EAAMS,EAAId,EAAItB,CAAC,GAAK,GAEzDX,EAAOR,EAAIyC,EAAItB,CAAC,EAAImC,CACtB,CAGF,OAAO,IAAIrD,EAAeO,EAAQ,CAAC2C,EAAGV,CAAC,EAAGE,EAAE,KAAK,CACnD,CAKM,SAAUa,EAAQC,EAAmBC,EAAe,GAAE,CAC1D,IAAMvD,EAAOsD,EAAE,eAAc,EACvBjD,EAAS,IAAI,aAAaiD,EAAE,IAAI,EAGhCE,EAAaD,EAAO,EAAID,EAAE,MAAM,OAASC,EAAOA,EAEtD,GAAIC,EAAa,GAAKA,GAAcF,EAAE,MAAM,OAC1C,MAAM,IAAIhE,EACR,gBAAgBiE,CAAI,oBAAoBD,EAAE,MAAM,MAAM,cACtD/D,EAAW,iBACX,CAAE,KAAAgE,EAAM,MAAOD,EAAE,KAAK,CAAE,EAK5B,GAAIA,EAAE,MAAM,SAAW,EAAG,CACxB,IAAIG,EAAM,KACV,QAAS5D,EAAI,EAAGA,EAAIyD,EAAE,KAAMzD,KACrBG,EAAKH,CAAC,GAAK,GAAK4D,IAAKA,EAAMzD,EAAKH,CAAC,GAAK,GAG7C,IAAIsD,EAAM,EACV,QAAStD,EAAI,EAAGA,EAAIyD,EAAE,KAAMzD,IAC1BQ,EAAOR,CAAC,EAAI,KAAK,KAAKG,EAAKH,CAAC,GAAK,GAAK4D,CAAG,EACzCN,GAAO9C,EAAOR,CAAC,GAAK,EAGtB,QAASA,EAAI,EAAGA,EAAIyD,EAAE,KAAMzD,IAC1BQ,EAAOR,CAAC,GAAKQ,EAAOR,CAAC,GAAK,GAAKsD,EAGjC,OAAO,IAAIrD,EAAeO,EAAQiD,EAAE,MAAOA,EAAE,KAAK,CACpD,CAGA,GAAIA,EAAE,MAAM,SAAW,GAAKE,IAAe,EAAG,CAC5C,GAAM,CAAC1C,EAAMC,CAAI,EAAIuC,EAAE,MAEvB,QAASzD,EAAI,EAAGA,EAAIiB,EAAMjB,IAAK,CAC7B,IAAI4D,EAAM,KACV,QAASzC,EAAI,EAAGA,EAAID,EAAMC,KACnBhB,EAAKH,EAAIkB,EAAOC,CAAC,GAAK,GAAKyC,IAAKA,EAAMzD,EAAKH,EAAIkB,EAAOC,CAAC,GAAK,GAGnE,IAAImC,EAAM,EACV,QAASnC,EAAI,EAAGA,EAAID,EAAMC,IACxBX,EAAOR,EAAIkB,EAAOC,CAAC,EAAI,KAAK,KAAKhB,EAAKH,EAAIkB,EAAOC,CAAC,GAAK,GAAKyC,CAAG,EAC/DN,GAAO9C,EAAOR,EAAIkB,EAAOC,CAAC,GAAK,EAGjC,QAASA,EAAI,EAAGA,EAAID,EAAMC,IACxBX,EAAOR,EAAIkB,EAAOC,CAAC,GAAKX,EAAOR,EAAIkB,EAAOC,CAAC,GAAK,GAAKmC,CAEzD,CAEA,OAAO,IAAIrD,EAAeO,EAAQiD,EAAE,MAAOA,EAAE,KAAK,CACpD,CAEA,MAAM,IAAIhE,EACR,+EACAC,EAAW,gBACX,CAAE,MAAO+D,EAAE,MAAO,KAAAC,CAAI,CAAE,CAE5B,CAKM,SAAUG,GAAKJ,EAAiB,CACpC,IAAMtD,EAAOsD,EAAE,eAAc,EACvBjD,EAAS,IAAI,aAAaiD,EAAE,IAAI,EAEtC,QAASzD,EAAI,EAAGA,EAAIyD,EAAE,KAAMzD,IAC1BQ,EAAOR,CAAC,EAAI,KAAK,IAAI,EAAGG,EAAKH,CAAC,GAAK,CAAC,EAGtC,OAAO,IAAIC,EAAeO,EAAQiD,EAAE,MAAOA,EAAE,KAAK,CACpD,CAKM,SAAUK,GAAQL,EAAiB,CACvC,IAAMtD,EAAOsD,EAAE,eAAc,EACvBjD,EAAS,IAAI,aAAaiD,EAAE,IAAI,EAEtC,QAASzD,EAAI,EAAGA,EAAIyD,EAAE,KAAMzD,IAC1BQ,EAAOR,CAAC,EAAI,GAAK,EAAI,KAAK,IAAI,EAAEG,EAAKH,CAAC,GAAK,EAAE,GAG/C,OAAO,IAAIC,EAAeO,EAAQiD,EAAE,MAAOA,EAAE,KAAK,CACpD,CAKM,SAAUM,GAAKN,EAAiB,CACpC,IAAMtD,EAAOsD,EAAE,eAAc,EACvBjD,EAAS,IAAI,aAAaiD,EAAE,IAAI,EAEtC,QAASzD,EAAI,EAAGA,EAAIyD,EAAE,KAAMzD,IAC1BQ,EAAOR,CAAC,EAAI,KAAK,KAAKG,EAAKH,CAAC,GAAK,CAAC,EAGpC,OAAO,IAAIC,EAAeO,EAAQiD,EAAE,MAAOA,EAAE,KAAK,CACpD,CAKM,SAAUH,GAAIG,EAAmBC,EAAa,CAClD,IAAMvD,EAAOsD,EAAE,eAAc,EAE7B,GAAIC,IAAS,OAAW,CACtB,IAAIM,EAAQ,EACZ,QAAS,EAAI,EAAG,EAAIP,EAAE,KAAM,IAC1BO,GAAS7D,EAAK,CAAC,GAAK,EAEtB,OAAO6D,CACT,CAGA,IAAML,EAAaD,EAAO,EAAID,EAAE,MAAM,OAASC,EAAOA,EAEtD,GAAIC,EAAa,GAAKA,GAAcF,EAAE,MAAM,OAC1C,MAAM,IAAIhE,EACR,gBAAgBiE,CAAI,oBAAoBD,EAAE,MAAM,MAAM,cACtD/D,EAAW,iBACX,CAAE,KAAAgE,EAAM,MAAOD,EAAE,KAAK,CAAE,EAK5B,IAAM1C,EAAW,CAAC,GAAG0C,EAAE,KAAK,EAG5B,GAFA1C,EAAS,OAAO4C,EAAY,CAAC,EAEzB5C,EAAS,SAAW,EAAG,CACzB,IAAIiD,EAAQ,EACZ,QAAS,EAAI,EAAG,EAAIP,EAAE,KAAM,IAC1BO,GAAS7D,EAAK,CAAC,GAAK,EAEtB,OAAO6D,CACT,CAGA,GAAIP,EAAE,MAAM,SAAW,EAAG,CACxB,GAAM,CAACxC,EAAMC,CAAI,EAAIuC,EAAE,MAEvB,GAAIE,IAAe,EAAG,CACpB,IAAMnD,EAAS,IAAI,aAAaU,CAAI,EACpC,QAASC,EAAI,EAAGA,EAAID,EAAMC,IACxB,QAASnB,EAAI,EAAGA,EAAIiB,EAAMjB,IACxBQ,EAAOW,CAAC,GAAKX,EAAOW,CAAC,GAAK,IAAMhB,EAAKH,EAAIkB,EAAOC,CAAC,GAAK,GAG1D,OAAO,IAAIlB,EAAeO,EAAQ,CAACU,CAAI,EAAGuC,EAAE,KAAK,CACnD,KAAO,CACL,IAAMjD,EAAS,IAAI,aAAaS,CAAI,EACpC,QAASjB,EAAI,EAAGA,EAAIiB,EAAMjB,IACxB,QAASmB,EAAI,EAAGA,EAAID,EAAMC,IACxBX,EAAOR,CAAC,GAAKQ,EAAOR,CAAC,GAAK,IAAMG,EAAKH,EAAIkB,EAAOC,CAAC,GAAK,GAG1D,OAAO,IAAIlB,EAAeO,EAAQ,CAACS,CAAI,EAAGwC,EAAE,KAAK,CACnD,CACF,CAEA,MAAM,IAAIhE,EACR,0DACAC,EAAW,gBACX,CAAE,MAAO+D,EAAE,MAAO,KAAAC,CAAI,CAAE,CAE5B,CAKM,SAAUO,GAAKR,EAAmBC,EAAa,CACnD,GAAIA,IAAS,OACX,OAAQJ,GAAIG,CAAC,EAAeA,EAAE,KAGhC,IAAMjD,EAAS8C,GAAIG,EAAGC,CAAI,EAC1B,GAAI,OAAOlD,GAAW,SACpB,OAAOA,GAAUiD,EAAE,MAAMC,CAAI,GAAK,GAGpC,IAAMQ,EAAWT,EAAE,MAAMC,CAAI,GAAK,EAClC,OAAOT,GAAIzC,EAAQ0D,CAAQ,CAC7B,CAKM,SAAUC,GAAOV,EAAmBC,EAAa,CACrD,IAAMvD,EAAOsD,EAAE,eAAc,EAE7B,GAAIC,IAAS,OAAW,CACtB,IAAIU,EAAS,EACTC,EAASlE,EAAK,CAAC,GAAK,KAExB,QAAS,EAAI,EAAG,EAAIsD,EAAE,KAAM,KACrBtD,EAAK,CAAC,GAAK,MAAakE,IAC3BA,EAASlE,EAAK,CAAC,GAAK,KACpBiE,EAAS,GAGb,OAAOA,CACT,CAGA,IAAMT,EAAaD,EAAO,EAAID,EAAE,MAAM,OAASC,EAAOA,EAGtD,GAAID,EAAE,MAAM,SAAW,GAAKE,IAAe,EAAG,CAC5C,GAAM,CAAC1C,EAAMC,CAAI,EAAIuC,EAAE,MACjBjD,EAAS,IAAI,aAAaS,CAAI,EAEpC,QAASjB,EAAI,EAAGA,EAAIiB,EAAMjB,IAAK,CAC7B,IAAIoE,EAAS,EACTC,EAASlE,EAAKH,EAAIkB,CAAI,GAAK,KAE/B,QAASC,EAAI,EAAGA,EAAID,EAAMC,KACnBhB,EAAKH,EAAIkB,EAAOC,CAAC,GAAK,MAAakD,IACtCA,EAASlE,EAAKH,EAAIkB,EAAOC,CAAC,GAAK,KAC/BiD,EAASjD,GAGbX,EAAOR,CAAC,EAAIoE,CACd,CAEA,OAAO,IAAInE,EAAeO,EAAQ,CAACS,CAAI,EAAG,OAAO,CACnD,CAEA,MAAM,IAAIxB,EACR,2EACAC,EAAW,gBACX,CAAE,MAAO+D,EAAE,MAAO,KAAAC,CAAI,CAAE,CAE5B,CAKM,SAAUY,GAAOC,EAA2Bb,EAAe,EAAC,CAChE,GAAIa,EAAQ,SAAW,EACrB,MAAM,IAAI9E,EACR,4CACAC,EAAW,gBAAgB,EAI/B,GAAI6E,EAAQ,SAAW,EACrB,OAAOA,EAAQ,CAAC,GAAG,MAAK,GAAM/C,GAAM,CAAC,CAAC,CAAC,EAGzC,IAAMgD,EAAQD,EAAQ,CAAC,EACvB,GAAI,CAACC,EACH,MAAM,IAAI/E,EAAc,4BAA6BC,EAAW,gBAAgB,EAIlF,IAAMiE,EAAaD,EAAO,EAAIc,EAAM,MAAM,OAASd,EAAOA,EAG1D,QAAS,EAAI,EAAG,EAAIa,EAAQ,OAAQ,IAAK,CACvC,IAAMd,EAAIc,EAAQ,CAAC,EACnB,GAAKd,EAEL,IAAIA,EAAE,MAAM,SAAWe,EAAM,MAAM,OACjC,MAAM,IAAI/E,EACR,sDACAC,EAAW,qBAAqB,EAIpC,QAASyB,EAAI,EAAGA,EAAIqD,EAAM,MAAM,OAAQrD,IACtC,GAAIA,IAAMwC,GAAca,EAAM,MAAMrD,CAAC,IAAMsC,EAAE,MAAMtC,CAAC,EAClD,MAAM,IAAI1B,EACR,+BAA+B0B,CAAC,GAChCzB,EAAW,qBAAqB,EAIxC,CAGA,IAAMqB,EAAW,CAAC,GAAGyD,EAAM,KAAK,EAC5BC,EAAgB,EACpB,QAAWhB,KAAKc,EACVd,IAAGgB,GAAiBhB,EAAE,MAAME,CAAU,GAAK,GAKjD,GAHA5C,EAAS4C,CAAU,EAAIc,EAGnBD,EAAM,MAAM,SAAW,EAAG,CAC5B,IAAMhE,EAAS,IAAI,aAAaiE,CAAa,EACzCC,EAAS,EAEb,QAAWjB,KAAKc,EACTd,IACLjD,EAAO,IAAIiD,EAAE,eAAc,EAAIiB,CAAM,EACrCA,GAAUjB,EAAE,MAGd,OAAO,IAAIxD,EAAeO,EAAQO,EAAUyD,EAAM,KAAK,CACzD,CAEA,MAAM,IAAI/E,EACR,mDACAC,EAAW,eAAe,CAE9B,CCh8BA,IAAMiF,GAAN,KAAU,CAkBR,YACEC,EACAC,EACAC,EACAC,EAA0B,CArBnBC,EAAA,WACAA,EAAA,gBACAA,EAAA,iBACAA,EAAA,kBAEDA,EAAA,eAAsB,WACtBA,EAAA,mBACAA,EAAA,qBACAA,EAAA,gBACAA,EAAA,eACAA,EAAA,kBACAA,EAAA,kBAGH,CAAA,GACGA,EAAA,kBAAa,IAQnB,KAAK,GAAKJ,EACV,KAAK,QAAUC,EACf,KAAK,SAAWC,EAChB,KAAK,UAAY,KAAK,IAAG,EACzB,KAAK,UAAYC,CACnB,CAEA,IAAI,QAAM,CACR,OAAO,KAAK,OACd,CAEA,IAAI,WAAS,CACX,OAAO,KAAK,UACd,CAEA,IAAI,aAAW,CACb,OAAO,KAAK,YACd,CAEA,IAAI,QAAM,CACR,OAAO,KAAK,OACd,CAEA,IAAI,OAAK,CACP,OAAO,KAAK,MACd,CAKA,QAAM,CACJ,GAAI,KAAK,UAAY,UAAW,CAC9B,KAAK,WAAa,GAClB,KAAK,QAAU,YACf,KAAK,aAAe,KAAK,IAAG,EAE5B,IAAME,EAAc,IAAIC,EACtB,qBACAC,EAAW,oBACX,CAAE,OAAQ,KAAK,EAAE,CAAE,EAGrB,OAAW,CAAE,OAAAC,CAAM,IAAM,KAAK,WAC5BA,EAAOH,CAAW,EAEpB,KAAK,WAAa,CAAA,CACpB,CACF,CAKA,MAAI,CACF,OAAI,KAAK,UAAY,YACZ,QAAQ,QAAQ,KAAK,OAAY,EAGtC,KAAK,UAAY,SACZ,QAAQ,OAAO,KAAK,MAAM,EAG/B,KAAK,UAAY,YACZ,QAAQ,OAAO,IAAIC,EACxB,qBACAC,EAAW,oBACX,CAAE,OAAQ,KAAK,EAAE,CAAE,CACpB,EAGI,IAAI,QAAW,CAACE,EAASD,IAAU,CACxC,KAAK,WAAW,KAAK,CAAE,QAAAC,EAAS,OAAAD,CAAM,CAAE,CAC1C,CAAC,CACH,CAKA,MAAM,SAAO,CACX,GAAI,MAAK,WAIT,MAAK,QAAU,UACf,KAAK,WAAa,KAAK,IAAG,EAE1B,GAAI,CACF,KAAK,QAAU,MAAM,KAAK,UAAS,EACnC,KAAK,QAAU,YACf,KAAK,aAAe,KAAK,IAAG,EAE5B,OAAW,CAAE,QAAAC,CAAO,IAAM,KAAK,WAC7BA,EAAQ,KAAK,OAAO,CAExB,OAASC,EAAK,CACZ,KAAK,OAASA,aAAe,MAAQA,EAAM,IAAI,MAAM,OAAOA,CAAG,CAAC,EAChE,KAAK,QAAU,SACf,KAAK,aAAe,KAAK,IAAG,EAE5B,OAAW,CAAE,OAAAF,CAAM,IAAM,KAAK,WAC5BA,EAAO,KAAK,MAAM,CAEtB,CAEA,KAAK,WAAa,CAAA,EACpB,GAUIG,GAA+C,CACnD,SAAU,EACV,KAAM,EACN,OAAQ,EACR,IAAK,GAMDC,GAAN,KAAmB,CAAnB,cACUR,EAAA,aAAa,CAAA,GAErB,IAAI,QAAM,CACR,OAAO,KAAK,MAAM,MACpB,CAEA,SAAO,CACL,OAAO,KAAK,MAAM,SAAW,CAC/B,CAKA,QAAQS,EAAO,CACb,IAAIC,EAAW,GAEf,QAASC,EAAI,EAAGA,EAAI,KAAK,MAAM,OAAQA,IAAK,CAC1C,IAAMC,EAAc,KAAK,MAAMD,CAAC,EAChC,GAAIC,GAAeL,GAAeE,EAAK,QAAQ,EAAIF,GAAeK,EAAY,QAAQ,EAAG,CACvF,KAAK,MAAM,OAAOD,EAAG,EAAGF,CAAI,EAC5BC,EAAW,GACX,KACF,CACF,CAEKA,GACH,KAAK,MAAM,KAAKD,CAAI,CAExB,CAKA,SAAO,CACL,OAAO,KAAK,MAAM,MAAK,CACzB,CAKA,MAAI,CACF,OAAO,KAAK,MAAM,CAAC,CACrB,CAKA,OAAOb,EAAU,CACf,IAAMiB,EAAQ,KAAK,MAAM,UAAUJ,GAAQA,EAAK,KAAOb,CAAE,EACzD,GAAIiB,IAAU,GAAI,CAChB,GAAM,CAACC,CAAO,EAAI,KAAK,MAAM,OAAOD,EAAO,CAAC,EAC5C,OAAOC,CACT,CAEF,CAKA,QAAM,CACJ,MAAO,CAAC,GAAG,KAAK,KAAK,CACvB,CAKA,OAAK,CACH,KAAK,MAAQ,CAAA,CACf,GAgEF,IAAIC,GAAgB,EAKpB,SAASC,IAAc,CACrB,MAAO,QAAQ,EAAED,EAAa,IAAI,KAAK,IAAG,EAAG,SAAS,EAAE,CAAC,EAC3D,CAKA,IAAME,GAA8C,CAClD,mBAAoB,EACpB,sBAAuB,EACvB,eAAgB,IAChB,eAAgB,GAChB,aAAc,GACd,aAAc,IAaHC,GAAP,KAAyB,CAW7B,YAAYC,EAA4B,CAAA,EAAE,CAVzBC,EAAA,gBACAA,EAAA,cAA2C,IAAI,KAC/CA,EAAA,oBAAyC,IAAI,KAC7CA,EAAA,gBAA8B,IAAI,KAClCA,EAAA,gBAAiD,IAAI,KACrDA,EAAA,iBAAgD,IAAI,KAC7DA,EAAA,0BAAqB,GACrBA,EAAA,oBAAe,IACfA,EAAA,gBAAW,IAGjB,KAAK,QAAU,CAAE,GAAGH,GAAiB,GAAGE,CAAO,CACjD,CAKQ,SAASE,EAAe,CAC9B,IAAIC,EAAQ,KAAK,OAAO,IAAID,CAAO,EACnC,OAAKC,IACHA,EAAQ,IAAIC,GACZ,KAAK,OAAO,IAAIF,EAASC,CAAK,GAEzBA,CACT,CAKQ,cAAcD,EAAe,CACnC,IAAIG,EAAU,KAAK,aAAa,IAAIH,CAAO,EAC3C,OAAKG,IACHA,EAAU,IAAI,IACd,KAAK,aAAa,IAAIH,EAASG,CAAO,GAEjCA,CACT,CAKQ,aAAaH,EAAe,CAClC,GAAI,KAAK,oBAAsB,KAAK,QAAQ,mBAC1C,MAAO,GAGT,IAAMG,EAAU,KAAK,aAAa,IAAIH,CAAO,EAC7C,MAAI,EAAAG,GAAWA,EAAQ,MAAQ,KAAK,QAAQ,sBAK9C,CAKQ,MAAM,cAAY,CACxB,GAAI,KAAK,cAAgB,KAAK,SAC5B,OAGF,KAAK,aAAe,GAEpB,GAAI,CAEF,IAAMC,EAAuB,CAAA,EAE7B,OAAW,CAACJ,EAASC,CAAK,IAAK,KAAK,OAClC,KAAO,CAACA,EAAM,QAAO,GAAM,KAAK,aAAaD,CAAO,GAAG,CACrD,IAAMK,EAAOJ,EAAM,QAAO,EACtBI,GAAQA,EAAK,SAAW,YAC1BD,EAAa,KAAKC,CAAI,EAEN,KAAK,cAAcL,CAAO,EAClC,IAAIK,EAAK,EAAE,EACnB,KAAK,qBAET,CAIF,MAAM,QAAQ,IACZD,EAAa,IAAI,MAAOC,GAAQ,CAC9B,KAAK,KAAK,kBAAmB,CAAE,OAAQA,EAAK,GAAI,QAASA,EAAK,OAAO,CAAE,EAEvE,GAAI,CACF,MAAMA,EAAK,QAAO,EAClB,KAAK,KAAK,qBAAsB,CAC9B,OAAQA,EAAK,GACb,QAASA,EAAK,QACd,UAAWA,EAAK,aAAe,IAAMA,EAAK,WAAa,GACxD,CACH,OAASC,EAAO,CACd,KAAK,KAAK,kBAAmB,CAC3B,OAAQD,EAAK,GACb,QAASA,EAAK,QACd,MAAAC,EACD,CACH,SAEE,IAAMH,EAAU,KAAK,aAAa,IAAIE,EAAK,OAAO,EAC9CF,GACFA,EAAQ,OAAOE,EAAK,EAAE,EAExB,KAAK,oBACP,CACF,CAAC,CAAC,CAEN,SACE,KAAK,aAAe,EACtB,CAGA,IAAIE,EAAa,GACjB,QAAWN,KAAS,KAAK,OAAO,OAAM,EACpC,GAAI,CAACA,EAAM,QAAO,EAAI,CACpBM,EAAa,GACb,KACF,CAGEA,GAEF,WAAW,IAAM,KAAK,aAAY,EAAI,CAAC,CAE3C,CAKA,SACEP,EACAQ,EACAC,EAAyB,SAAQ,CAEjC,GAAI,KAAK,SACP,MAAM,IAAIC,EACR,8BACAC,EAAW,uBAAuB,EAItC,IAAMN,EAAO,IAAIO,GACfjB,GAAc,EACdK,EACAS,EACAD,CAAQ,EAGV,YAAK,SAAS,IAAIH,EAAK,GAAIA,CAAY,EAGzB,KAAK,SAASL,CAAO,EAC7B,QAAQK,CAAY,EAG1B,KAAK,aAAY,EAEVA,CACT,CAKA,oBACEL,EACAQ,EACAK,EAAkB,KAAK,QAAQ,eAC/BJ,EAAyB,SAAQ,CAEjC,IAAMK,EAAkB,IACf,IAAI,QAAW,CAACC,EAASC,IAAU,CACxC,IAAMC,EAAQ,WAAW,IAAK,CAC5BD,EAAO,IAAIN,EACT,wBAAwBG,CAAO,KAC/BF,EAAW,kBACX,CAAE,QAAAE,CAAO,CAAE,CACZ,CACH,EAAGA,CAAO,EAEVL,EAAQ,EACL,KAAKU,GAAS,CACb,aAAaD,CAAK,EAClBF,EAAQG,CAAM,CAChB,CAAC,EACA,MAAMZ,GAAQ,CACb,aAAaW,CAAK,EAClBD,EAAOV,CAAK,CACd,CAAC,CACL,CAAC,EAGH,OAAO,KAAK,SAASN,EAASc,EAAiBL,CAAQ,CACzD,CAKA,MAAM,YACJU,EAIE,CAEF,IAAMC,EAAiBD,EAAM,IAAI,CAAC,CAAE,QAAAnB,EAAS,SAAAQ,EAAU,SAAAC,CAAQ,IAC7D,KAAK,SAAYT,EAASQ,EAAUC,CAAQ,CAAC,EAG/C,OAAO,QAAQ,IAAIW,EAAe,IAAIf,GAAQA,EAAK,KAAI,CAAE,CAAC,CAC5D,CAKA,QAAQgB,EAAc,CACpB,OAAO,KAAK,SAAS,IAAIA,CAAM,CACjC,CAKA,WAAWA,EAAc,CACvB,IAAMhB,EAAO,KAAK,SAAS,IAAIgB,CAAM,EACrC,GAAIhB,GAAQA,EAAK,SAAW,UAAW,CACrCA,EAAK,OAAM,EAGX,QAAWJ,KAAS,KAAK,OAAO,OAAM,EACpCA,EAAM,OAAOoB,CAAM,EAGrB,MAAO,EACT,CACA,MAAO,EACT,CAKA,kBAAkBrB,EAAe,CAC/B,IAAMC,EAAQ,KAAK,OAAO,IAAID,CAAO,EACrC,GAAI,CAACC,EAAO,MAAO,GAEnB,IAAIqB,EAAY,EAChB,QAAWjB,KAAQJ,EAAM,OAAM,EACzBI,EAAK,SAAW,YAClBA,EAAK,OAAM,EACXiB,KAGJ,OAAArB,EAAM,MAAK,EAEJqB,CACT,CAKA,UAAQ,CASN,IAAMC,EAAQ,CACZ,WAAY,KAAK,SAAS,KAC1B,aAAc,EACd,aAAc,EACd,eAAgB,EAChB,YAAa,EACb,eAAgB,EAChB,cAAe,CAAA,GAGjB,QAAWlB,KAAQ,KAAK,SAAS,OAAM,EACrC,OAAQA,EAAK,OAAQ,CACnB,IAAK,UACHkB,EAAM,eACN,MACF,IAAK,UACHA,EAAM,eACN,MACF,IAAK,YACHA,EAAM,iBACN,MACF,IAAK,SACHA,EAAM,cACN,MACF,IAAK,YACHA,EAAM,iBACN,KACJ,CAGF,OAAW,CAACvB,EAASC,CAAK,IAAK,KAAK,OAClCsB,EAAM,cAAcvB,CAAO,EAAIC,EAAM,OAGvC,OAAOsB,CACT,CAKA,GAAgBC,EAAkBC,EAA0B,CAC1D,IAAIC,EAAY,KAAK,UAAU,IAAIF,CAAK,EACnCE,IACHA,EAAY,IAAI,IAChB,KAAK,UAAU,IAAIF,EAAOE,CAAS,GAErCA,EAAU,IAAID,CAAyB,CACzC,CAKA,IAAiBD,EAAkBC,EAA0B,CAC3D,IAAMC,EAAY,KAAK,UAAU,IAAIF,CAAK,EACtCE,GACFA,EAAU,OAAOD,CAAyB,CAE9C,CAKQ,KAAQE,EAAiBC,EAAO,CACtC,IAAMJ,EAA0B,CAC9B,KAAAG,EACA,UAAW,KAAK,IAAG,EACnB,KAAAC,GAGIF,EAAY,KAAK,UAAU,IAAIC,CAAI,EACzC,GAAID,EACF,QAAWD,KAAYC,EACrB,GAAI,CACFD,EAASD,CAAK,CAChB,OAASlB,EAAO,CACd,QAAQ,MAAM,2BAA4BA,CAAK,CACjD,CAGN,CAKA,cAAY,CACV,OAAW,CAACe,EAAQhB,CAAI,IAAK,KAAK,UAE9BA,EAAK,SAAW,aAChBA,EAAK,SAAW,UAChBA,EAAK,SAAW,cAEhB,KAAK,SAAS,OAAOgB,CAAM,CAGjC,CAKA,SAAO,CACL,KAAK,SAAW,GAGhB,QAAWpB,KAAS,KAAK,OAAO,OAAM,EAAI,CACxC,QAAWI,KAAQJ,EAAM,OAAM,EAC7BI,EAAK,OAAM,EAEbJ,EAAM,MAAK,CACb,CAGA,QAAW4B,KAAW,KAAK,SAAS,OAAM,EACxCA,EAAQ,MAAK,EAGf,KAAK,OAAO,MAAK,EACjB,KAAK,aAAa,MAAK,EACvB,KAAK,SAAS,MAAK,EACnB,KAAK,SAAS,MAAK,EACnB,KAAK,UAAU,MAAK,CACtB,GAOEC,GAA6C,KAK3C,SAAUC,IAAY,CAC1B,OAAKD,KACHA,GAAkB,IAAIjC,IAEjBiC,EACT,CAKM,SAAUE,GAAaC,EAA6B,CACpDH,IACFA,GAAgB,QAAO,EAEzBA,GAAkBG,CACpB,CAKM,SAAUC,GAAmBpC,EAAyB,CAC1DkC,GAAa,IAAInC,GAAmBC,CAAO,CAAC,CAC9C,CCntBA,IAAMqC,GAAkD,CACtD,YAAa,SACb,QAAS,UACT,aAAc,IACd,OAAQ,GACR,YAAa,IAgBFC,EAAP,MAAOA,CAAa,CAaxB,YAAoBC,EAA2B,CAAA,EAAE,CAVhCC,EAAA,eACAA,EAAA,iBAA0C,IAAI,KAC9CA,EAAA,iBAAqC,IAAI,KACzCA,EAAA,iBAAgD,IAAI,KAE7DA,EAAA,iBAAY,GACZA,EAAA,YAAO,GACPA,EAAA,mBAAc,IACdA,EAAA,gBAAW,IAGjB,KAAK,OAAS,CAAE,GAAGH,GAAqB,GAAGE,CAAM,CACnD,CAKA,OAAO,aAAW,CAChB,OAAKD,EAAc,WACjBA,EAAc,SAAW,IAAIA,GAExBA,EAAc,QACvB,CAKA,OAAO,UAAUC,EAAwB,CACnCD,EAAc,UAChB,QAAQ,KAAK,gEAAgE,EAE/EA,EAAc,SAAW,IAAIA,EAAcC,CAAM,CACnD,CAKA,MAAME,EAAgBC,EAAqB,CACzC,GAAI,KAAK,SAAU,OAEnB,IAAMC,EAAO,KAAK,mBAAmBF,CAAM,EAE3C,KAAK,UAAU,IAAIA,EAAO,GAAI,CAC5B,GAAIA,EAAO,GACX,KAAM,SACN,KAAAE,EACA,UAAW,KAAK,IAAG,EACnB,WAAY,KAAK,kBAAiB,EACnC,EAEGD,GACF,KAAK,UAAU,IAAID,EAAO,GAAIC,CAAQ,EAGxC,KAAK,WAAaC,EAClB,KAAK,KAAO,KAAK,IAAI,KAAK,KAAM,KAAK,SAAS,EAE9C,KAAK,qBAAoB,CAC3B,CAKA,WAAWC,EAAoBF,EAAqB,CAClD,GAAI,KAAK,SAAU,OAEnB,IAAMC,EAAOC,EAAM,SAAS,UAE5B,KAAK,UAAU,IAAIA,EAAM,GAAI,CAC3B,GAAIA,EAAM,GACV,KAAM,QACN,KAAAD,EACA,UAAW,KAAK,IAAG,EACnB,WAAY,KAAK,kBAAiB,EACnC,EAEGD,GACF,KAAK,UAAU,IAAIE,EAAM,GAAIF,CAAQ,EAGvC,KAAK,WAAaC,EAClB,KAAK,KAAO,KAAK,IAAI,KAAK,KAAM,KAAK,SAAS,EAE9C,KAAK,qBAAoB,CAC3B,CAKA,QAAQE,EAAU,CAChB,IAAMC,EAAW,KAAK,UAAU,IAAID,CAAE,EAClCC,IACF,KAAK,WAAaA,EAAS,KAC3B,KAAK,UAAU,OAAOD,CAAE,EACxB,KAAK,UAAU,OAAOA,CAAE,EAE5B,CAKA,QAAQE,EAA2C,CACjD,IAAMF,EAAK,OAAOE,GAAiB,SAAWA,EAAeA,EAAa,GAEpEL,EAAW,KAAK,UAAU,IAAIG,CAAE,EACtC,GAAIH,EACF,GAAI,CACFA,EAAQ,CACV,OAASM,EAAO,CACd,QAAQ,MAAM,4BAA6BA,CAAK,CAClD,CAGF,KAAK,QAAQH,CAAE,CACjB,CAKQ,mBAAmBJ,EAAc,CACvC,IAAMQ,EAAkB,KAAK,mBAAmBR,EAAO,KAAK,EAC5D,OAAOA,EAAO,KAAOQ,CACvB,CAKQ,mBAAmBC,EAAa,CACtC,OAAQA,EAAO,CACb,IAAK,UACH,MAAO,GACT,IAAK,UACH,MAAO,GACT,IAAK,QACH,MAAO,GACT,IAAK,QACH,MAAO,GACT,IAAK,QACL,IAAK,OACL,IAAK,OACH,MAAO,GACT,QACE,MAAO,EACX,CACF,CAKQ,mBAAiB,CACvB,GAAI,OAAO,MAAM,mBAAsB,WAAY,CACjD,IAAMC,EAA0B,CAAA,EAChC,aAAM,kBAAkBA,EAAK,KAAK,iBAAiB,EAC5CA,EAAI,KACb,CACA,OAAO,IAAI,MAAK,EAAG,KACrB,CAKQ,sBAAoB,CAC1B,GAAI,CAAC,KAAK,OAAO,OAAQ,OAEzB,IAAMC,EAAQ,KAAK,UAAY,KAAK,OAAO,QAEvCA,GAAS,KAAK,OAAO,aAAe,CAAC,KAAK,cAC5C,KAAK,YAAc,GACnB,KAAK,KAAK,iBAAkB,CAC1B,UAAW,KAAK,UAChB,QAAS,KAAK,OAAO,QACrB,MAAAA,EACD,EAGD,WAAW,IAAK,CACd,KAAK,GAAE,EACP,KAAK,YAAc,EACrB,EAAG,CAAC,EAER,CAKA,IAAE,CACA,KAAK,KAAK,YAAa,CAAE,OAAQ,KAAK,SAAS,CAAE,EAMjD,IAAMC,EAAM,KAAK,IAAG,EACdC,EAAyB,CAAA,EAE/B,OAAW,CAACT,EAAIC,CAAQ,IAAK,KAAK,UAE5BO,EAAMP,EAAS,UAAY,EAAI,GAAK,KACtCQ,EAAa,KAAKT,CAAE,EAQxB,KAAK,KAAK,YAAa,CACrB,MAAO,KAAK,UACZ,iBAAkBS,EAAa,OAChC,CACH,CAKA,UAAQ,CACN,IAAIC,EAAc,EACdC,EAAa,EAEjB,QAAWV,KAAY,KAAK,UAAU,OAAM,EACtCA,EAAS,OAAS,SACpBS,IAEAC,IAIJ,MAAO,CACL,UAAW,KAAK,UAChB,KAAM,KAAK,UACX,KAAM,KAAK,KACX,YAAAD,EACA,WAAAC,EAEJ,CAKA,oBAAkB,CAChB,OAAO,MAAM,KAAK,KAAK,UAAU,OAAM,CAAE,CAC3C,CAKA,YAAYC,EAAiB,GAAK,GAAK,IAAI,CACzC,IAAMJ,EAAM,KAAK,IAAG,EACdK,EAAoC,CAAA,EAE1C,QAAWZ,KAAY,KAAK,UAAU,OAAM,EACtCO,EAAMP,EAAS,UAAYW,GAC7BC,EAAe,KAAKZ,CAAQ,EAIhC,OAAOY,CACT,CAKA,GAAgBC,EAAkBC,EAA0B,CAC1D,IAAIC,EAAY,KAAK,UAAU,IAAIF,CAAK,EACnCE,IACHA,EAAY,IAAI,IAChB,KAAK,UAAU,IAAIF,EAAOE,CAAS,GAErCA,EAAU,IAAID,CAAyB,CACzC,CAKA,IAAiBD,EAAkBC,EAA0B,CAC3D,IAAMC,EAAY,KAAK,UAAU,IAAIF,CAAK,EACtCE,GACFA,EAAU,OAAOD,CAAyB,CAE9C,CAKQ,KAAQE,EAAiBC,EAAO,CACtC,IAAMJ,EAA0B,CAC9B,KAAAG,EACA,UAAW,KAAK,IAAG,EACnB,KAAAC,GAGIF,EAAY,KAAK,UAAU,IAAIC,CAAI,EACzC,GAAID,EACF,QAAWD,KAAYC,EACrB,GAAI,CACFD,EAASD,CAAK,CAChB,OAASX,EAAO,CACd,QAAQ,MAAM,2BAA4BA,CAAK,CACjD,CAGN,CAKA,YAAU,CACR,KAAK,KAAO,KAAK,SACnB,CAKA,YAAU,CACR,QAAWH,KAAM,KAAK,UAAU,KAAI,EAClC,KAAK,QAAQA,CAAE,CAEnB,CAKA,SAAO,CACL,KAAK,WAAU,EACf,KAAK,SAAW,GAChB,KAAK,UAAU,MAAK,EACpBP,EAAc,SAAW,IAC3B,GAzUQE,EADGF,EACI,WAAiC,MAD5C,IAAO0B,EAAP1B,EA+VO2B,GAAP,MAAOC,CAAW,CAKtB,YAAYC,EAAoB,CAJxB3B,EAAA,iBAA4C,CAAA,GAC5CA,EAAA,gBAA0B,CAAA,GAC1BA,EAAA,cAA6B,MAG/B2B,IACF,KAAK,OAASA,EACdA,EAAO,SAAS,KAAK,IAAI,EAE7B,CAKA,MAAyCrB,EAAW,CAClD,YAAK,UAAU,KAAKA,CAAQ,EACrBA,CACT,CAKA,aAAW,CACT,OAAO,IAAIoB,EAAY,IAAI,CAC7B,CAKA,KAAwCpB,EAAW,CACjD,IAAMsB,EAAQ,KAAK,UAAU,QAAQtB,CAAQ,EAC7C,OAAIsB,IAAU,IACZ,KAAK,UAAU,OAAOA,EAAO,CAAC,EAEzBtB,CACT,CAKA,SAAO,CAEL,QAAWuB,KAAS,KAAK,SACvBA,EAAM,QAAO,EAEf,KAAK,SAAW,CAAA,EAGhB,QAASC,EAAI,KAAK,UAAU,OAAS,EAAGA,GAAK,EAAGA,IAC9C,GAAI,CACF,KAAK,UAAUA,CAAC,GAAG,QAAO,CAC5B,OAAStB,EAAO,CACd,QAAQ,MAAM,qCAAsCA,CAAK,CAC3D,CAKF,GAHA,KAAK,UAAY,CAAA,EAGb,KAAK,OAAQ,CACf,IAAMoB,EAAQ,KAAK,OAAO,SAAS,QAAQ,IAAI,EAC3CA,IAAU,IACZ,KAAK,OAAO,SAAS,OAAOA,EAAO,CAAC,EAEtC,KAAK,OAAS,IAChB,CACF,GAMF,eAAsBG,GACpBC,EAAsC,CAEtC,IAAMC,EAAQ,IAAIR,GAClB,GAAI,CACF,OAAO,MAAMO,EAAGC,CAAK,CACvB,SACEA,EAAM,QAAO,CACf,CACF,CAKM,SAAUC,GACdF,EAA6B,CAE7B,IAAMC,EAAQ,IAAIR,GAClB,GAAI,CACF,OAAOO,EAAGC,CAAK,CACjB,SACEA,EAAM,QAAO,CACf,CACF,CASM,IAAOE,GAAP,KAAiB,CAMrB,YAAYC,EAAoD,CAAA,EAAE,CALjDpC,EAAA,gBACAA,EAAA,kBACAA,EAAA,aAA+E,IAAI,KAC5FA,EAAA,mBAAc,GAGpB,KAAK,QAAUoC,EAAQ,SAAW,IAAM,KAAO,KAC/C,KAAK,UAAYA,EAAQ,WAAa,CACxC,CAKA,IAAIC,EAAW,CACb,IAAMC,EAAQ,KAAK,MAAM,IAAID,CAAG,EAChC,GAAIC,EACF,OAAAA,EAAM,WAAa,KAAK,IAAG,EACpBA,EAAM,KAGjB,CAKA,IAAID,EAAajC,EAAkB,CACjC,IAAMD,EAAOC,EAAM,SAAS,UAG5B,MACG,KAAK,YAAcD,EAAO,KAAK,SAAW,KAAK,MAAM,MAAQ,KAAK,YACnE,KAAK,MAAM,KAAO,GAElB,KAAK,SAAQ,EAIf,KAAK,MAAM,IAAIkC,EAAK,CAClB,MAAAjC,EACA,KAAAD,EACA,WAAY,KAAK,IAAG,EACrB,EACD,KAAK,aAAeA,CACtB,CAKA,OAAOkC,EAAW,CAChB,IAAMC,EAAQ,KAAK,MAAM,IAAID,CAAG,EAChC,OAAIC,GACFA,EAAM,MAAM,QAAO,EACnB,KAAK,aAAeA,EAAM,KAC1B,KAAK,MAAM,OAAOD,CAAG,EACd,IAEF,EACT,CAKA,IAAIA,EAAW,CACb,OAAO,KAAK,MAAM,IAAIA,CAAG,CAC3B,CAKQ,UAAQ,CACd,IAAIE,EAA2B,KAC3BC,EAAa,IAEjB,OAAW,CAACH,EAAKC,CAAK,IAAK,KAAK,MAC1BA,EAAM,WAAaE,IACrBA,EAAaF,EAAM,WACnBC,EAAYF,GAIZE,GACF,KAAK,OAAOA,CAAS,CAEzB,CAKA,OAAK,CACH,QAAWD,KAAS,KAAK,MAAM,OAAM,EACnCA,EAAM,MAAM,QAAO,EAErB,KAAK,MAAM,MAAK,EAChB,KAAK,YAAc,CACrB,CAKA,UAAQ,CACN,MAAO,CACL,KAAM,KAAK,YACX,MAAO,KAAK,MAAM,KAClB,QAAS,KAAK,QACd,UAAW,KAAK,UAEpB,GAUI,SAAUG,GAAgB,CAC9B,OAAOjB,EAAc,YAAW,CAClC,CAKM,SAAUkB,IAAc,CAC5B,OAAOlB,EAAc,YAAW,EAAG,SAAQ,CAC7C,CAKM,SAAUmB,GAAQrC,EAA8B,CACpDkB,EAAc,YAAW,EAAG,QAAQlB,CAAQ,CAC9C,CAKM,SAAUsC,IAAE,CAChBpB,EAAc,YAAW,EAAG,GAAE,CAChC,CChnBA,IAAMqB,GAAoD,IAAI,IAKxDC,EAA8C,IAAI,IAKlDC,GAAkC,CAAC,SAAU,QAAS,MAAM,EAerDC,EAAP,MAAOA,CAAc,CAMzB,aAAA,CAHiBC,EAAA,iBAAgD,IAAI,KAC7DA,EAAA,sBAA8B,OAEf,CAKvB,OAAO,aAAW,CAChB,OAAKD,EAAe,WAClBA,EAAe,SAAW,IAAIA,GAEzBA,EAAe,QACxB,CAKA,SAASE,EAAmBC,EAAsB,CAChDN,GAAiB,IAAIK,EAAMC,CAAO,CACpC,CAKA,MAAM,WAAWD,EAAoB,OAAM,CACzC,GAAIA,IAAS,OACX,OAAO,KAAK,eAAc,EAI5B,IAAIE,EAAUN,EAAiB,IAAII,CAAI,EACvC,GAAIE,EACF,OAAOA,EAIT,IAAMD,EAAUN,GAAiB,IAAIK,CAAI,EACzC,GAAI,CAACC,EACH,MAAM,IAAIE,EACR,YAAYH,CAAI,sBAChBI,EAAW,sBACX,CAAE,QAASJ,CAAI,CAAE,EAQrB,GAJAE,EAAUD,EAAO,EAIb,CADc,MAAMC,EAAQ,YAAW,EAEzC,MAAM,IAAIC,EACR,YAAYH,CAAI,yCAChBI,EAAW,sBACX,CAAE,QAASJ,CAAI,CAAE,EAKrB,GAAI,CACF,MAAME,EAAQ,WAAU,CAC1B,OAASG,EAAO,CACd,MAAM,IAAIF,EACR,iCAAiCH,CAAI,MAAMK,aAAiB,MAAQA,EAAM,QAAU,OAAOA,CAAK,CAAC,GACjGD,EAAW,oBACX,CAAE,QAASJ,EAAM,MAAAK,CAAK,CAAE,CAE5B,CAEA,OAAAT,EAAiB,IAAII,EAAME,CAAO,EAClC,KAAK,KAAK,gBAAiB,CAAE,QAASF,CAAI,CAAE,EAErCE,CACT,CAKA,MAAM,gBAAc,CAClB,QAAWF,KAAQH,GACjB,GAAI,CAEF,IAAMS,EAAWV,EAAiB,IAAII,CAAI,EAC1C,GAAIM,EACF,OAAOA,EAIT,IAAML,EAAUN,GAAiB,IAAIK,CAAI,EACzC,GAAI,CAACC,EAAS,SAEd,IAAMC,EAAUD,EAAO,EAGvB,GAFkB,MAAMC,EAAQ,YAAW,EAGzC,aAAMA,EAAQ,WAAU,EACxBN,EAAiB,IAAII,EAAME,CAAO,EAClC,KAAK,KAAK,gBAAiB,CAAE,QAASF,CAAI,CAAE,EACrCE,CAEX,MAAQ,CAEN,QACF,CAGF,MAAM,IAAIC,EACR,2EACAC,EAAW,sBACX,CAAE,cAAeP,EAAgB,CAAE,CAEvC,CAKA,MAAM,yBAAuB,CAC3B,IAAMU,EAAU,IAAI,IAEpB,QAAWP,KAAQH,GAAkB,CACnC,IAAMI,EAAUN,GAAiB,IAAIK,CAAI,EACzC,GAAI,CAACC,EAAS,CACZM,EAAQ,IAAIP,EAAM,EAAK,EACvB,QACF,CAEA,GAAI,CACF,IAAME,EAAUD,EAAO,EACvBM,EAAQ,IAAIP,EAAM,MAAME,EAAQ,YAAW,CAAE,CAC/C,MAAQ,CACNK,EAAQ,IAAIP,EAAM,EAAK,CACzB,CACF,CAEA,OAAOO,CACT,CAKA,MAAM,gBAAgBP,EAAiB,CAErC,OADgB,MAAM,KAAK,WAAWA,CAAI,GAC3B,YACjB,CAKA,kBAAkBA,EAAiB,CACjC,KAAK,eAAiBA,CACxB,CAKA,uBAAqB,CACnB,OAAO,KAAK,cACd,CAKA,eAAeA,EAAiB,CAC9B,IAAME,EAAUN,EAAiB,IAAII,CAAI,EACrCE,IACFA,EAAQ,QAAO,EACfN,EAAiB,OAAOI,CAAI,EAEhC,CAKA,YAAU,CACR,OAAW,CAACA,EAAME,CAAO,IAAKN,EAC5BM,EAAQ,QAAO,EACfN,EAAiB,OAAOI,CAAI,CAEhC,CAKA,GAAgBQ,EAAkBC,EAA0B,CAC1D,IAAIC,EAAY,KAAK,UAAU,IAAIF,CAAK,EACnCE,IACHA,EAAY,IAAI,IAChB,KAAK,UAAU,IAAIF,EAAOE,CAAS,GAErCA,EAAU,IAAID,CAAyB,CACzC,CAKA,IAAiBD,EAAkBC,EAA0B,CAC3D,IAAMC,EAAY,KAAK,UAAU,IAAIF,CAAK,EACtCE,GACFA,EAAU,OAAOD,CAAyB,CAE9C,CAKQ,KAAQT,EAAiBW,EAAO,CACtC,IAAMH,EAA0B,CAC9B,KAAAR,EACA,UAAW,KAAK,IAAG,EACnB,KAAAW,GAGID,EAAY,KAAK,UAAU,IAAIV,CAAI,EACzC,GAAIU,EACF,QAAWD,KAAYC,EACrB,GAAI,CACFD,EAASD,CAAK,CAChB,OAASH,EAAO,CACd,QAAQ,MAAM,2BAA4BA,CAAK,CACjD,CAGN,GAhOQN,EADGD,EACI,WAAkC,MAD7C,IAAOc,EAAPd,EA2OFe,GAAiB,EAKrB,SAASC,IAAe,CACtB,MAAO,SAAS,EAAED,EAAc,IAAI,KAAK,IAAG,EAAG,SAAS,EAAE,CAAC,EAC7D,CAKM,IAAOE,EAAP,KAAsB,CAQ1B,YACEC,EACAd,EACAe,EAAmB,CAVZlB,EAAA,WACAA,EAAA,iBACAA,EAAA,gBAEDA,EAAA,iBAAY,IACHA,EAAA,iBAOf,KAAK,GAAKe,GAAe,EACzB,KAAK,SAAWE,EAChB,KAAK,QAAUd,EACf,KAAK,SAAWe,CAClB,CAEA,IAAI,UAAQ,CACV,OAAO,KAAK,SACd,CAEA,SAAO,CACD,KAAK,YACP,KAAK,UAAY,GACjB,KAAK,SAAQ,EACbC,EAAgB,EAAG,QAAQ,KAAK,EAAE,EAEtC,GAWF,eAAsBC,GACpBC,EACAC,EAMI,CAAA,EAAE,CAGN,IAAMnB,EAAU,MADAU,EAAe,YAAW,EACZ,WAAWS,EAAQ,SAAW,MAAM,EAG5D,CAAE,cAAAC,CAAa,EAAK,KAAM,uCAG1BC,EAAY,MAAMD,EAAcF,EAAK,CACzC,MAAOC,EAAQ,OAAS,GACxB,UAAWA,EAAQ,WAAa,GAChC,UAAWA,EAAQ,UACnB,cAAeA,EAAQ,cACvB,WAAYA,EAAQ,WAAcG,GAAY,CAC5CH,EAAQ,WAAYG,EAAS,QAAU,GAAG,CAC5C,EAAI,OACL,EAKD,OAFc,MAAMtB,EAAQ,UAAUqB,EAAWF,CAAO,CAG1D,CAKA,eAAsBI,GACpBd,EACAU,EAAwD,CAAA,EAAE,CAI1D,OADgB,MADAT,EAAe,YAAW,EACZ,WAAWS,EAAQ,SAAW,MAAM,GACnD,UAAUV,EAAMU,CAAO,CACxC,CASA,eAAsBK,GACpBC,EACAC,EAAgB,CAEhB,GAAI,CAACD,EAAM,SACT,MAAM,IAAIxB,EACR,0BACAC,EAAW,iBACX,CAAE,QAASuB,EAAM,EAAE,CAAE,EAKzB,IAAMzB,EAAU,MADAU,EAAe,YAAW,EACZ,WAAWe,EAAM,OAAO,EAMtD,OAHkBE,GAAY,EACP,SAASF,EAAM,GAAI,IAAMzB,EAAQ,IAAIyB,EAAOC,CAAM,CAAC,EAE9D,KAAI,CAClB,CAKA,eAAsBE,GACpBH,EACAI,EAAmB,CAEnB,IAAMC,EAAYH,GAAY,EAExB3B,EAAU,MADAU,EAAe,YAAW,EACZ,WAAWe,EAAM,OAAO,EAGhDM,EAAQF,EAAQ,IAAIH,GACxBI,EAAU,SAASL,EAAM,GAAI,IAAMzB,EAAQ,IAAIyB,EAAOC,CAAM,CAAC,CAAC,EAIhE,OAAO,QAAQ,IAAIK,EAAM,IAAIC,GAAQA,EAAK,KAAI,CAAE,CAAC,CACnD,CASM,SAAUC,IAAiB,CAC/B,OAAOvB,EAAe,YAAW,CACnC,CAKM,SAAUwB,GAAgBpC,EAAmBC,EAAsB,CACvEW,EAAe,YAAW,EAAG,SAASZ,EAAMC,CAAO,CACrD,CAKA,eAAsBoC,IAAc,CAClC,OAAOzB,EAAe,YAAW,EAAG,eAAc,CACpD,CAKA,eAAsB0B,IAAoB,CACxC,OAAO1B,EAAe,YAAW,EAAG,wBAAuB,CAC7D,CCjWA,IAAM2B,GAAiB,CACrB,QAAS,IACT,SAAU,EACV,SAAU,EACV,SAAU,GAGNC,GAAiB,CACrB,QAAS,GAoDEC,GAAP,KAAoB,CAApB,cACKC,EAAA,YAAoB,UAErBA,EAAA,eAA6B,MAC7BA,EAAA,cAA2B,MAC3BA,EAAA,cAAuC,IAAI,KAC3CA,EAAA,mBAAc,IAEtB,IAAI,cAAY,CACd,MAAO,CACL,YAAa,GACb,aAAc,GACd,QAAS,GACT,cAAe,GACf,aAAc,GACd,gBAAiB,KAAK,QAAQ,OAAO,eAAiB,IAAM,KAAO,KAEvE,CAKA,MAAM,aAAW,CAEf,GADI,OAAO,UAAc,KACrB,CAAC,UAAU,IAAK,MAAO,GAE3B,GAAI,CAEF,OADgB,MAAM,UAAU,IAAI,eAAc,IAC/B,IACrB,MAAQ,CACN,MAAO,EACT,CACF,CAKA,MAAM,YAAU,CACd,GAAI,MAAK,YAET,IAAI,CAAC,UAAU,IACb,MAAM,IAAIC,EACR,0CACAC,EAAW,qBAAqB,EASpC,GAJA,KAAK,QAAU,MAAM,UAAU,IAAI,eAAe,CAChD,gBAAiB,mBAClB,EAEG,CAAC,KAAK,QACR,MAAM,IAAID,EACR,+BACAC,EAAW,mBAAmB,EAKlC,KAAK,OAAS,MAAM,KAAK,QAAQ,cAAc,CAC7C,iBAAkB,CAAA,EAClB,eAAgB,CAAA,EACjB,EAGD,KAAK,OAAO,KAAK,KAAMC,GAA2B,CAChD,QAAQ,MAAM,0BAA2BA,EAAK,OAAO,EACrD,KAAK,YAAc,GACnB,KAAK,OAAS,IAChB,CAAC,EAED,KAAK,YAAc,GACrB,CAKA,MAAM,UACJC,EACAC,EAA4B,CAAA,EAAE,CAE9B,KAAK,kBAAiB,EAGtB,IAAMC,EAAS,KAAK,eAAeF,CAAS,EAGtCG,EAA8B,CAClC,QAAS,IAAI,IACb,UAAW,IAAI,IACf,QAAS,IAAI,IACb,iBAAkB,CAAA,EAClB,OAAAD,GAIF,MAAM,KAAK,cAAcF,EAAWG,CAAU,EAG9C,MAAM,KAAK,gBAAgBA,CAAU,EAGrC,IAAMC,EAAU,UAAU,KAAK,IAAG,EAAG,SAAS,EAAE,CAAC,GACjD,KAAK,OAAO,IAAIA,EAASD,CAAU,EAGnC,IAAME,EAA0B,CAC9B,KAAMH,EAAO,MAAQD,EAAQ,UAAU,MAAQ,UAC/C,QAASC,EAAO,QAChB,OAAQA,EAAO,OAAO,IAAII,IAAM,CAC9B,KAAMA,EAAE,KACR,MAAOA,EAAE,MACT,MAAOA,EAAE,OACT,EACF,QAASJ,EAAO,QAAQ,IAAIK,IAAM,CAChC,KAAMA,EAAE,KACR,MAAOA,EAAE,MACT,MAAOA,EAAE,OACT,EACF,UAAWP,EAAU,WACrB,aAAcC,EAAQ,cAAgB,UACtC,OAAQ,YAIJO,EAAQ,IAAIC,EAChBJ,EACA,SACA,IAAM,KAAK,YAAYD,CAAO,CAAC,EAIjC,OAAAM,EAAgB,EAAG,WAAWF,EAAO,IAAMA,EAAM,QAAO,CAAE,EAEnDA,CACT,CAKA,MAAM,IAAIA,EAAoBG,EAAgB,CAC5C,YAAK,kBAAiB,EAIf,KAAK,aAAaA,EAAQH,EAAM,QAAQ,CACjD,CAKQ,MAAM,aAAaG,EAAkBN,EAAuB,CAOlE,IAAMO,EAAS,KAAK,OACdC,EAAoB,CAAA,EAE1B,QAAWC,KAAcT,EAAS,QAAS,CAEzC,IAAMU,EAAaD,EAAW,MAAM,OAAO,CAACE,EAAGC,IAAMD,EAAIC,EAAG,CAAC,EACvDC,EAAeN,EAAO,aAAa,CACvC,KAAMG,EAAa,EACnB,MAAOtB,GAAe,QAAUA,GAAe,SAChD,EAGK0B,EAAgBP,EAAO,aAAa,CACxC,KAAMG,EAAa,EACnB,MAAOtB,GAAe,SAAWA,GAAe,SACjD,EAIK2B,EAAa,IAAI,aAAaL,CAAU,EAG9C,GAAIJ,EAAO,OAAS,GAAKA,EAAO,CAAC,EAAG,CAClC,IAAMU,EAAYV,EAAO,CAAC,EAAE,eAAc,EAC1C,QAASL,EAAI,EAAGA,EAAI,KAAK,IAAIS,EAAYM,EAAU,MAAM,EAAGf,IAC1Dc,EAAWd,CAAC,EAAKe,EAAUf,CAAC,GAAK,CAErC,CAEAO,EAAQ,KAAK,IAAIS,EAAeF,EAAYN,EAAW,MAAO,SAAS,CAAC,EAGxEI,EAAa,QAAO,EACpBC,EAAc,QAAO,CACvB,CAEA,OAAON,CACT,CAKQ,eAAeU,EAAiB,CAEtC,GAAI,CACF,IAAMC,EAAU,IAAI,YACdC,EAAOD,EAAQ,OAAO,IAAI,WAAWD,EAAM,EAAG,KAAK,IAAI,KAAMA,EAAK,UAAU,CAAC,CAAC,EAGpF,GAAIE,EAAK,KAAI,EAAG,WAAW,GAAG,EAAG,CAE/B,IAAIC,EAAUD,EAAK,QAAQ;;CAAS,EAChCC,IAAY,KAAIA,EAAUH,EAAK,YAEnC,IAAMI,EAAUH,EAAQ,OAAO,IAAI,WAAWD,EAAM,EAAGG,CAAO,CAAC,EAC/D,OAAO,KAAK,MAAMC,CAAO,CAC3B,CACF,MAAQ,CAER,CAGA,MAAO,CACL,KAAM,UACN,QAAS,QACT,OAAQ,CAAA,EACR,OAAQ,CAAC,CAAE,KAAM,QAAS,MAAO,CAAC,GAAI,GAAG,EAAG,MAAO,SAAS,CAAE,EAC9D,QAAS,CAAC,CAAE,KAAM,SAAU,MAAO,CAAC,GAAI,GAAG,EAAG,MAAO,SAAS,CAAE,EAEpE,CAKQ,MAAM,cACZC,EACA5B,EAA0B,CAQ1B,IAAM6B,EANS,KAAK,OAMS,aAAa,CACxC,KAAM,KACN,MAAOpC,GAAe,QAAUA,GAAe,SAChD,EAEDO,EAAU,QAAQ,IAAI,UAAW6B,CAAa,CAChD,CAKQ,MAAM,gBAAgB7B,EAA0B,CACtD,IAAMY,EAAS,KAAK,OAgBdkB,EAAelB,EAAO,mBAAmB,CAC7C,KAd4B;;;;;;;;;;;MAe7B,EAEDZ,EAAU,QAAQ,IAAI,UAAW8B,CAAY,EAG7C,IAAMC,EAAkBnB,EAAO,sBAAsB,CACnD,QAAS,CACP,CACE,QAAS,EACT,WAAYlB,GAAe,QAC3B,OAAQ,CAAE,KAAM,mBAAmB,GAErC,CACE,QAAS,EACT,WAAYA,GAAe,QAC3B,OAAQ,CAAE,KAAM,SAAS,IAG9B,EAEDM,EAAU,iBAAiB,KAAK+B,CAAe,EAG/C,IAAMC,EAAiBpB,EAAO,qBAAqB,CACjD,iBAAkB,CAACmB,CAAe,EACnC,EAGKE,EAAWrB,EAAO,sBAAsB,CAC5C,OAAQoB,EACR,QAAS,CACP,OAAQF,EACR,WAAY,QAEf,EAED9B,EAAU,UAAU,IAAI,UAAWiC,CAAQ,CAC7C,CAKQ,YAAY7B,EAAe,CACjC,IAAMJ,EAAY,KAAK,OAAO,IAAII,CAAO,EACzC,GAAIJ,EAAW,CAEb,QAAWkC,KAAUlC,EAAU,QAAQ,OAAM,EAC3CkC,EAAO,QAAO,EAEhB,KAAK,OAAO,OAAO9B,CAAO,CAC5B,CACF,CAKQ,mBAAiB,CACvB,GAAI,CAAC,KAAK,aAAe,CAAC,KAAK,OAC7B,MAAM,IAAIP,EACR,oCACAC,EAAW,uBAAuB,CAGxC,CAKA,SAAO,CAEL,QAAWM,KAAW,KAAK,OAAO,KAAI,EACpC,KAAK,YAAYA,CAAO,EAItB,KAAK,SACP,KAAK,OAAO,QAAO,EACnB,KAAK,OAAS,MAGhB,KAAK,QAAU,KACf,KAAK,YAAc,EACrB,GAMI,SAAU+B,IAAmB,CACjC,OAAO,IAAIxC,EACb,CCtZM,IAAOyC,GAAP,KAAmB,CAAnB,cACKC,EAAA,YAAoB,SAErBA,EAAA,eAA4B,MAC5BA,EAAA,cAAsC,IAAI,KAC1CA,EAAA,mBAAc,IACdA,EAAA,kBAA4B,WAEpC,IAAI,cAAY,CACd,MAAO,CACL,YAAa,GACb,aAAc,GACd,QAAS,GACT,cAAe,GACf,aAAc,GACd,gBAAiB,IAAM,KAAO,KAElC,CAKA,MAAM,aAAW,CAEf,GADI,OAAO,UAAc,KACrB,CAAC,UAAU,GAAI,MAAO,GAE1B,GAAI,CAEF,OADgB,MAAM,UAAU,GAAG,cAAc,CAAE,WAAY,SAAS,CAAE,IACvD,IACrB,MAAQ,CACN,MAAO,EACT,CACF,CAKA,MAAM,YAAU,CACd,GAAI,MAAK,YAET,IAAI,CAAC,UAAU,GACb,MAAM,IAAIC,EACR,yCACAC,EAAW,qBAAqB,EAKpC,GAAI,CACF,KAAK,QAAU,MAAM,UAAU,GAAG,cAAc,CAC9C,WAAY,MACZ,gBAAiB,mBAClB,EACD,KAAK,WAAa,KACpB,MAAQ,CACN,GAAI,CACF,KAAK,QAAU,MAAM,UAAU,GAAG,cAAc,CAAE,WAAY,KAAK,CAAE,EACrE,KAAK,WAAa,KACpB,OAASC,EAAO,CACd,MAAM,IAAIF,EACR,mCAAmCE,aAAiB,MAAQA,EAAM,QAAU,OAAOA,CAAK,CAAC,GACzFD,EAAW,mBAAmB,CAElC,CACF,CAEA,KAAK,YAAc,GACrB,CAKA,MAAM,UACJE,EACAC,EAA4B,CAAA,EAAE,CAE9B,KAAK,kBAAiB,EAGtB,IAAMC,EAAS,KAAK,iBAAiBF,CAAS,EAKxCG,EAAU,SAAS,KAAK,IAAG,EAAG,SAAS,EAAE,CAAC,GAG1CC,EAA0B,CAC9B,KAAMF,EAAO,MAAQD,EAAQ,UAAU,MAAQ,UAC/C,QAASC,EAAO,SAAW,QAC3B,OAAQA,EAAO,OAAO,IAAIG,IAAM,CAC9B,KAAMA,EAAE,KACR,MAAOA,EAAE,MACT,MAAOA,EAAE,OACT,EACF,QAASH,EAAO,QAAQ,IAAII,IAAM,CAChC,KAAMA,EAAE,KACR,MAAOA,EAAE,MACT,MAAOA,EAAE,OACT,EACF,UAAWN,EAAU,WACrB,aAAcC,EAAQ,cAAgB,UACtC,OAAQ,YAIJM,EAAQ,IAAIC,EAChBJ,EACA,QACA,IAAM,KAAK,YAAYD,CAAO,CAAC,EAIjC,OAAAM,EAAgB,EAAG,WAAWF,EAAO,IAAMA,EAAM,QAAO,CAAE,EAEnDA,CACT,CAKA,MAAM,IAAIA,EAAoBG,EAAgB,CAC5C,YAAK,kBAAiB,EAGf,KAAK,aAAaA,EAAQH,EAAM,QAAQ,CACjD,CAKQ,MAAM,aAAaG,EAAkBN,EAAuB,CAClE,IAAMO,EAAoB,CAAA,EAG1B,QAAWC,KAAcR,EAAS,QAAS,CACzC,IAAMS,EAAaD,EAAW,MAAM,OAAO,CAAC,EAAGE,IAAM,EAAIA,EAAG,CAAC,EACvDC,EAAa,IAAI,aAAaF,CAAU,EAG9C,GAAIH,EAAO,OAAS,GAAKA,EAAO,CAAC,EAAG,CAClC,IAAMM,EAAYN,EAAO,CAAC,EAAE,eAAc,EAC1C,QAASL,EAAI,EAAGA,EAAI,KAAK,IAAIQ,EAAYG,EAAU,MAAM,EAAGX,IAC1DU,EAAWV,CAAC,EAAIW,EAAUX,CAAC,GAAK,CAEpC,CAEAM,EAAQ,KAAK,IAAIM,EAAeF,EAAYH,EAAW,MAAO,SAAS,CAAC,CAC1E,CAEA,OAAOD,CACT,CAKQ,iBAAiBO,EAAiB,CACxC,GAAI,CACF,IAAMC,EAAU,IAAI,YACdC,EAAOD,EAAQ,OAAO,IAAI,WAAWD,EAAM,EAAG,KAAK,IAAI,KAAMA,EAAK,UAAU,CAAC,CAAC,EAEpF,GAAIE,EAAK,KAAI,EAAG,WAAW,GAAG,EAAG,CAC/B,IAAIC,EAAUD,EAAK,QAAQ;;CAAS,EAChCC,IAAY,KAAIA,EAAUH,EAAK,YAEnC,IAAMI,EAAUH,EAAQ,OAAO,IAAI,WAAWD,EAAM,EAAGG,CAAO,CAAC,EAC/D,OAAO,KAAK,MAAMC,CAAO,CAC3B,CACF,MAAQ,CAER,CAEA,MAAO,CACL,KAAM,UACN,QAAS,QACT,OAAQ,CAAC,CAAE,KAAM,QAAS,MAAO,CAAC,GAAI,GAAG,EAAG,MAAO,SAAS,CAAE,EAC9D,QAAS,CAAC,CAAE,KAAM,SAAU,MAAO,CAAC,GAAI,GAAG,EAAG,MAAO,SAAS,CAAE,EAEpE,CAKQ,YAAYnB,EAAe,CACjC,KAAK,OAAO,OAAOA,CAAO,CAC5B,CAKQ,mBAAiB,CACvB,GAAI,CAAC,KAAK,aAAe,CAAC,KAAK,QAC7B,MAAM,IAAIN,EACR,mCACAC,EAAW,uBAAuB,CAGxC,CAKA,eAAa,CACX,OAAO,KAAK,UACd,CAKA,SAAO,CACL,KAAK,OAAO,MAAK,EACjB,KAAK,QAAU,KACf,KAAK,YAAc,EACrB,GAMI,SAAUyB,IAAkB,CAChC,OAAO,IAAI5B,EACb,CCpPM,IAAO6B,GAAP,KAAkB,CAAlB,cACKC,EAAA,YAAoB,QAErBA,EAAA,cAA4B,MAC5BA,EAAA,qBAAgB,IAChBA,EAAA,cAAqC,IAAI,KACzCA,EAAA,mBAAc,IAEtB,IAAI,cAAY,CACd,MAAO,CACL,YAAa,GACb,aAAc,GACd,QAAS,GACT,cAAe,GACf,aAAc,GACd,gBAAiB,IAAM,KAAO,KAElC,CAKA,MAAM,aAAW,CACf,GAAI,OAAO,YAAgB,IAAa,MAAO,GAE/C,GAAI,CAEF,IAAMC,EAAQ,IAAI,WAAW,CAC3B,EAAM,GAAM,IAAM,IAClB,EAAM,EAAM,EAAM,EACnB,EACD,aAAM,YAAY,YAAYA,CAAK,EAC5B,EACT,MAAQ,CACN,MAAO,EACT,CACF,CAKA,MAAM,YAAU,CACd,GAAI,KAAK,YAAa,OAGtB,KAAK,cAAgB,MAAM,KAAK,iBAAgB,EAGhD,IAAMC,EAAS,IAAI,YAAY,OAAO,CACpC,QAAS,IACT,QAAS,KACV,EAKD,KAAK,OAAS,CACZ,OAAAA,EACA,QAAS,KAAK,iBAAiBA,CAAM,GAGvC,KAAK,YAAc,EACrB,CAKQ,MAAM,kBAAgB,CAC5B,GAAI,CAEF,IAAMC,EAAW,IAAI,WAAW,CAC9B,EAAM,GAAM,IAAM,IAAM,EAAM,EAAM,EAAM,EAC1C,EAAM,EAAM,EAAM,GAAM,EAAM,EAAM,IAAM,EAC1C,EAAM,EAAM,EAAM,GAAM,GAAM,EAAM,EAAM,EAC1C,IAAM,GAAM,EAAM,EAAM,EAAM,EAAM,GACrC,EACD,aAAM,YAAY,YAAYA,CAAQ,EAC/B,EACT,MAAQ,CACN,MAAO,EACT,CACF,CAKQ,iBAAiBD,EAA0B,CACjD,IAAIE,EAAU,EACRC,EAAmC,IAAI,IAE7C,MAAO,CACL,OAASC,GAAwB,CAC/B,IAAMC,EAAMH,EACZ,OAAAA,GAAWE,EACXD,EAAY,IAAIE,EAAKD,CAAI,EAClBC,CACT,EAEA,KAAOA,GAAqB,CAC1BF,EAAY,OAAOE,CAAG,CACxB,EAEA,WAAY,CACVC,EAAcC,EAAeC,EAC7BC,EAAcC,EAAgBC,EAC9BC,IACQ,CACR,IAAMC,EAAO,IAAI,aAAab,EAAO,MAAM,EACrCc,EAAUR,EAAO,EACjBS,EAAUN,EAAO,EACjBO,EAAYJ,EAAS,EAE3B,QAASK,EAAI,EAAGA,EAAIV,EAAOU,IACzB,QAASC,EAAI,EAAGA,EAAIP,EAAOO,IAAK,CAC9B,IAAIC,EAAM,EACV,QAAS,EAAI,EAAG,EAAIX,EAAO,IACzBW,IAAQN,EAAKC,EAAUG,EAAIT,EAAQ,CAAC,GAAK,IAAMK,EAAKE,EAAU,EAAIJ,EAAQO,CAAC,GAAK,GAElFL,EAAKG,EAAYC,EAAIN,EAAQO,CAAC,EAAIC,CACpC,CAEJ,EAEA,QAAS,CAACb,EAAcG,EAAcG,EAAgBR,IAAsB,CAC1E,IAAMS,EAAO,IAAI,aAAab,EAAO,MAAM,EACrCc,EAAUR,EAAO,EACjBS,EAAUN,EAAO,EACjBO,EAAYJ,EAAS,EAE3B,QAASK,EAAI,EAAGA,EAAIb,EAAMa,IACxBJ,EAAKG,EAAYC,CAAC,GAAKJ,EAAKC,EAAUG,CAAC,GAAK,IAAMJ,EAAKE,EAAUE,CAAC,GAAK,EAE3E,EAEA,QAAS,CAACX,EAAcG,EAAcG,EAAgBR,IAAsB,CAC1E,IAAMS,EAAO,IAAI,aAAab,EAAO,MAAM,EACrCc,EAAUR,EAAO,EACjBS,EAAUN,EAAO,EACjBO,EAAYJ,EAAS,EAE3B,QAASK,EAAI,EAAGA,EAAIb,EAAMa,IACxBJ,EAAKG,EAAYC,CAAC,GAAKJ,EAAKC,EAAUG,CAAC,GAAK,IAAMJ,EAAKE,EAAUE,CAAC,GAAK,EAE3E,EAEA,SAAU,CAACG,EAAkBC,EAAmBjB,IAAsB,CACpE,IAAMS,EAAO,IAAI,aAAab,EAAO,MAAM,EACrCsB,EAAWF,EAAW,EACtBJ,EAAYK,EAAY,EAE9B,QAASJ,EAAI,EAAGA,EAAIb,EAAMa,IACxBJ,EAAKG,EAAYC,CAAC,EAAI,KAAK,IAAI,EAAGJ,EAAKS,EAAWL,CAAC,GAAK,CAAC,CAE7D,EAEA,YAAa,CAACG,EAAkBC,EAAmBjB,IAAsB,CACvE,IAAMS,EAAO,IAAI,aAAab,EAAO,MAAM,EACrCsB,EAAWF,EAAW,EACtBJ,EAAYK,EAAY,EAE9B,QAASJ,EAAI,EAAGA,EAAIb,EAAMa,IACxBJ,EAAKG,EAAYC,CAAC,EAAI,GAAK,EAAI,KAAK,IAAI,EAAEJ,EAAKS,EAAWL,CAAC,GAAK,EAAE,EAEtE,EAEA,YAAa,CAACG,EAAkBC,EAAmBjB,IAAsB,CACvE,IAAMS,EAAO,IAAI,aAAab,EAAO,MAAM,EACrCsB,EAAWF,EAAW,EACtBJ,EAAYK,EAAY,EAG1BE,EAAM,KACV,QAASN,EAAI,EAAGA,EAAIb,EAAMa,KACnBJ,EAAKS,EAAWL,CAAC,GAAK,GAAKM,IAAKA,EAAMV,EAAKS,EAAWL,CAAC,GAAK,GAInE,IAAIE,EAAM,EACV,QAASF,EAAI,EAAGA,EAAIb,EAAMa,IACxBJ,EAAKG,EAAYC,CAAC,EAAI,KAAK,KAAKJ,EAAKS,EAAWL,CAAC,GAAK,GAAKM,CAAG,EAC9DJ,GAAON,EAAKG,EAAYC,CAAC,GAAK,EAIhC,QAASA,EAAI,EAAGA,EAAIb,EAAMa,IACxBJ,EAAKG,EAAYC,CAAC,GAAKJ,EAAKG,EAAYC,CAAC,GAAK,GAAKE,CAEvD,EAEJ,CAKA,MAAM,UACJK,EACAC,EAA4B,CAAA,EAAE,CAE9B,KAAK,kBAAiB,EAGtB,IAAMC,EAAS,KAAK,iBAAiBF,CAAS,EAGxCG,EAA0B,CAC9B,QAAS,IAAI,IACb,OAAAD,EACA,eAAgBA,EAAO,OAAO,IAAIE,GAAKA,EAAE,IAAI,GAI/C,MAAM,KAAK,YAAYJ,EAAWG,CAAQ,EAE1C,IAAME,EAAU,QAAQ,KAAK,IAAG,EAAG,SAAS,EAAE,CAAC,GAC/C,KAAK,OAAO,IAAIA,EAASF,CAAQ,EAGjC,IAAMG,EAA0B,CAC9B,KAAMJ,EAAO,MAAQD,EAAQ,UAAU,MAAQ,UAC/C,QAASC,EAAO,SAAW,QAC3B,OAAQA,EAAO,OAAO,IAAIT,IAAM,CAC9B,KAAMA,EAAE,KACR,MAAOA,EAAE,MACT,MAAOA,EAAE,OACT,EACF,QAASS,EAAO,QAAQ,IAAIK,IAAM,CAChC,KAAMA,EAAE,KACR,MAAOA,EAAE,MACT,MAAOA,EAAE,OACT,EACF,UAAWP,EAAU,WACrB,aAAcC,EAAQ,cAAgB,UACtC,OAAQ,YAIJO,EAAQ,IAAIC,EAChBH,EACA,OACA,IAAM,KAAK,YAAYD,CAAO,CAAC,EAIjC,OAAAK,EAAgB,EAAG,WAAWF,EAAO,IAAMA,EAAM,QAAO,CAAE,EAEnDA,CACT,CAKA,MAAM,IAAIA,EAAoBG,EAAgB,CAC5C,YAAK,kBAAiB,EAGf,KAAK,aAAaA,EAAQH,EAAM,QAAQ,CACjD,CAKQ,MAAM,aAAaG,EAAkBL,EAAuB,CAClE,IAAMM,EAAoB,CAAA,EAE1B,QAAWC,KAAcP,EAAS,QAAS,CACzC,IAAMQ,EAAaD,EAAW,MAAM,OAAO,CAAC,EAAGE,IAAM,EAAIA,EAAG,CAAC,EAIzDC,EAEJ,GAAIL,EAAO,OAAS,GAAKA,EAAO,CAAC,EAAG,CAClC,IAAMM,EAAcN,EAAO,CAAC,EAI5B,GAAIE,EAAW,KAAK,SAAS,QAAQ,GAAKA,EAAW,KAAK,SAAS,OAAO,EACxEG,EAAeE,EAAcD,CAAW,UAC/BJ,EAAW,KAAK,SAAS,MAAM,EACxCG,EAAeG,GAAWF,CAAW,UAC5BJ,EAAW,KAAK,SAAS,SAAS,EAC3CG,EAAeI,GAAcH,CAAW,MACnC,CAEL,IAAMI,EAAa,IAAI,aAAaP,CAAU,EACxCQ,EAAYL,EAAY,eAAc,EAC5C,QAASxB,EAAI,EAAGA,EAAI,KAAK,IAAIqB,EAAYQ,EAAU,MAAM,EAAG7B,IAC1D4B,EAAW5B,CAAC,EAAI6B,EAAU7B,CAAC,GAAK,EAElCuB,EAAe,IAAIO,EAAeF,EAAYR,EAAW,MAAO,SAAS,CAC3E,CACF,MACEG,EAAe,IAAIO,EAAe,IAAI,aAAaT,CAAU,EAAGD,EAAW,MAAO,SAAS,EAG7FD,EAAQ,KAAKI,CAAY,CAC3B,CAEA,OAAOJ,CACT,CAKQ,iBAAiBY,EAAiB,CACxC,GAAI,CACF,IAAMC,EAAU,IAAI,YACdC,EAAOD,EAAQ,OAAO,IAAI,WAAWD,EAAM,EAAG,KAAK,IAAI,KAAMA,EAAK,UAAU,CAAC,CAAC,EAEpF,GAAIE,EAAK,KAAI,EAAG,WAAW,GAAG,EAAG,CAC/B,IAAIC,EAAUD,EAAK,QAAQ;;CAAS,EACpC,GAAIC,IAAY,GAEd,GAAI,CACF,OAAO,KAAK,MAAMD,CAAI,CACxB,MAAQ,CACNC,EAAUH,EAAK,UACjB,CAGF,IAAMI,EAAUH,EAAQ,OAAO,IAAI,WAAWD,EAAM,EAAGG,CAAO,CAAC,EAC/D,OAAO,KAAK,MAAMC,CAAO,CAC3B,CACF,MAAQ,CAER,CAEA,MAAO,CACL,KAAM,UACN,QAAS,QACT,OAAQ,CAAA,EACR,OAAQ,CAAC,CAAE,KAAM,QAAS,MAAO,CAAC,GAAI,GAAG,EAAG,MAAO,SAAS,CAAE,EAC9D,QAAS,CAAC,CAAE,KAAM,SAAU,MAAO,CAAC,GAAI,GAAG,EAAG,MAAO,SAAS,CAAE,EAEpE,CAKQ,MAAM,YACZC,EACAC,EAAwB,CAI1B,CAKQ,YAAYzB,EAAe,CACjC,IAAML,EAAY,KAAK,OAAO,IAAIK,CAAO,EACzC,GAAIL,GAAa,KAAK,OAEpB,QAAW+B,KAAU/B,EAAU,QAAQ,OAAM,EAC3C,KAAK,OAAO,QAAQ,KAAK+B,EAAO,GAAG,EAGvC,KAAK,OAAO,OAAO1B,CAAO,CAC5B,CAKQ,mBAAiB,CACvB,GAAI,CAAC,KAAK,aAAe,CAAC,KAAK,OAC7B,MAAM,IAAI2B,EACR,kCACAC,EAAW,uBAAuB,CAGxC,CAKA,gBAAc,CACZ,OAAO,KAAK,aACd,CAKA,SAAO,CAEL,QAAW5B,KAAW,KAAK,OAAO,KAAI,EACpC,KAAK,YAAYA,CAAO,EAG1B,KAAK,OAAS,KACd,KAAK,YAAc,EACrB,GAMI,SAAU6B,IAAiB,CAC/B,OAAO,IAAI7D,EACb,CCneA,IAAM8D,GAAe,SACfC,GAAgB,gDAAgDD,EAAY,SAC5EE,GAAkB,GAAGD,EAAa,aAGpCE,EAA+C,KAC/CC,GAAmE,KAKvE,eAAeC,IAAe,CAE5B,OAAIF,GAGAC,KAEJA,GAAiB,IAAI,QAAQ,CAACE,EAASC,IAAU,CAE/C,GAAI,OAAO,OAAW,KAAgB,OAAe,IAAK,CACxDJ,EAAO,OAAe,IAEtBA,EAAK,IAAI,KAAK,UAAYF,GAC1BK,EAAQH,CAAI,EACZ,MACF,CAGA,IAAMK,EAAS,SAAS,cAAc,QAAQ,EAC9CA,EAAO,IAAMN,GACbM,EAAO,MAAQ,GAEfA,EAAO,OAAS,IAAK,CACd,OAAe,KAClBL,EAAO,OAAe,IAEtBA,EAAK,IAAI,KAAK,UAAYF,GAC1B,QAAQ,IAAI,wBAAmBD,EAAY,kBAAkB,EAC7DM,EAAQH,CAAI,GAEZI,EAAO,IAAI,MAAM,8CAA8C,CAAC,CAEpE,EAEAC,EAAO,QAAU,IAAK,CACpBD,EAAO,IAAI,MAAM,oCAAoCL,EAAe,EAAE,CAAC,CACzE,EAEA,SAAS,KAAK,YAAYM,CAAM,CAClC,CAAC,EAEMJ,GACT,CAKA,eAAeK,IAAM,CACnB,OAAKN,IACHA,EAAM,MAAME,GAAe,GAEtBF,CACT,CAYA,IAAMO,GAA6C,IAAI,IAU1CC,GAAP,KAAkB,CAAlB,cACKC,EAAA,YAAoB,QAErBA,EAAA,mBAAc,IACdA,EAAA,yBAAuC,QAE/C,IAAI,cAAY,CACd,MAAO,CACL,YAAa,GACb,aAAc,GACd,QAAS,KAAK,oBAAsB,SACpC,cAAe,GACf,aAAc,GACd,gBAAiB,IAAM,KAAO,KAElC,CAKA,MAAM,aAAW,CAEf,MAAO,EACT,CAKA,MAAM,YAAU,CACd,GAAI,KAAK,YAAa,OAGtB,IAAMC,EAAc,MAAMJ,GAAM,EAGhCI,EAAY,IAAI,KAAK,UAAYZ,GAGjC,KAAK,kBAAoB,OAEzB,KAAK,YAAc,EACrB,CAKA,MAAM,UACJa,EACAC,EAA4B,CAAA,EAAE,CAEzB,KAAK,aACR,MAAM,KAAK,WAAU,EAGvB,IAAMF,EAAc,MAAMJ,GAAM,EAEhC,GAAI,CAEF,IAAMO,EAAiB,CACrB,mBAAoB,CAAC,KAAK,iBAAiB,EAC3C,uBAAwB,OAIpBC,EAAa,IAAI,WAAWH,CAAS,EACrCI,EAAU,MAAML,EAAY,iBAAiB,OAAOI,EAAYD,CAAc,EAG9EG,EAAaD,EAAQ,WACrBE,EAAcF,EAAQ,YAGtBG,EAAU,QAAQ,KAAK,IAAG,EAAG,SAAS,EAAE,CAAC,IAAI,KAAK,OAAM,EAAG,SAAS,EAAE,EAAE,MAAM,EAAG,CAAC,CAAC,GAGzFX,GAAa,IAAIW,EAAS,CACxB,QAAAH,EACA,WAAY,CAAC,GAAGC,CAAU,EAC1B,YAAa,CAAC,GAAGC,CAAW,EAC7B,EAGD,IAAME,EAA0B,CAC9B,KAAMP,EAAQ,UAAU,MAAQ,aAChC,QAAS,QACT,OAAQI,EAAW,IAAII,IAAS,CAC9B,KAAAA,EACA,MAAO,UACP,MAAO,CAAC,EAAE,GACV,EACF,QAASH,EAAY,IAAIG,IAAS,CAChC,KAAAA,EACA,MAAO,UACP,MAAO,CAAC,EAAE,GACV,EACF,UAAWT,EAAU,WACrB,aAAcC,EAAQ,cAAgB,UACtC,OAAQ,QAIJS,EAAQ,IAAIC,EAChBH,EACA,OACA,IAAM,KAAK,YAAYD,CAAO,CAAC,EAIjC,cAAO,eAAeG,EAAO,KAAM,CAAE,MAAOH,EAAS,SAAU,EAAK,CAAE,EAGtEK,EAAgB,EAAG,WAAWF,EAAO,IAAMA,EAAM,QAAO,CAAE,EAEnDA,CACT,OAASG,EAAO,CACd,MAAM,IAAIC,EACR,8BAA8BD,aAAiB,MAAQA,EAAM,QAAU,OAAOA,CAAK,CAAC,GACpFE,EAAW,kBACX,CAAE,MAAAF,CAAK,CAAE,CAEb,CACF,CAKA,MAAM,IAAIH,EAAoBM,EAAgB,CAC5C,IAAMC,EAAcrB,GAAa,IAAIc,EAAM,EAAE,EAC7C,GAAI,CAACO,EACH,MAAM,IAAIH,EACR,oCAAoCJ,EAAM,EAAE,GAC5CK,EAAW,iBACX,CAAE,QAASL,EAAM,EAAE,CAAE,EAIzB,IAAMX,EAAc,MAAMJ,GAAM,EAC1B,CAAE,QAAAS,EAAS,WAAAC,EAAY,YAAAC,CAAW,EAAKW,EAE7C,GAAI,CAEF,IAAMC,EAA6B,CAAA,EAEnC,QAASC,EAAI,EAAGA,EAAI,KAAK,IAAIH,EAAO,OAAQX,EAAW,MAAM,EAAGc,IAAK,CACnE,IAAMC,EAAYf,EAAWc,CAAC,EACxBE,EAAcL,EAAOG,CAAC,EAE5B,GAAIC,GAAaC,EAAa,CAE5B,IAAMC,EAAQD,EAAY,MACtBE,EAEJ,GAAID,IAAU,QAAS,CAErB,IAAME,EAAOH,EAAY,KACzBE,EAAY,IAAIxB,EAAY,OAAO,QAASyB,EAAMH,EAAY,KAAiB,CACjF,SAAWC,IAAU,QAAS,CAC5B,IAAME,EAAOH,EAAY,KACzBE,EAAY,IAAIxB,EAAY,OAAO,QAASyB,EAAMH,EAAY,KAAiB,CACjF,KAAO,CACL,IAAMG,EAAOH,EAAY,eAAc,EACvCE,EAAY,IAAIxB,EAAY,OAAO,UAAWyB,EAAMH,EAAY,KAAiB,CACnF,CAEAH,EAAME,CAAS,EAAIG,CACrB,CACF,CAGA,IAAME,EAAU,MAAMrB,EAAQ,IAAIc,CAAK,EAGjCQ,EAAoB,CAAA,EAE1B,QAAWC,KAAcrB,EAAa,CACpC,IAAMiB,EAAYE,EAAQE,CAAU,EACpC,GAAIJ,EAAW,CACb,IAAMC,EAAOD,EAAU,KACjBK,EAAQ,MAAM,KAAKL,EAAU,IAAI,EAAE,IAAIM,GAAK,OAAOA,CAAC,CAAC,EAC3DH,EAAQ,KAAK,IAAII,EAAe,IAAI,aAAaN,CAAI,EAAGI,EAAO,SAAS,CAAC,CAC3E,CACF,CAEA,OAAOF,CACT,OAASb,EAAO,CACd,MAAM,IAAIC,EACR,0BAA0BD,aAAiB,MAAQA,EAAM,QAAU,OAAOA,CAAK,CAAC,GAChFE,EAAW,iBACX,CAAE,QAASL,EAAM,GAAI,MAAAG,CAAK,CAAE,CAEhC,CACF,CAKQ,MAAM,YAAYN,EAAe,CACnBX,GAAa,IAAIW,CAAO,GAG1CX,GAAa,OAAOW,CAAO,CAE/B,CAKA,SAAO,CAELX,GAAa,MAAK,EAClB,KAAK,YAAc,EACrB,GAMI,SAAUmC,IAAiB,CAC/B,OAAO,IAAIlC,EACb,CC1SM,SAAUmC,IAAmB,CACjCC,GAAgB,SAAUC,EAAmB,EAC7CD,GAAgB,QAASE,EAAkB,EAE3CF,GAAgB,OAAQG,EAAiB,CAC3C,CAKAJ,GAAmB,EC4Bb,IAAOK,GAAP,KAAY,CAOhB,YAAYC,EAAwB,CAAA,EAAE,CANrBC,EAAA,gBACAA,EAAA,aAAoC,IAAI,KACjDA,EAAA,mBAAc,GACdA,EAAA,YAAO,GACPA,EAAA,cAAS,GAGf,KAAK,QAAU,CACb,SAAUD,EAAQ,UAAY,MAC9B,QAASA,EAAQ,SAAW,IAAM,KAAO,KACzC,WAAYA,EAAQ,YAAc,IAClC,IAAKA,EAAQ,KAAO,EACpB,WAAYA,EAAQ,YAAc,GAClC,KAAMA,EAAQ,MAAQ,kBAIpB,KAAK,QAAQ,YACf,KAAK,gBAAe,CAExB,CAKA,IAAIE,EAAW,CACb,IAAMC,EAAQ,KAAK,MAAM,IAAID,CAAG,EAEhC,GAAI,CAACC,EAAO,CACV,KAAK,SACL,MACF,CAGA,GAAIA,EAAM,KAAO,KAAK,IAAG,EAAKA,EAAM,UAAYA,EAAM,IAAK,CACzD,KAAK,OAAOD,CAAG,EACf,KAAK,SACL,MACF,CAGA,OAAAC,EAAM,WAAa,KAAK,IAAG,EAC3BA,EAAM,cACN,KAAK,OAEEA,EAAM,KACf,CAKA,IAAID,EAAaE,EAAUC,EAAcC,EAAY,CAOnD,IALI,KAAK,MAAM,IAAIJ,CAAG,GACpB,KAAK,OAAOA,CAAG,GAKd,KAAK,YAAcG,EAAO,KAAK,QAAQ,SACvC,KAAK,MAAM,MAAQ,KAAK,QAAQ,aACjC,KAAK,MAAM,KAAO,GAElB,KAAK,MAAK,EAIZ,IAAME,EAAWD,IAAQ,OAAYA,EAAO,KAAK,QAAQ,IAAM,EAAI,KAAK,QAAQ,IAAM,OAGhFH,EAAuB,CAC3B,MAAAC,EACA,KAAAC,EACA,UAAW,KAAK,IAAG,EACnB,WAAY,KAAK,IAAG,EACpB,YAAa,EACb,IAAKE,GAGP,KAAK,MAAM,IAAIL,EAAKC,CAAK,EACzB,KAAK,aAAeE,EAGhB,KAAK,QAAQ,YACf,KAAK,cAAa,CAEtB,CAKA,IAAIH,EAAW,CACb,IAAMC,EAAQ,KAAK,MAAM,IAAID,CAAG,EAChC,OAAKC,EAGDA,EAAM,KAAO,KAAK,IAAG,EAAKA,EAAM,UAAYA,EAAM,KACpD,KAAK,OAAOD,CAAG,EACR,IAGF,GARY,EASrB,CAKA,OAAOA,EAAW,CAChB,IAAMC,EAAQ,KAAK,MAAM,IAAID,CAAG,EAChC,OAAIC,GACF,KAAK,aAAeA,EAAM,KAC1B,KAAK,MAAM,OAAOD,CAAG,EAEjB,KAAK,QAAQ,YACf,KAAK,cAAa,EAGb,IAEF,EACT,CAKA,OAAK,CACH,KAAK,MAAM,MAAK,EAChB,KAAK,YAAc,EACnB,KAAK,KAAO,EACZ,KAAK,OAAS,EAEV,KAAK,QAAQ,YACf,KAAK,aAAY,CAErB,CAKA,UAAQ,CACN,IAAMM,EAAQ,KAAK,KAAO,KAAK,OAC/B,MAAO,CACL,QAAS,KAAK,MAAM,KACpB,KAAM,KAAK,YACX,KAAM,KAAK,KACX,OAAQ,KAAK,OACb,QAASA,EAAQ,EAAI,KAAK,KAAOA,EAAQ,EAE7C,CAKQ,OAAK,CACX,IAAIC,EAA4B,KAEhC,OAAQ,KAAK,QAAQ,SAAU,CAC7B,IAAK,MACHA,EAAa,KAAK,QAAO,EACzB,MACF,IAAK,MACHA,EAAa,KAAK,QAAO,EACzB,MACF,IAAK,OACHA,EAAa,KAAK,WAAU,EAC5B,MACF,IAAK,MACHA,EAAa,KAAK,YAAW,GAAM,KAAK,WAAU,EAClD,KACJ,CAEIA,GACF,KAAK,OAAOA,CAAU,CAE1B,CAKQ,SAAO,CACb,IAAIC,EAAwB,KACxBC,EAAa,IAEjB,OAAW,CAACT,EAAKC,CAAK,IAAK,KAAK,MAC1BA,EAAM,WAAaQ,IACrBA,EAAaR,EAAM,WACnBO,EAASR,GAIb,OAAOQ,CACT,CAKQ,SAAO,CACb,IAAIE,EAAqB,KACrBC,EAAW,IAEf,OAAW,CAACX,EAAKC,CAAK,IAAK,KAAK,MAC1BA,EAAM,YAAcU,IACtBA,EAAWV,EAAM,YACjBS,EAAMV,GAIV,OAAOU,CACT,CAKQ,YAAU,CAChB,IAAIF,EAAwB,KACxBC,EAAa,IAEjB,OAAW,CAACT,EAAKC,CAAK,IAAK,KAAK,MAC1BA,EAAM,UAAYQ,IACpBA,EAAaR,EAAM,UACnBO,EAASR,GAIb,OAAOQ,CACT,CAKQ,aAAW,CACjB,IAAMI,EAAM,KAAK,IAAG,EAEpB,OAAW,CAACZ,EAAKC,CAAK,IAAK,KAAK,MAC9B,GAAIA,EAAM,KAAOW,EAAMX,EAAM,UAAYA,EAAM,IAC7C,OAAOD,EAIX,OAAO,IACT,CAKQ,MAAM,iBAAe,CAC3B,GAAI,SAAO,UAAc,KAEzB,GAAI,CAIF,IAAMa,GAHK,MAAM,KAAK,OAAM,GACd,YAAY,QAAS,UAAU,EAC5B,YAAY,OAAO,EACd,OAAM,EAE5B,OAAO,IAAI,QAAQ,CAACC,EAASC,IAAU,CACrCF,EAAQ,UAAY,IAAK,CACvB,IAAMG,EAAUH,EAAQ,OACxB,OAAW,CAAE,IAAAb,EAAK,MAAAC,CAAK,IAAMe,EAC3B,KAAK,MAAM,IAAIhB,EAAKC,CAAK,EACzB,KAAK,aAAeA,EAAM,KAE5Ba,EAAO,CACT,EACAD,EAAQ,QAAU,IAAME,EAAOF,EAAQ,KAAK,CAC9C,CAAC,CACH,MAAQ,CAER,CACF,CAKQ,MAAM,eAAa,CACzB,GAAI,SAAO,UAAc,KAEzB,GAAI,CAEF,IAAMI,GADK,MAAM,KAAK,OAAM,GACd,YAAY,QAAS,WAAW,EACxCC,EAAQD,EAAG,YAAY,OAAO,EAGpCC,EAAM,MAAK,EAGX,OAAW,CAAClB,EAAKC,CAAK,IAAK,KAAK,MAC9BiB,EAAM,IAAI,CAAE,IAAAlB,EAAK,MAAAC,CAAK,CAAE,EAG1B,OAAO,IAAI,QAAQ,CAACa,EAASC,IAAU,CACrCE,EAAG,WAAa,IAAMH,EAAO,EAC7BG,EAAG,QAAU,IAAMF,EAAOE,EAAG,KAAK,CACpC,CAAC,CACH,MAAQ,CAER,CACF,CAKQ,MAAM,cAAY,CACxB,GAAI,SAAO,UAAc,KAEzB,GAAI,EACS,MAAM,KAAK,OAAM,GACd,YAAY,QAAS,WAAW,EAC7B,YAAY,OAAO,EAC9B,MAAK,CACb,MAAQ,CAER,CACF,CAKQ,QAAM,CACZ,OAAO,IAAI,QAAQ,CAACH,EAASC,IAAU,CACrC,IAAMF,EAAU,UAAU,KAAK,KAAK,QAAQ,KAAM,CAAC,EAEnDA,EAAQ,gBAAkB,IAAK,CAC7B,IAAMM,EAAKN,EAAQ,OACdM,EAAG,iBAAiB,SAAS,OAAO,GACvCA,EAAG,kBAAkB,QAAS,CAAE,QAAS,KAAK,CAAE,CAEpD,EAEAN,EAAQ,UAAY,IAAMC,EAAQD,EAAQ,MAAM,EAChDA,EAAQ,QAAU,IAAME,EAAOF,EAAQ,KAAK,CAC9C,CAAC,CACH,GAUWO,GAAP,cAA8BvB,EAAmB,CAIrD,YAAYwB,EAAiBC,EAA8B,CAEzD,IAAMC,EAAa,MAAM,QAAQD,CAAK,EAAIA,EAAQ,MAAM,KAAKA,CAAK,EAC5DE,EAAO,KAAK,UAAUD,CAAU,EACtC,MAAO,GAAGF,CAAO,IAAIG,CAAI,EAC3B,CAKQ,UAAUC,EAAa,CAC7B,IAAID,EAAO,EACLE,EAASD,EAAI,OAAS,IACxBA,EAAI,OAAO,CAACE,EAAGC,IAAMA,EAAI,KAAK,MAAMH,EAAI,OAAS,GAAG,IAAM,CAAC,EAC3DA,EAEJ,QAASG,EAAI,EAAGA,EAAIF,EAAO,OAAQE,IAAK,CACtC,IAAM1B,EAAQwB,EAAOE,CAAC,GAAK,EAC3BJ,GAASA,GAAQ,GAAKA,GAAStB,EAAQ,IAAO,GAC9CsB,GAAQ,CACV,CAEA,OAAOA,EAAK,SAAS,EAAE,CACzB,GAUWK,EAAP,KAAyB,CAI7B,YAAYC,EAAoB,kBAAiB,CAHhC/B,EAAA,kBACTA,EAAA,aAAiC,MAGvC,KAAK,UAAY+B,CACnB,CAKQ,MAAM,aAAW,CACvB,GAAI,CAAC,KAAK,MAAO,CACf,GAAI,OAAO,OAAW,IACpB,MAAM,IAAI,MAAM,4BAA4B,EAE9C,KAAK,MAAQ,MAAM,OAAO,KAAK,KAAK,SAAS,CAC/C,CACA,OAAO,KAAK,KACd,CAKA,MAAM,IAAIC,EAAW,CACnB,GAAI,CAEF,OAAO,MADO,MAAM,KAAK,YAAW,GACjB,MAAMA,CAAG,GAAK,MACnC,MAAQ,CACN,MACF,CACF,CAKA,MAAM,IAAIA,EAAaC,EAAkB,CACvC,GAAI,CAEF,MADc,MAAM,KAAK,YAAW,GACxB,IAAID,EAAKC,EAAS,MAAK,CAAE,CACvC,MAAQ,CAER,CACF,CAKA,MAAM,OAAOD,EAAW,CACtB,GAAI,CAEF,OAAO,MADO,MAAM,KAAK,YAAW,GACjB,OAAOA,CAAG,CAC/B,MAAQ,CACN,MAAO,EACT,CACF,CAKA,MAAM,OAAK,CACT,GAAI,CACF,MAAM,OAAO,OAAO,KAAK,SAAS,EAClC,KAAK,MAAQ,IACf,MAAQ,CAER,CACF,CAKA,MAAM,MAAI,CACR,GAAI,CAGF,OADiB,MADH,MAAM,KAAK,YAAW,GACP,KAAI,GACjB,IAAIE,GAAKA,EAAE,GAAG,CAChC,MAAQ,CACN,MAAO,CAAA,CACT,CACF,GAUI,SAAUC,GACdC,EAAkD,SAClDrC,EAAwB,CAAA,EAAE,CAE1B,IAAMsC,EAAwC,CAC5C,MAAO,CACL,QAAS,SACT,WAAY,KAEd,OAAQ,CACN,QAAS,UACT,WAAY,KAEd,MAAO,CACL,QAAS,SACT,WAAY,KAEd,OAAQ,CAAA,GAGV,OAAO,IAAIvC,GAAS,CAAE,GAAGuC,EAAQD,CAAM,EAAG,GAAGrC,CAAO,CAAE,CACxD,CC1eM,IAAgBuC,EAAhB,KAA4B,CAOhC,YAAYC,EAAsB,CANxBC,EAAA,aAA4B,MACnBA,EAAA,eACAA,EAAA,mBACAA,EAAA,sBACTA,EAAA,eAAU,IAGlB,KAAK,OAASD,EACd,KAAK,WAAa,IAAIE,GACtB,KAAK,cAAgB,IAAIC,CAC3B,CAKA,MAAM,YAAU,CACd,GAAI,KAAK,SAAW,KAAK,MAAO,OAGhC,IAAMC,EAAc,KAAK,WAAW,IAAI,KAAK,OAAO,KAAK,EACzD,GAAIA,EAAa,CACf,KAAK,MAAQA,EACb,KAAK,QAAU,GACf,MACF,CAGA,KAAK,MAAQ,MAAM,KAAK,mBAAmB,KAAK,OAAO,KAAK,EAC5D,KAAK,QAAU,EACjB,CAKU,MAAM,mBAAmBC,EAAiB,CAElD,IAAMC,EAAiB,MAAM,KAAK,cAAc,IAAID,CAAS,EAM7D,GAAI,CACF,IAAME,EAAW,MAAM,MAAMF,CAAS,EAClCE,EAAS,IAEX,MAAM,KAAK,cAAc,IAAIF,EAAWE,EAAS,MAAK,CAAE,CAE5D,MAAQ,CAER,CAGA,OAAOC,GAAUH,EAAW,CAC1B,QAAS,KAAK,OAAO,QACrB,aAAc,KAAK,OAAO,aAC1B,MAAO,KAAK,OAAO,MACpB,CACH,CAKA,MAAM,IAAII,EAAeC,EAAyB,CAChD,MAAM,KAAK,WAAU,EAErB,IAAMC,EAAY,YAAY,IAAG,EAG3BC,EAAe,MAAM,KAAK,WAAWH,CAAK,EAG1CI,EAAU,MAAMC,GAAa,KAAK,MAAQF,CAAY,EAGtDG,EAAS,MAAM,KAAK,YAAYF,EAA6BH,CAAO,EAE1E,OAAIK,GAAU,OAAOA,GAAW,UAAY,mBAAoBA,IAC7DA,EAA0B,eAAiB,YAAY,IAAG,EAAKJ,GAG3DI,CACT,CAKA,MAAM,SAASC,EAAkBN,EAAyB,CACxD,aAAM,KAAK,WAAU,EAGL,MAAM,QAAQ,IAC5BM,EAAO,IAAIP,GAAS,KAAK,IAAIA,EAAOC,CAAO,CAAC,CAAC,CAIjD,CAkBA,IAAI,MAAI,CACN,OAAO,KAAK,OAAO,IACrB,CAKA,IAAI,OAAK,CACP,OAAO,KAAK,OACd,CAKA,SAAO,CACD,KAAK,QACP,KAAK,MAAM,QAAO,EAClB,KAAK,MAAQ,MAEf,KAAK,QAAU,EACjB,GAgBIO,GAAwD,IAAI,IAK5D,SAAUC,EAAiBC,EAAoBC,EAAwB,CAC3EH,GAAkB,IAAIE,EAAMC,CAAO,CACrC,CAKM,SAAUC,GAAmBF,EAAkB,CACnD,OAAOF,GAAkB,IAAIE,CAAI,CACnC,CASO,IAAMG,GAAmB,CAAC,WAAY,UAAU,EAK1CC,GAAiB,CAC5B,QAAS,UAAW,OAAQ,MAAO,UAAW,WAAY,WAM/CC,GAAkB,CAC7B,QAAS,WAAY,oBAAqB,cAAe,aACzD,eAAgB,WAAY,OAAQ,MAAO,WCjKvC,IAAOC,EAAP,MAAOC,CAAS,CAgCpB,aAAA,CA/BQC,EAAA,aAA6B,IAAI,KACjCA,EAAA,oBAAoC,IAAI,KACxCA,EAAA,cAA8B,IAAI,KAClCA,EAAA,mBAAmC,IAAI,KACvCA,EAAA,qBAA6B,IAAI,KAEjCA,EAAA,iBAA4B,OAC5BA,EAAA,gBAAmB,SACnBA,EAAA,+BAAkC,MAGlCA,EAAA,kBAAqB,GACrBA,EAAA,kBAAqB,GACrBA,EAAA,mBACAA,EAAA,mBACAA,EAAA,oBACAA,EAAA,mBACAA,EAAA,mBAGAA,EAAA,iBAAoB,KACpBA,EAAA,mBAAuB,IACvBA,EAAA,oBAAwB,IAGxBA,EAAA,sBAGAA,EAAA,mBAAmC,IAAI,KACvCA,EAAA,mBAAmC,IAAI,KAG7C,KAAK,gBAAe,CACtB,CAKQ,iBAAe,CACrB,IAAMC,EAAkB,CAAA,EAGxB,QAASC,EAAI,GAAIA,GAAK,IAAKA,IAAKD,EAAM,KAAKC,CAAC,EAC5C,QAASA,EAAI,IAAKA,GAAK,IAAKA,IAAKD,EAAM,KAAKC,CAAC,EAC7C,QAASA,EAAI,IAAKA,GAAK,IAAKA,IAAKD,EAAM,KAAKC,CAAC,EAE7C,IAAMC,EAAQ,CAAC,GAAGF,CAAK,EACnBG,EAAI,EAER,QAASF,EAAI,EAAGA,EAAI,IAAKA,IAClBD,EAAM,SAASC,CAAC,IACnBD,EAAM,KAAKC,CAAC,EACZC,EAAM,KAAK,IAAMC,CAAC,EAClBA,KAIJ,QAASF,EAAI,EAAGA,EAAID,EAAM,OAAQC,IAAK,CACrC,IAAMG,EAAOJ,EAAMC,CAAC,EACdI,EAAO,OAAO,aAAaH,EAAMD,CAAC,CAAE,EAC1C,KAAK,YAAY,IAAIG,EAAMC,CAAI,EAC/B,KAAK,YAAY,IAAIA,EAAMD,CAAI,CACjC,CACF,CAKA,aAAa,SAASE,EAA8B,CAClD,IAAMC,EAAY,IAAIT,EAChBU,EAAO,OAAOF,GAAS,SAAW,KAAK,MAAMA,CAAI,EAAuBA,EAG9E,GAAIE,EAAK,MAAO,CAId,GAHAD,EAAU,UAAYC,EAAK,MAAM,KAG7BA,EAAK,MAAM,MACb,OAAW,CAACC,EAAOC,CAAE,IAAK,OAAO,QAAQF,EAAK,MAAM,KAAK,EACvDD,EAAU,MAAM,IAAIE,EAAOC,CAAE,EAC7BH,EAAU,aAAa,IAAIG,EAAID,CAAK,EAKxC,GAAID,EAAK,MAAM,OACb,QAASP,EAAI,EAAGA,EAAIO,EAAK,MAAM,OAAO,OAAQP,IAC5CM,EAAU,OAAO,IAAIC,EAAK,MAAM,OAAOP,CAAC,EAAIA,CAAC,EAKjDM,EAAU,SAAWC,EAAK,MAAM,WAAa,QAC7CD,EAAU,wBAA0BC,EAAK,MAAM,2BAA6B,IAC9E,CAGA,GAAIA,EAAK,aACP,QAAWC,KAASD,EAAK,aAAc,CACrCD,EAAU,YAAY,IAAIE,EAAM,QAASA,EAAM,EAAE,EACjDF,EAAU,aAAa,IAAIE,EAAM,GAAIA,EAAM,OAAO,EAC9CA,EAAM,SACRF,EAAU,cAAc,IAAIE,EAAM,OAAO,EAI3C,IAAME,EAAUF,EAAM,QAAQ,YAAW,EACrCE,EAAQ,SAAS,KAAK,IAAGJ,EAAU,WAAaE,EAAM,IACtDE,EAAQ,SAAS,KAAK,IAAGJ,EAAU,WAAaE,EAAM,KACtDE,EAAQ,SAAS,KAAK,GAAKA,IAAY,WAASJ,EAAU,WAAaE,EAAM,KAC7EE,EAAQ,SAAS,KAAK,GAAKA,IAAY,WAASJ,EAAU,WAAaE,EAAM,IAC7EE,EAAQ,SAAS,MAAM,IAAGJ,EAAU,YAAcE,EAAM,KACxDE,EAAQ,SAAS,KAAK,GAAKA,IAAY,SAAOJ,EAAU,WAAaE,EAAM,KAC3EE,EAAQ,SAAS,KAAK,GAAKA,IAAY,UAAQJ,EAAU,WAAaE,EAAM,GAClF,CAIF,OAAID,EAAK,aACPD,EAAU,YAAcC,EAAK,WAAW,WAAa,GACrDD,EAAU,aAAeC,EAAK,WAAW,eAAiB,IAIxDA,EAAK,aACPD,EAAU,UAAYC,EAAK,WAAW,YAIpCA,EAAK,iBACPD,EAAU,cAAgBC,EAAK,gBAG1BD,CACT,CAKA,aAAa,QAAQK,EAAW,CAC9B,IAAMC,EAAW,MAAM,MAAMD,CAAG,EAChC,GAAI,CAACC,EAAS,GACZ,MAAM,IAAIC,EACR,iCAAiCF,CAAG,KAAKC,EAAS,MAAM,GACxDE,EAAW,eAAe,EAG9B,IAAMT,EAAO,MAAMO,EAAS,KAAI,EAChC,OAAOf,EAAU,SAASQ,CAAI,CAChC,CAKA,aAAa,gBAAgBU,EAAiBC,EAA+B,CAC3E,IAAMC,EAAWD,GAAS,UAAY,OAChCL,EAAM,0BAA0BI,CAAO,YAAYE,CAAQ,kBACjE,OAAOpB,EAAU,QAAQc,CAAG,CAC9B,CAKQ,UAAUO,EAAY,CAC5B,IAAIC,EAASD,EAEb,OAAI,KAAK,cACPC,EAASA,EAAO,YAAW,GAGzB,KAAK,eACPA,EAASA,EAAO,UAAU,KAAK,EAAE,QAAQ,mBAAoB,EAAE,GAIjEA,EAASA,EAAO,QAAQ,OAAQ,GAAG,EAAE,KAAI,EAElCA,CACT,CAKQ,YAAYD,EAAY,CAE9B,IAAME,EAAU,+EAEhB,OADgBF,EAAK,MAAME,CAAO,GAChB,CAACF,CAAI,CACzB,CAKQ,YAAYA,EAAY,CAE9B,IAAMnB,EADU,IAAI,YAAW,EACT,OAAOmB,CAAI,EACjC,OAAO,MAAM,KAAKnB,CAAK,EAAE,IAAIsB,GAAK,KAAK,YAAY,IAAIA,CAAC,GAAK,EAAE,EAAE,KAAK,EAAE,CAC1E,CAKQ,YAAYH,EAAY,CAC9B,IAAMnB,EAAQ,IAAI,WAChBmB,EAAK,MAAM,EAAE,EAAE,IAAII,GAAK,KAAK,YAAY,IAAIA,CAAC,GAAK,CAAC,CAAC,EAGvD,OADgB,IAAI,YAAY,QAAS,CAAE,MAAO,EAAK,CAAE,EAC1C,OAAOvB,CAAK,CAC7B,CAKQ,SAASwB,EAAc,CAC7B,IAAMC,EAAQ,IAAI,IAClB,QAASxB,EAAI,EAAGA,EAAIuB,EAAK,OAAS,EAAGvB,IACnCwB,EAAM,IAAI,GAAGD,EAAKvB,CAAC,CAAC,IAAIuB,EAAKvB,EAAI,CAAC,CAAC,EAAE,EAEvC,OAAOwB,CACT,CAKQ,IAAIhB,EAAa,CACvB,GAAI,KAAK,MAAM,IAAIA,CAAK,EACtB,MAAO,CAACA,CAAK,EAGf,IAAIe,EAAOf,EAAM,MAAM,EAAE,EACrBgB,EAAQ,KAAK,SAASD,CAAI,EAE9B,GAAIC,EAAM,OAAS,EACjB,MAAO,CAAChB,CAAK,EAGf,OAAa,CAEX,IAAIiB,EAAyB,KACzBC,EAAU,IAEd,QAAWC,KAAQH,EAAO,CACxB,IAAMI,EAAO,KAAK,OAAO,IAAID,CAAI,EAC7BC,IAAS,QAAaA,EAAOF,IAC/BA,EAAUE,EACVH,EAAUE,EAEd,CAEA,GAAIF,IAAY,KAAM,MAEtB,IAAMI,EAAQJ,EAAQ,MAAM,GAAG,EACzBK,EAAQD,EAAM,CAAC,EACfE,EAASF,EAAM,CAAC,EACtB,GAAI,CAACC,GAAS,CAACC,EAAQ,MAEvB,IAAMC,EAAoB,CAAA,EACtBhC,EAAI,EAER,KAAOA,EAAIuB,EAAK,QAAQ,CACtB,IAAMU,EAAIV,EAAK,QAAQO,EAAO9B,CAAC,EAC/B,GAAIiC,IAAM,GAAI,CACZD,EAAQ,KAAK,GAAGT,EAAK,MAAMvB,CAAC,CAAC,EAC7B,KACF,CAEAgC,EAAQ,KAAK,GAAGT,EAAK,MAAMvB,EAAGiC,CAAC,CAAC,EAE5BV,EAAKU,CAAC,IAAMH,GAASG,EAAIV,EAAK,OAAS,GAAKA,EAAKU,EAAI,CAAC,IAAMF,GAC9DC,EAAQ,KAAKF,EAAQC,CAAM,EAC3B/B,EAAIiC,EAAI,IAERD,EAAQ,KAAKT,EAAKU,CAAC,CAAE,EACrBjC,EAAIiC,EAAI,EAEZ,CAIA,GAFAV,EAAOS,EAEHT,EAAK,SAAW,EAAG,MAEvBC,EAAQ,KAAK,SAASD,CAAI,CAC5B,CAEA,OAAOA,CACT,CAKQ,UAAUA,EAAY,CAC5B,GAAI,KAAK,MAAM,IAAIA,CAAI,EACrB,MAAO,CAACA,CAAI,EAGd,IAAMW,EAAmB,CAAA,EACrBC,EAAQ,EAEZ,KAAOA,EAAQZ,EAAK,QAAQ,CAC1B,IAAIa,EAAMb,EAAK,OACXc,EAA2B,KAE/B,KAAOF,EAAQC,GAAK,CAClB,IAAIE,EAASf,EAAK,MAAMY,EAAOC,CAAG,EAKlC,GAJID,EAAQ,IACVG,EAAS,KAAK,wBAA0BA,GAGtC,KAAK,MAAM,IAAIA,CAAM,EAAG,CAC1BD,EAAYC,EACZ,KACF,CACAF,GACF,CAEIC,IAAc,MAChBH,EAAO,KAAK,KAAK,QAAQ,EACzBC,MAEAD,EAAO,KAAKG,CAAS,EACrBF,EAAQC,EAEZ,CAEA,OAAOF,CACT,CAKQ,aAAaX,EAAY,CAE/B,GAAI,KAAK,YAAY,IAAIA,CAAI,EAC3B,MAAO,CAACA,CAAI,EAGd,OAAQ,KAAK,UAAW,CACtB,IAAK,MAAO,CAEV,IAAMgB,EAAU,KAAK,YAAYhB,CAAI,EACrC,OAAO,KAAK,IAAIgB,CAAO,CACzB,CACA,IAAK,YACH,OAAO,KAAK,UAAUhB,CAAI,EAC5B,QACE,OAAO,KAAK,MAAM,IAAIA,CAAI,EAAI,CAACA,CAAI,EAAI,CAAC,KAAK,QAAQ,CACzD,CACF,CAKQ,SAASL,EAAY,CAE3B,IAAMsB,EAAa,KAAK,UAAUtB,CAAI,EAGhCgB,EAAmB,CAAA,EACrBO,EAAYD,EAGVE,EAAoB,MAAM,KAAK,KAAK,YAAY,KAAI,CAAE,EACzD,KAAK,CAACC,EAAGtB,IAAMA,EAAE,OAASsB,EAAE,MAAM,EAGrC,QAAWC,KAAcF,EACvB,GAAID,EAAU,SAASG,CAAU,EAAG,CAClC,IAAMf,EAAQY,EAAU,MAAMG,CAAU,EAClCC,EAAyB,CAAA,EAE/B,QAAS7C,EAAI,EAAGA,EAAI6B,EAAM,OAAQ7B,IAC5B6B,EAAM7B,CAAC,GACT6C,EAAa,KAAKhB,EAAM7B,CAAC,CAAE,EAEzBA,EAAI6B,EAAM,OAAS,GACrBK,EAAO,KAAKU,CAAU,EAI1BH,EAAYI,EAAa,KAAK,GAAG,CACnC,CAIF,GAAIJ,EAAU,KAAI,EAAI,CACpB,IAAMK,EAAQ,KAAK,YAAYL,CAAS,EAExC,QAAWlB,KAAQuB,EAAO,CACxB,GAAI,CAACvB,EAAM,SACX,IAAMwB,EAAa,KAAK,aAAaxB,CAAI,EACzCW,EAAO,KAAK,GAAGa,CAAU,CAC3B,CACF,CAEA,OAAOb,CACT,CAKQ,mBAAmBA,EAAgB,CACzC,OAAOA,EAAO,IAAI1B,GAAQ,CAExB,IAAMwC,EAAU,KAAK,YAAY,IAAIxC,CAAK,EAC1C,GAAIwC,IAAY,OAAW,OAAOA,EAGlC,IAAMC,EAAU,KAAK,MAAM,IAAIzC,CAAK,EACpC,OAAIyC,IAAY,OAAkBA,EAG3B,KAAK,UACd,CAAC,CACH,CAKQ,mBAAmBC,EAAa,CACtC,OAAOA,EAAI,IAAIzC,GAAM,KAAK,aAAa,IAAIA,CAAE,GAAK,KAAK,QAAQ,CACjE,CAKQ,YACNyC,EACAC,EAAkB,CAElB,GAAI,CAAC,KAAK,cAAe,CAEvB,IAAMhC,EAAmB,CAAA,EACnBiC,EAAoB,CAAA,EAE1B,OAAI,KAAK,aAAe,SACtBjC,EAAO,KAAK,KAAK,UAAU,EAC3BiC,EAAQ,KAAK,CAAC,GAGhBjC,EAAO,KAAK,GAAG+B,CAAG,EAClBE,EAAQ,KAAK,GAAGF,EAAI,IAAI,IAAM,CAAC,CAAC,EAE5B,KAAK,aAAe,SACtB/B,EAAO,KAAK,KAAK,UAAU,EAC3BiC,EAAQ,KAAK,CAAC,GAGZD,IACFhC,EAAO,KAAK,GAAGgC,CAAO,EACtBC,EAAQ,KAAK,GAAGD,EAAQ,IAAI,IAAM,CAAC,CAAC,EAEhC,KAAK,aAAe,SACtBhC,EAAO,KAAK,KAAK,UAAU,EAC3BiC,EAAQ,KAAK,CAAC,IAIX,CAAE,IAAKjC,EAAQ,QAAAiC,CAAO,CAC/B,CAGA,IAAMC,EAAWF,EAAU,KAAK,cAAc,KAAO,KAAK,cAAc,OACxE,GAAI,CAACE,EACH,MAAO,CAAE,IAAAH,EAAK,QAASA,EAAI,IAAI,IAAM,CAAC,CAAC,EAGzC,IAAM/B,EAAmB,CAAA,EACnBiC,EAAoB,CAAA,EAE1B,QAAWE,KAAQD,EACjB,GAAI,iBAAkBC,EAAM,CAC1B,IAAMC,EAAe,KAAK,cAAc,iBAAiBD,EAAK,aAAa,EAAE,EACzEC,IACFpC,EAAO,KAAK,GAAGoC,EAAa,GAAG,EAC/BH,EAAQ,KAAK,GAAGG,EAAa,IAAI,IAAI,IAAMD,EAAK,aAAa,OAAO,CAAC,EAEzE,SAAW,aAAcA,EAAM,CAC7B,IAAME,EAASF,EAAK,SAAS,KAAO,IAAMJ,EAAMC,GAAW,CAAA,EAC3DhC,EAAO,KAAK,GAAGqC,CAAM,EACrBJ,EAAQ,KAAK,GAAGI,EAAO,IAAI,IAAMF,EAAK,SAAS,OAAO,CAAC,CACzD,CAGF,MAAO,CAAE,IAAKnC,EAAQ,QAAAiC,CAAO,CAC/B,CAKA,OAAOlC,EAAcF,EAA4B,CAAA,EAAE,CACjD,GAAM,CACJ,iBAAAyC,EAAmB,GACnB,UAAAC,EAAY,KAAK,UACjB,QAAAC,EAAU,aACV,WAAAC,EAAa,GACb,oBAAAC,EAAsB,GACtB,mBAAAC,EAAqB,GACrB,SAAAC,CAAQ,EACN/C,EAGEkB,EAAS,KAAK,SAAShB,CAAI,EAC7B8C,EAAW,KAAK,mBAAmB9B,CAAM,EAGzCiB,EACJ,GAAIY,EAAU,CACZ,IAAME,EAAa,KAAK,SAASF,CAAQ,EACzCZ,EAAU,KAAK,mBAAmBc,CAAU,CAC9C,CAGA,IAAIC,EACJ,GAAIT,EAAkB,CACpB,IAAMU,EAAY,KAAK,YAAYH,EAAUb,CAAO,EACpDa,EAAWG,EAAU,IACjBL,IACFI,EAAeC,EAAU,QAE7B,MAAWhB,IACTa,EAAW,CAAC,GAAGA,EAAU,GAAGb,CAAO,EAC/BW,IACFI,EAAe,CAAC,GAAGF,EAAS,IAAI,IAAM,CAAC,EAAG,GAAGb,EAAQ,IAAI,IAAM,CAAC,CAAC,IAKjES,GAAcI,EAAS,OAASN,IAClCM,EAAWA,EAAS,MAAM,EAAGN,CAAS,EAClCQ,IACFA,EAAeA,EAAa,MAAM,EAAGR,CAAS,IAKlD,IAAIU,EAA0B,CAAA,EAM9B,GALIP,IACFO,EAAgBJ,EAAS,IAAI,IAAM,CAAC,GAIlCL,IAAY,cAAgBK,EAAS,OAASN,EAAW,CAC3D,IAAMW,EAAYX,EAAYM,EAAS,OACvCA,EAAW,CAAC,GAAGA,EAAU,GAAG,IAAI,MAAMK,CAAS,EAAE,KAAK,KAAK,UAAU,CAAa,EAC9ER,IACFO,EAAgB,CAAC,GAAGA,EAAe,GAAG,IAAI,MAAMC,CAAS,EAAE,KAAK,CAAC,CAAa,GAE5EH,IACFA,EAAe,CAAC,GAAGA,EAAc,GAAG,IAAI,MAAMG,CAAS,EAAE,KAAK,CAAC,CAAa,EAEhF,CAEA,IAAMlD,EAA0B,CAC9B,SAAA6C,EACA,cAAAI,GAGF,OAAIN,GAAsBI,IACxB/C,EAAO,aAAe+C,GAGjB/C,CACT,CAKA,YAAYmD,EAAiBtD,EAA4B,CAAA,EAAE,CAEzD,GAAIA,EAAQ,UAAY,UAAW,CACjC,IAAMuD,EAAYD,EAAM,IAAIE,GAAK,KAAK,OAAOA,EAAG,CAAE,GAAGxD,EAAS,QAAS,YAAY,CAAE,CAAC,EAChFyD,EAAS,KAAK,IAAI,GAAGF,EAAU,IAAIG,GAAKA,EAAE,SAAS,MAAM,CAAC,EAChE,OAAOJ,EAAM,IAAIE,GAAK,KAAK,OAAOA,EAAG,CAAE,GAAGxD,EAAS,UAAWyD,EAAQ,QAAS,YAAY,CAAE,CAAC,CAChG,CAEA,OAAOH,EAAM,IAAIE,GAAK,KAAK,OAAOA,EAAGxD,CAAO,CAAC,CAC/C,CAKA,OAAOkC,EAAeyB,EAAoB,GAAI,CAC5C,IAAIzC,EAAS,KAAK,mBAAmBgB,CAAG,EAGpCyB,IACFzC,EAASA,EAAO,OAAOsC,GAAK,CAAC,KAAK,cAAc,IAAIA,CAAC,CAAC,GAIxD,IAAItD,EAAOgB,EAAO,KAAK,EAAE,EAGzB,OAAI,KAAK,YAAc,QACrBhB,EAAO,KAAK,YAAYA,CAAI,GAI1B,KAAK,YAAc,cACrBA,EAAOA,EAAK,QAAQ,IAAI,OAAO,KAAK,wBAAyB,GAAG,EAAG,EAAE,GAIvEA,EAAOA,EAAK,QAAQ,OAAQ,GAAG,EAAE,KAAI,EAE9BA,CACT,CAKA,YAAY0D,EAAsBD,EAAoB,GAAI,CACxD,OAAOC,EAAS,IAAI1B,GAAO,KAAK,OAAOA,EAAKyB,CAAiB,CAAC,CAChE,CAKA,IAAI,WAAS,CACX,OAAO,KAAK,MAAM,KAAO,KAAK,YAAY,IAC5C,CAKA,oBAAkB,CAShB,MAAO,CACL,WAAY,KAAK,WACjB,WAAY,KAAK,WACjB,WAAY,KAAK,WACjB,WAAY,KAAK,WACjB,YAAa,KAAK,YAClB,WAAY,KAAK,WACjB,WAAY,KAAK,WAErB,CAKA,WAAS,CACP,MAAO,CACL,UAAW,KAAK,UAChB,UAAW,KAAK,UAChB,WAAY,KAAK,WACjB,WAAY,KAAK,WACjB,WAAY,KAAK,WACjB,WAAY,KAAK,WACjB,YAAa,KAAK,YAClB,WAAY,KAAK,WACjB,WAAY,KAAK,WAErB,CAKA,eAAenE,EAAa,CAC1B,OAAO,KAAK,cAAc,IAAIA,CAAK,CACrC,CAKA,WAAWA,EAAa,CACtB,OAAO,KAAK,YAAY,IAAIA,CAAK,GAAK,KAAK,MAAM,IAAIA,CAAK,CAC5D,CAKA,SAASC,EAAU,CACjB,OAAO,KAAK,aAAa,IAAIA,CAAE,CACjC,GAUI,SAAUoE,IAAoB,CAElC,OADkB,IAAIjF,CAExB,CAKA,eAAsBkF,GAAcnE,EAAW,CAC7C,OAAOf,EAAU,QAAQe,CAAG,CAC9B,CAKA,eAAsBoE,GACpBhE,EACAC,EAA+B,CAE/B,OAAOpB,EAAU,gBAAgBmB,EAASC,CAAO,CACnD,CChwBM,IAAOgE,EAAP,cAA0CC,CAG/C,CAIC,YAAYC,EAAwBC,EAAiB,CACnD,MAAMD,CAAM,EAJNE,EAAA,iBAA8B,MAC9BA,EAAA,eAIN,KAAK,OAASD,GAAUE,EAC1B,CAKS,MAAM,YAAU,CACvB,MAAM,MAAM,WAAU,EAGjB,KAAK,YACR,KAAK,UAAYC,GAAoB,EAEzC,CAKA,UAAUH,EAAgB,CACxB,KAAK,OAASA,CAChB,CAKS,MAAM,IACbI,EACAC,EAAmC,CAEnC,IAAMC,EAAU,MAAM,QAAQF,CAAK,EAC7BG,EAASD,EAAUF,EAAQ,CAACA,CAAK,EAEvC,MAAM,KAAK,WAAU,EAErB,IAAMI,EAAY,YAAY,IAAG,EAC3BC,EAAsC,CAAA,EAE5C,QAAWC,KAAQH,EAAQ,CAEzB,IAAMI,EAAe,MAAM,KAAK,WAAWD,CAAI,EAGzCE,EAAU,MAAM,KAAK,aAAaD,CAAY,EAG9CE,EAAS,MAAM,KAAK,YAAYD,EAASP,CAAO,EACtDI,EAAQ,KAAKI,CAAM,CACrB,CAEA,IAAMC,EAAiB,YAAY,IAAG,EAAKN,EAG3C,QAAWK,KAAUJ,EACnBI,EAAO,eAAiBC,EAAiBL,EAAQ,OAGnD,OAAOH,EAAUG,EAAUA,EAAQ,CAAC,CACtC,CAKmB,MAAM,WAAWL,EAAwB,CAC1D,IAAMM,EAAO,MAAM,QAAQN,CAAK,EAAIA,EAAM,CAAC,EAAKA,EAG1CW,EAAU,KAAK,UAAW,OAAOL,EAAM,CAC3C,UAAW,IACX,QAAS,aACT,WAAY,GACb,EAGKM,EAAW,IAAIC,EACnB,IAAI,aAAaF,EAAQ,QAAQ,EACjC,CAAC,EAAGA,EAAQ,SAAS,MAAM,EAC3B,SAAS,EAGLG,EAAgB,IAAID,EACxB,IAAI,aAAaF,EAAQ,aAAa,EACtC,CAAC,EAAGA,EAAQ,cAAc,MAAM,EAChC,SAAS,EAGX,MAAO,CAACC,EAAUE,CAAa,CACjC,CAKQ,MAAM,aAAaX,EAAwB,CAGjD,IAAMY,EAAa,KAAK,OAAO,OACzBC,EAAS,IAAI,aAAaD,CAAU,EAIpCE,GADYd,EAAO,CAAC,GAAG,eAAc,GAAM,IAAI,aAAa,CAAC,GAC7C,OAAO,CAAC,EAAGe,IAAM,EAAIA,EAAG,CAAC,EAG/C,QAASC,EAAI,EAAGA,EAAIJ,EAAYI,IAC9BH,EAAOG,CAAC,EAAI,KAAK,IAAIF,GAAOE,EAAI,EAAE,EAAI,EAGxC,MAAO,CAAC,IAAIN,EAAeG,EAAQ,CAAC,EAAGD,CAAU,EAAG,SAAS,CAAC,CAChE,CAKmB,MAAM,YACvBP,EACAP,EAAmC,CAEnC,IAAMe,EAASR,EAAQ,CAAC,EACxB,GAAI,CAACQ,EACH,MAAO,CAAE,MAAO,UAAW,MAAO,CAAC,EAKrC,IAAMI,EADQC,EAAQL,EAAQ,EAAE,EACP,eAAc,EAGjCM,EAAOrB,GAAS,MAAQ,GACNA,GAAS,iBAAmB,KAE7BqB,EAAO,EAM9B,IAAIC,EAAS,EACTC,EAAWJ,EAAW,CAAC,GAAK,EAEhC,QAASD,EAAI,EAAGA,EAAIC,EAAW,OAAQD,KAChCC,EAAWD,CAAC,GAAK,GAAKK,IACzBA,EAAWJ,EAAWD,CAAC,GAAK,EAC5BI,EAASJ,GAMb,MAAO,CACL,MAHYlB,GAAS,SAASsB,CAAM,GAAK,KAAK,OAAOA,CAAM,GAAK,SAASA,CAAM,GAI/E,MAAOC,EAEX,GAUWC,EAAP,cAAyChC,CAA0B,CACvE,YAAYE,EAAsB,CAChC,MAAMA,EAAQG,EAAgB,CAChC,CAKA,MAAM,QACJQ,EACAL,EAAmC,CAEnC,OAAO,KAAK,IAAIK,EAAML,CAAO,CAC/B,GAUI,SAAUyB,GACd/B,EAAkC,CAAA,EAAE,CAEpC,OAAO,IAAIF,EAA2B,CACpC,KAAM,sBACN,MAAOE,EAAO,OAAS,UACvB,QAASA,EAAO,QAChB,MAAOA,EAAO,OAAS,GACvB,aAAcA,EAAO,aACtB,CACH,CAKM,SAAUgC,GACdhC,EAAkC,CAAA,EAAE,CAEpC,OAAO,IAAI8B,EAA0B,CACnC,KAAM,qBACN,MAAO9B,EAAO,OAAS,UACvB,QAASA,EAAO,QAChB,MAAOA,EAAO,OAAS,GACvB,aAAcA,EAAO,aACtB,CACH,CAGAiC,EAAiB,sBAAwBjC,GAAW,IAAIF,EAA2BE,CAAM,CAAC,EAC1FiC,EAAiB,qBAAuBjC,GAAW,IAAI8B,EAA0B9B,CAAM,CAAC,EChOlF,IAAOkC,EAAP,cAAyCC,CAG9C,CAIC,YAAYC,EAAwBC,EAAuB,IAAG,CAC5D,MAAMD,CAAM,EAJNE,EAAA,iBAA8B,MAC9BA,EAAA,qBAIN,KAAK,aAAeD,CACtB,CAKS,MAAM,YAAU,CACvB,MAAM,MAAM,WAAU,EAEjB,KAAK,YACR,KAAK,UAAYE,GAAoB,EAEzC,CAKS,MAAM,IACbC,EACAC,EAAkC,CAElC,IAAMC,EAAU,MAAM,QAAQF,CAAK,EAC7BG,EAASD,EAAUF,EAAQ,CAACA,CAAK,EAEvC,MAAM,KAAK,WAAU,EAErB,IAAMI,EAAY,YAAY,IAAG,EAC3BC,EAAqC,CAAA,EAE3C,QAAWC,KAAQH,EAAQ,CAEzB,IAAMI,EAAe,MAAM,KAAK,WAAWD,CAAI,EAGzCE,EAAU,MAAM,KAAK,aAAaD,CAAY,EAG9CE,EAAS,MAAM,KAAK,YAAYD,EAASP,CAAO,EACtDI,EAAQ,KAAKI,CAAM,CACrB,CAEA,IAAMC,EAAiB,YAAY,IAAG,EAAKN,EAE3C,QAAWK,KAAUJ,EACnBI,EAAO,eAAiBC,EAAiBL,EAAQ,OAGnD,OAAOH,EAAUG,EAAUA,EAAQ,CAAC,CACtC,CAKmB,MAAM,WAAWL,EAAwB,CAC1D,IAAMM,EAAO,MAAM,QAAQN,CAAK,EAAIA,EAAM,CAAC,EAAKA,EAE1CW,EAAU,KAAK,UAAW,OAAOL,EAAM,CAC3C,UAAW,IACX,QAAS,aACT,WAAY,GACb,EAEKM,EAAW,IAAIC,EACnB,IAAI,aAAaF,EAAQ,QAAQ,EACjC,CAAC,EAAGA,EAAQ,SAAS,MAAM,EAC3B,SAAS,EAGLG,EAAgB,IAAID,EACxB,IAAI,aAAaF,EAAQ,aAAa,EACtC,CAAC,EAAGA,EAAQ,cAAc,MAAM,EAChC,SAAS,EAGX,MAAO,CAACC,EAAUE,CAAa,CACjC,CAKQ,MAAM,aAAaX,EAAwB,CAGjD,IAAMY,EAASZ,EAAO,CAAC,GAAG,MAAM,CAAC,GAAK,IAChCa,EAAa,IAAI,aAAaD,EAAS,KAAK,YAAY,EAGxDE,EAAYd,EAAO,CAAC,GAAG,eAAc,GAAM,IAAI,aAAa,CAAC,EAEnE,QAAS,EAAI,EAAG,EAAIY,EAAQ,IAC1B,QAASG,EAAI,EAAGA,EAAI,KAAK,aAAcA,IAAK,CAC1C,IAAMC,EAAWF,EAAU,CAAC,GAAK,EACjCD,EAAW,EAAI,KAAK,aAAeE,CAAC,EAClC,KAAK,IAAIC,GAAYD,EAAI,GAAK,GAAI,EAAI,EAC1C,CAGF,MAAO,CAAC,IAAIL,EAAeG,EAAY,CAAC,EAAGD,EAAQ,KAAK,YAAY,EAAG,SAAS,CAAC,CACnF,CAKmB,MAAM,YACvBP,EACAP,EAAkC,CAElC,IAAMmB,EAAeZ,EAAQ,CAAC,EAC9B,GAAI,CAACY,EACH,MAAO,CAAE,WAAY,CAAA,CAAE,EAGzB,IAAMC,EAAUpB,GAAS,SAAW,OAC9BqB,EAAYrB,GAAS,WAAa,GAEpCe,EAEJ,OAAQK,EAAS,CACf,IAAK,MAEHL,EAAa,KAAK,oBAAoBI,CAAY,EAClD,MACF,IAAK,MAEHJ,EAAa,KAAK,WAAWI,CAAY,EACzC,MACF,IAAK,OAEHJ,EAAaI,EAAa,QAAO,EACjC,MACF,IAAK,OACL,QAEEJ,EAAa,KAAK,YAAYI,CAAY,EAC1C,KACJ,CAGA,OAAIE,IACFN,EAAa,KAAK,gBAAgBA,CAAU,GAI1Cf,GAAS,WAAaA,EAAQ,UAAYe,EAAW,SACvDA,EAAaA,EAAW,MAAM,EAAGf,EAAQ,SAAS,GAG7C,CAAE,WAAAe,CAAU,CACrB,CAKQ,oBAAoBI,EAA4B,CACtD,IAAMG,EAAOH,EAAa,eAAc,EAClCvB,EAAeuB,EAAa,MAAM,CAAC,GAAK,KAAK,aACnD,OAAO,MAAM,KAAKG,EAAK,MAAM,EAAG1B,CAAY,CAAC,CAC/C,CAKQ,YAAYuB,EAA4B,CAC9C,IAAMG,EAAOH,EAAa,eAAc,EAClCL,EAASK,EAAa,MAAM,CAAC,GAAK,EAClCvB,EAAeuB,EAAa,MAAM,CAAC,GAAK,KAAK,aAE7CX,EAAS,IAAI,aAAaZ,CAAY,EAE5C,QAAS2B,EAAI,EAAGA,EAAIT,EAAQS,IAC1B,QAASN,EAAI,EAAGA,EAAIrB,EAAcqB,IAChCT,EAAOS,CAAC,GAAKT,EAAOS,CAAC,GAAK,IAAMK,EAAKC,EAAI3B,EAAeqB,CAAC,GAAK,GAAKH,EAIvE,OAAO,MAAM,KAAKN,CAAM,CAC1B,CAKQ,WAAWW,EAA4B,CAC7C,IAAMG,EAAOH,EAAa,eAAc,EAClCL,EAASK,EAAa,MAAM,CAAC,GAAK,EAClCvB,EAAeuB,EAAa,MAAM,CAAC,GAAK,KAAK,aAE7CX,EAAS,IAAI,MAAMZ,CAAY,EAAE,KAAK,IAAS,EAErD,QAAS2B,EAAI,EAAGA,EAAIT,EAAQS,IAC1B,QAASN,EAAI,EAAGA,EAAIrB,EAAcqB,IAAK,CACrC,IAAMO,EAAMF,EAAKC,EAAI3B,EAAeqB,CAAC,GAAK,EACtCO,GAAOhB,EAAOS,CAAC,GAAK,QACtBT,EAAOS,CAAC,EAAIO,EAEhB,CAGF,OAAOhB,CACT,CAKQ,gBAAgBiB,EAAa,CACnC,IAAIC,EAAO,EACX,QAAWC,KAAKF,EACdC,GAAQC,EAAIA,EAId,OAFAD,EAAO,KAAK,KAAKA,CAAI,EAEjBA,IAAS,EAAUD,EAEhBA,EAAI,IAAIE,GAAKA,EAAID,CAAI,CAC9B,GAUI,SAAUE,GACdjC,EAAkC,CAAA,EAAE,CAEpC,OAAO,IAAIF,EAA0B,CACnC,KAAM,qBACN,MAAOE,EAAO,OAAS,UACvB,QAASA,EAAO,QAChB,MAAOA,EAAO,OAAS,GACvB,aAAcA,EAAO,aACtB,CACH,CAGAkC,EAAiB,qBAAuBlC,GAAW,IAAIF,EAA0BE,CAAM,CAAC,ECzMxF,IAAMmC,GAAkD,CACtD,MAAO,IACP,OAAQ,IACR,WAAY,QACZ,KAAM,CAAC,KAAO,KAAO,IAAK,EAC1B,IAAK,CAAC,KAAO,KAAO,IAAK,EACzB,cAAe,EAAI,IACnB,UAAW,GACX,cAAe,MACf,MAAO,UACP,SAAU,GACV,UAAW,GACX,YAAa,GACb,aAAc,GACd,aAAc,CAAC,EAAG,EAAG,CAAC,GAQXC,EAAP,MAAOC,CAAiB,CAK5B,YAAYC,EAAoC,CAAA,EAAE,CAJjCC,EAAA,gBACTA,EAAA,cAAmC,MACnCA,EAAA,WAAuC,MAI7C,IAAMC,EAAOF,EAAQ,KACfG,EAAQH,EAAQ,OAASE,GAAQL,GAAsB,MACvDO,EAASJ,EAAQ,QAAUE,GAAQL,GAAsB,OAE/D,KAAK,QAAU,CACb,GAAGA,GACH,GAAGG,EACH,MAAAG,EACA,OAAAC,EACA,KAAMF,GAAQC,EACd,SAAUH,EAAQ,UAAYA,EAAQ,MAAQG,EAElD,CAKA,OAAO,WAAWE,EAA+B,CAC/C,IAAML,EAAoC,CAAA,EAGpCE,EAAOG,EAAO,KACpB,GAAIH,IAAS,QACX,GAAI,OAAOA,GAAS,SAClBF,EAAQ,KAAOE,UACN,OAAOA,GAAS,UAAYA,IAAS,KAAM,CACpD,IAAMI,EAAUJ,EAChBF,EAAQ,MAAQM,EAAQ,OAASA,EAAQ,cACzCN,EAAQ,OAASM,EAAQ,QAAUA,EAAQ,aAC7C,EAGF,IAAMC,EAAWF,EAAO,UACxB,GAAIE,IAAa,QACf,GAAI,OAAOA,GAAa,SACtBP,EAAQ,SAAWO,UACV,OAAOA,GAAa,UAAYA,IAAa,KAAM,CAC5D,IAAMC,EAAUD,EAChBP,EAAQ,SAAW,CAAE,MAAOQ,EAAQ,OAAS,IAAK,OAAQA,EAAQ,QAAU,GAAG,CACjF,EAGF,IAAMC,EAAYJ,EAAO,WACrB,MAAM,QAAQI,CAAS,IACzBT,EAAQ,KAAOS,GAGjB,IAAMC,EAAWL,EAAO,UACpB,MAAM,QAAQK,CAAQ,IACxBV,EAAQ,IAAMU,GAGhB,IAAMC,EAAgBN,EAAO,eACzB,OAAOM,GAAkB,WAC3BX,EAAQ,cAAgBW,GAG1B,IAAMC,EAAWP,EAAO,UACpB,OAAOO,GAAa,YACtBZ,EAAQ,SAAWY,GAGrB,IAAMC,EAAYR,EAAO,WACrB,OAAOQ,GAAc,YACvBb,EAAQ,UAAYa,GAGtB,IAAMC,EAAcT,EAAO,aACvB,OAAOS,GAAgB,YACzBd,EAAQ,YAAcc,GAGxB,IAAMC,EAAeV,EAAO,eAC5B,OAAI,OAAOU,GAAiB,YAC1Bf,EAAQ,aAAee,GAGrBV,EAAO,WAAgB,SAEzBL,EAAQ,WAAa,SAGhB,IAAID,EAAkBC,CAAO,CACtC,CAKA,aAAa,QAAQgB,EAAW,CAC9B,IAAMC,EAAW,MAAM,MAAMD,CAAG,EAChC,GAAI,CAACC,EAAS,GACZ,MAAM,IAAI,MAAM,2CAA2CD,CAAG,EAAE,EAElE,IAAMX,EAAS,MAAMY,EAAS,KAAI,EAClC,OAAOlB,EAAkB,WAAWM,CAAM,CAC5C,CAKA,aAAa,gBACXa,EACAlB,EAA+B,CAE/B,IAAMmB,EAAWnB,GAAS,UAAY,OAChCgB,EAAM,0BAA0BE,CAAO,YAAYC,CAAQ,4BACjE,OAAOpB,EAAkB,QAAQiB,CAAG,CACtC,CAKQ,cAAY,CAClB,GAAI,CAAC,KAAK,OACR,GAAI,OAAO,SAAa,IACtB,KAAK,OAAS,SAAS,cAAc,QAAQ,EAC7C,KAAK,IAAM,KAAK,OAAO,WAAW,IAAI,MAEtC,OAAM,IAAI,MAAM,kDAAkD,CAGxE,CAKA,MAAM,QAAQI,EAAiB,CAC7B,IAAIC,EAEA,OAAOD,GAAU,SAEnBC,EAAY,MAAM,KAAK,YAAYD,CAAK,EAC/BA,aAAiB,MAAQA,aAAiB,KACnDC,EAAY,MAAM,KAAK,aAAaD,CAAK,EAChCA,aAAiB,UAC1BC,EAAYD,EAGZC,EAAY,KAAK,YAAYD,CAAK,EAIpC,IAAIE,EAAYD,EAGhB,OAAI,KAAK,QAAQ,WACfC,EAAY,KAAK,OAAOA,CAAS,GAI/B,KAAK,QAAQ,eACfA,EAAY,KAAK,WAAWA,CAAS,GAIhC,KAAK,SAASA,CAAS,CAChC,CAKA,MAAM,aAAaC,EAAoB,CACrC,IAAMC,EAAU,MAAM,QAAQ,IAAID,EAAO,IAAIH,GAAS,KAAK,QAAQA,CAAK,CAAC,CAAC,EAGpEK,EAAYD,EAAQ,OACpBE,EAAcF,EAAQ,CAAC,EAC7B,GAAI,CAACE,EACH,OAAO,IAAIC,EAAe,IAAI,aAAa,CAAC,EAAG,CAAC,CAAC,EAAG,SAAS,EAG/D,IAAMC,EAAWF,EAAY,MAAM,CAAC,GAAK,EACnCtB,EAASsB,EAAY,MAAM,CAAC,GAAK,KAAK,QAAQ,OAC9CvB,EAAQuB,EAAY,MAAM,CAAC,GAAK,KAAK,QAAQ,MAE7CG,EAAY,IAAI,aAAaJ,EAAYG,EAAWxB,EAASD,CAAK,EAExE,QAAS2B,EAAI,EAAGA,EAAIN,EAAQ,OAAQM,IAAK,CACvC,IAAMC,EAAIP,EAAQM,CAAC,EACfC,GACFF,EAAU,IAAIE,EAAE,eAAc,EAAID,EAAIF,EAAWxB,EAASD,CAAK,CAEnE,CAEA,OAAO,IAAIwB,EACTE,EACA,CAACJ,EAAWG,EAAUxB,EAAQD,CAAK,EACnC,SAAS,CAEb,CAKQ,MAAM,YAAYa,EAAW,CACnC,OAAO,IAAI,QAAQ,CAACgB,EAASC,IAAU,CACrC,IAAMC,EAAM,IAAI,MAChBA,EAAI,YAAc,YAElBA,EAAI,OAAS,IAAK,CAChBF,EAAQ,KAAK,YAAYE,CAAG,CAAC,CAC/B,EAEAA,EAAI,QAAU,IAAK,CACjBD,EAAO,IAAI,MAAM,6BAA6BjB,CAAG,EAAE,CAAC,CACtD,EAEAkB,EAAI,IAAMlB,CACZ,CAAC,CACH,CAKQ,MAAM,aAAamB,EAAU,CACnC,IAAMnB,EAAM,IAAI,gBAAgBmB,CAAI,EACpC,GAAI,CACF,OAAO,MAAM,KAAK,YAAYnB,CAAG,CACnC,SACE,IAAI,gBAAgBA,CAAG,CACzB,CACF,CAKQ,WAAWK,EAAoB,CACrC,IAAMd,EAAW,KAAK,QAAQ,SAC1B6B,EACAC,EAEA,OAAO9B,GAAa,UACtB6B,EAAY7B,EACZ8B,EAAa9B,IAEb6B,EAAY7B,EAAS,MACrB8B,EAAa9B,EAAS,QAGxB,IAAM+B,EAAO,KAAK,IAAI,EAAG,KAAK,OAAOjB,EAAU,MAAQe,GAAa,CAAC,CAAC,EAChEG,EAAO,KAAK,IAAI,EAAG,KAAK,OAAOlB,EAAU,OAASgB,GAAc,CAAC,CAAC,EAExE,KAAK,aAAY,EAGjB,IAAMG,EAAY,SAAS,cAAc,QAAQ,EACjD,OAAAA,EAAU,MAAQnB,EAAU,MAC5BmB,EAAU,OAASnB,EAAU,OACdmB,EAAU,WAAW,IAAI,EACjC,aAAanB,EAAW,EAAG,CAAC,EAGnC,KAAK,OAAQ,MAAQe,EACrB,KAAK,OAAQ,OAASC,EACtB,KAAK,IAAK,UAAUG,EAAWF,EAAMC,EAAMH,EAAWC,EAAY,EAAG,EAAGD,EAAWC,CAAU,EAEtF,KAAK,IAAK,aAAa,EAAG,EAAGD,EAAWC,CAAU,CAC3D,CAKQ,YACNI,EAA0D,CAE1D,KAAK,aAAY,EAEjB,GAAM,CAAE,MAAAtC,EAAO,OAAAC,CAAM,EAAKqC,EAC1B,YAAK,OAAQ,MAAQtC,EACrB,KAAK,OAAQ,OAASC,EAEtB,KAAK,IAAK,UAAUqC,EAAQ,EAAG,CAAC,EACzB,KAAK,IAAK,aAAa,EAAG,EAAGtC,EAAOC,CAAM,CACnD,CAKQ,OAAOiB,EAAoB,CACjC,GAAM,CAAE,MAAAlB,EAAO,OAAAC,EAAQ,WAAAsC,CAAU,EAAK,KAAK,QAE3C,KAAK,aAAY,EAGjB,IAAIJ,EAAO,EAAGC,EAAO,EAAGI,EAAOtB,EAAU,MAAOuB,EAAOvB,EAAU,OAC7DwB,EAAO,EAAGC,EAAO,EAAGC,EAAO5C,EAAO6C,EAAO5C,EAE7C,GAAIsC,IAAe,UAAW,CAC5B,IAAMO,EAAQ,KAAK,IAAI9C,EAAQkB,EAAU,MAAOjB,EAASiB,EAAU,MAAM,EACzE0B,EAAO,KAAK,MAAM1B,EAAU,MAAQ4B,CAAK,EACzCD,EAAO,KAAK,MAAM3B,EAAU,OAAS4B,CAAK,EAC1CJ,EAAO,KAAK,OAAO1C,EAAQ4C,GAAQ,CAAC,EACpCD,EAAO,KAAK,OAAO1C,EAAS4C,GAAQ,CAAC,CACvC,SAAWN,IAAe,QAAS,CACjC,IAAMO,EAAQ,KAAK,IAAI9C,EAAQkB,EAAU,MAAOjB,EAASiB,EAAU,MAAM,EACzEsB,EAAO,KAAK,MAAMxC,EAAQ8C,CAAK,EAC/BL,EAAO,KAAK,MAAMxC,EAAS6C,CAAK,EAChCX,EAAO,KAAK,OAAOjB,EAAU,MAAQsB,GAAQ,CAAC,EAC9CJ,EAAO,KAAK,OAAOlB,EAAU,OAASuB,GAAQ,CAAC,CACjD,CAGA,IAAMJ,EAAY,SAAS,cAAc,QAAQ,EACjD,OAAAA,EAAU,MAAQnB,EAAU,MAC5BmB,EAAU,OAASnB,EAAU,OACdmB,EAAU,WAAW,IAAI,EACjC,aAAanB,EAAW,EAAG,CAAC,EAGnC,KAAK,OAAQ,MAAQlB,EACrB,KAAK,OAAQ,OAASC,GAGlBsC,IAAe,WAAaA,IAAe,SAC7C,KAAK,IAAK,UAAY,QACtB,KAAK,IAAK,SAAS,EAAG,EAAGvC,EAAOC,CAAM,GAGxC,KAAK,IAAK,UAAUoC,EAAWF,EAAMC,EAAMI,EAAMC,EAAMC,EAAMC,EAAMC,EAAMC,CAAI,EAEtE,KAAK,IAAK,aAAa,EAAG,EAAG7C,EAAOC,CAAM,CACnD,CAKQ,SAASiB,EAAoB,CACnC,GAAM,CACJ,KAAA6B,EAAM,IAAAC,EAAK,UAAAC,EAAW,cAAAC,EAAe,MAAAC,EACrC,UAAAzC,EAAW,cAAAF,EAAe,YAAAG,CAAW,EACnC,KAAK,QAEHV,EAASiB,EAAU,OACnBlB,EAAQkB,EAAU,MAClBO,EAAWwB,EAAY,EAAI,EAE3BG,EAAO,IAAI,aAAa3B,EAAWxB,EAASD,CAAK,EACjDqD,EAASnC,EAAU,KAEzB,QAAS,EAAI,EAAG,EAAIjB,EAAQ,IAC1B,QAASqD,EAAI,EAAGA,EAAItD,EAAOsD,IAAK,CAC9B,IAAMC,GAAY,EAAIvD,EAAQsD,GAAK,EAEnC,GAAIL,EAAW,CAEb,IAAIO,EACF,MAASH,EAAOE,CAAQ,GAAK,GAC7B,MAASF,EAAOE,EAAW,CAAC,GAAK,GACjC,MAASF,EAAOE,EAAW,CAAC,GAAK,GAG/B7C,IACF8C,GAAQhD,GAGNG,IACF6C,GAAQA,GAAQT,EAAK,CAAC,GAAK,KAAOC,EAAI,CAAC,GAAK,IAG9C,IAAMS,EAAM,EAAIzD,EAAQsD,EACxBF,EAAKK,CAAG,EAAID,CACd,SAAWN,IAAkB,MAE3B,QAASQ,EAAI,EAAGA,EAAI,EAAGA,IAAK,CAC1B,IAAIC,EAAQN,EAAOE,EAAWG,CAAC,GAAK,EAEhChD,IACFiD,GAASnD,GAGPG,IACFgD,GAASA,GAASZ,EAAKW,CAAC,GAAK,KAAOV,EAAIU,CAAC,GAAK,IAGhD,IAAMD,EAAMC,EAAIzD,EAASD,EAAQ,EAAIA,EAAQsD,EAC7CF,EAAKK,CAAG,EAAIE,CACd,KAGA,SAASD,EAAI,EAAGA,EAAI,EAAGA,IAAK,CAC1B,IAAIC,EAAQN,EAAOE,EAAWG,CAAC,GAAK,EAEhChD,IACFiD,GAASnD,GAGPG,IACFgD,GAASA,GAASZ,EAAKW,CAAC,GAAK,KAAOV,EAAIU,CAAC,GAAK,IAGhD,IAAMD,EAAM,EAAIzD,EAAQ,EAAIsD,EAAI,EAAII,EACpCN,EAAKK,CAAG,EAAIE,CACd,CAEJ,CAGF,IAAMC,EAAQV,IAAkB,MAC5B,CAACzB,EAAUxB,EAAQD,CAAK,EACxB,CAACC,EAAQD,EAAOyB,CAAQ,EAE5B,OAAO,IAAID,EAAe4B,EAAMQ,EAAOT,CAAK,CAC9C,CAKA,YAAU,CACR,MAAO,CAAE,GAAG,KAAK,OAAO,CAC1B,GA4BIU,GAA4D,CAChE,WAAY,KACZ,MAAO,GACP,KAAM,IACN,UAAW,IACX,UAAW,GACX,YAAa,IAQFC,EAAP,MAAOC,CAAiB,CAI5B,YAAYlE,EAAoC,CAAA,EAAE,CAHjCC,EAAA,gBACTA,EAAA,oBAAoC,MAG1C,KAAK,QAAU,CAAE,GAAG+D,GAAuB,GAAGhE,CAAO,CACvD,CAKA,OAAO,WAAWK,EAA+B,CAC/C,IAAML,EAAoC,CAAA,EAEpCmE,EAAe9D,EAAO,cACxB,OAAO8D,GAAiB,WAC1BnE,EAAQ,WAAamE,GAGvB,IAAMC,EAAc/D,EAAO,aACvB,OAAO+D,GAAgB,WACzBpE,EAAQ,MAAQoE,GAGlB,IAAMC,EAAOhE,EAAO,MAChB,OAAOgE,GAAS,WAClBrE,EAAQ,KAAOqE,GAGjB,IAAMC,EAAYjE,EAAO,WACzB,OAAI,OAAOiE,GAAc,WACvBtE,EAAQ,UAAYsE,GAGf,IAAIJ,EAAkBlE,CAAO,CACtC,CAKA,aAAa,gBACXkB,EACAlB,EAA+B,CAE/B,IAAMmB,EAAWnB,GAAS,UAAY,OAChCgB,EAAM,0BAA0BE,CAAO,YAAYC,CAAQ,4BAE3DF,EAAW,MAAM,MAAMD,CAAG,EAChC,GAAI,CAACC,EAAS,GACZ,MAAM,IAAI,MAAM,oCAAoCD,CAAG,EAAE,EAE3D,IAAMX,EAAS,MAAMY,EAAS,KAAI,EAClC,OAAOiD,EAAkB,WAAW7D,CAAM,CAC5C,CAKQ,oBAAkB,CACxB,GAAI,CAAC,KAAK,aACR,GAAI,OAAO,aAAiB,IAC1B,KAAK,aAAe,IAAI,aAAa,CAAE,WAAY,KAAK,QAAQ,UAAU,CAAE,MAE5E,OAAM,IAAI,MAAM,kDAAkD,CAGxE,CAKA,MAAM,QAAQe,EAAiB,CAC7B,IAAImD,EAEA,OAAOnD,GAAU,SAEnBmD,EAAY,MAAM,KAAK,YAAYnD,CAAK,EAC/BA,aAAiB,MAAQA,aAAiB,KAEnDmD,EAAY,MAAM,KAAK,aAAanD,CAAK,EAChCA,aAAiB,YAC1BmD,EAAY,KAAK,qBAAqBnD,CAAK,EAClCA,aAAiB,aAC1BmD,EAAYnD,EAGZmD,EAAY,MAAM,KAAK,gBAAgBnD,CAAK,EAO1C,KAAK,QAAQ,YACfmD,EAAY,KAAK,eAAeA,CAAS,GAI3C,IAAMC,EAAa,KAAK,QAAQ,YAAc,KAAK,QAAQ,WAC3D,OAAID,EAAU,OAASC,IACrBD,EAAYA,EAAU,MAAM,EAAGC,CAAU,GAI3B,KAAK,sBAAsBD,CAAS,CAGtD,CAKA,MAAM,WAAWnD,EAAiB,CAChC,IAAImD,EAEA,OAAOnD,GAAU,SACnBmD,EAAY,MAAM,KAAK,YAAYnD,CAAK,EAC/BA,aAAiB,MAAQA,aAAiB,KACnDmD,EAAY,MAAM,KAAK,aAAanD,CAAK,EAChCA,aAAiB,YAC1BmD,EAAY,KAAK,qBAAqBnD,CAAK,EAClCA,aAAiB,aAC1BmD,EAAYnD,EAEZmD,EAAY,MAAM,KAAK,gBAAgBnD,CAAK,EAI1C,KAAK,QAAQ,YACfmD,EAAY,KAAK,eAAeA,CAAS,GAI3C,IAAMC,EAAa,KAAK,QAAQ,YAAc,KAAK,QAAQ,WAC3D,OAAID,EAAU,OAASC,IACrBD,EAAYA,EAAU,MAAM,EAAGC,CAAU,GAGpC,IAAI7C,EAAe4C,EAAW,CAAC,EAAGA,EAAU,MAAM,EAAG,SAAS,CACvE,CAKQ,MAAM,YAAYvD,EAAW,CACnC,IAAMC,EAAW,MAAM,MAAMD,CAAG,EAChC,GAAI,CAACC,EAAS,GACZ,MAAM,IAAI,MAAM,6BAA6BD,CAAG,EAAE,EAGpD,IAAMyD,EAAc,MAAMxD,EAAS,YAAW,EAC9C,OAAO,KAAK,gBAAgBwD,CAAW,CACzC,CAKQ,MAAM,aAAatC,EAAU,CACnC,IAAMsC,EAAc,MAAMtC,EAAK,YAAW,EAC1C,OAAO,KAAK,gBAAgBsC,CAAW,CACzC,CAKQ,MAAM,gBAAgBlB,EAAiB,CAC7C,KAAK,mBAAkB,EACvB,IAAMmB,EAAc,MAAM,KAAK,aAAc,gBAAgBnB,EAAK,MAAM,CAAC,CAAC,EAC1E,OAAO,KAAK,qBAAqBmB,CAAW,CAC9C,CAKQ,qBAAqBC,EAAmB,CAE9C,IAAMC,EAAcD,EAAO,eAAe,CAAC,EAC3C,OAAO,IAAI,aAAaC,CAAW,CACrC,CAKQ,eAAerB,EAAkB,CACvC,IAAIsB,EAAM,EACV,QAAS/C,EAAI,EAAGA,EAAIyB,EAAK,OAAQzB,IAAK,CACpC,IAAMgD,EAAM,KAAK,IAAIvB,EAAKzB,CAAC,GAAK,CAAC,EAC7BgD,EAAMD,IAAKA,EAAMC,EACvB,CAEA,GAAID,EAAM,EAAG,CACX,IAAME,EAAS,IAAI,aAAaxB,EAAK,MAAM,EAC3C,QAASzB,EAAI,EAAGA,EAAIyB,EAAK,OAAQzB,IAC/BiD,EAAOjD,CAAC,GAAKyB,EAAKzB,CAAC,GAAK,GAAK+C,EAE/B,OAAOE,CACT,CAEA,OAAOxB,CACT,CAKQ,sBAAsByB,EAAmB,CAC/C,GAAM,CAAE,MAAAC,EAAO,KAAAZ,EAAM,UAAAC,CAAS,EAAK,KAAK,QAGlCY,EAAY,KAAK,OAAOF,EAAM,OAASX,GAAQC,CAAS,EAAI,EAElE,GAAIY,GAAa,EAEf,OAAO,IAAIvD,EAAe,IAAI,aAAasD,CAAK,EAAG,CAAC,EAAGA,CAAK,EAAG,SAAS,EAG1E,IAAME,EAAU,IAAI,aAAaD,EAAYD,CAAK,EAIlD,QAASG,EAAQ,EAAGA,EAAQF,EAAWE,IAAS,CAC9C,IAAMC,EAAQD,EAAQd,EAGtB,QAASgB,EAAM,EAAGA,EAAML,EAAOK,IAAO,CACpC,IAAIC,EAAS,EACPC,EAAY,KAAK,MAAOF,EAAML,GAAUZ,EAAO,EAAE,EACjDoB,EAAU,KAAK,OAAQH,EAAM,GAAKL,GAAUZ,EAAO,EAAE,EAE3D,QAASvC,EAAI0D,EAAW1D,EAAI,KAAK,IAAI2D,EAASpB,CAAI,EAAGvC,IAAK,CACxD,IAAM4D,EAASV,EAAMK,EAAQvD,CAAC,GAAK,EACnCyD,GAAUG,EAASA,CACrB,CAGAP,EAAQC,EAAQH,EAAQK,CAAG,EAAI,KAAK,IAAIC,EAAS,KAAK,CACxD,CACF,CAEA,OAAO,IAAI5D,EAAewD,EAAS,CAACD,EAAWD,CAAK,EAAG,SAAS,CAClE,CAKA,SAAO,CACD,KAAK,eACP,KAAK,aAAa,MAAK,EACvB,KAAK,aAAe,KAExB,GAwBI,SAAUU,GACdC,EACA5F,EAAmC,CAAA,EAAE,CAErC,GAAM,CACJ,UAAA6F,EAAY,GACZ,kBAAAC,EAAoB,GACpB,oBAAAC,EAAsB,GACtB,UAAAC,CAAS,EACPhG,EAEA+E,EAASa,EAEb,OAAIC,IACFd,EAASA,EAAO,YAAW,GAGzBe,IACFf,EAASA,EAAO,QAAQ,WAAY,EAAE,GAGpCgB,IACFhB,EAASA,EAAO,QAAQ,OAAQ,GAAG,EAAE,KAAI,GAGvCiB,GAAajB,EAAO,OAASiB,IAC/BjB,EAASA,EAAO,MAAM,EAAGiB,CAAS,GAG7BjB,CACT,CASM,SAAUkB,GACdC,EAAiD,WACjDlG,EAAoC,CAAA,EAAE,CAEtC,IAAMmG,EAAoD,CACxD,SAAU,CACR,MAAO,IACP,OAAQ,IACR,KAAM,CAAC,KAAO,KAAO,IAAK,EAC1B,IAAK,CAAC,KAAO,KAAO,IAAK,GAE3B,KAAM,CACJ,MAAO,IACP,OAAQ,IACR,KAAM,CAAC,UAAY,SAAW,SAAU,EACxC,IAAK,CAAC,UAAY,UAAY,SAAU,GAE1C,IAAK,CACH,MAAO,IACP,OAAQ,IACR,KAAM,CAAC,GAAK,GAAK,EAAG,EACpB,IAAK,CAAC,GAAK,GAAK,EAAG,GAErB,OAAQ,CAAA,GAGV,OAAO,IAAIrG,EAAkB,CAAE,GAAGqG,EAAQD,CAAM,EAAG,GAAGlG,CAAO,CAAE,CACjE,CAKM,SAAUoG,GACdF,EAA2C,UAC3ClG,EAAoC,CAAA,EAAE,CAEtC,IAAMmG,EAAoD,CACxD,QAAS,CACP,WAAY,KACZ,MAAO,GACP,KAAM,IACN,UAAW,KAEb,QAAS,CACP,WAAY,KACZ,UAAW,IAEb,OAAQ,CAAA,GAGV,OAAO,IAAIlC,EAAkB,CAAE,GAAGkC,EAAQD,CAAM,EAAG,GAAGlG,CAAO,CAAE,CACjE,CC52BM,IAAOqG,EAAP,cAA2CC,CAGhD,CAKC,YACEC,EACAC,EACAC,EAAqB,IAAI,CAEzB,MAAMF,CAAM,EATNG,EAAA,oBAAyC,MACzCA,EAAA,eACAA,EAAA,mBAQN,KAAK,OAASF,GAAUG,GACxB,KAAK,WAAaF,CACpB,CAKS,MAAM,YAAU,CACvB,MAAM,MAAM,WAAU,EAEjB,KAAK,eACR,KAAK,aAAeG,GAAwB,UAAU,EAE1D,CAKA,UAAUJ,EAAgB,CACxB,KAAK,OAASA,EACd,KAAK,WAAaA,EAAO,MAC3B,CAKS,MAAM,IACbK,EACAC,EAAoC,CAEpC,IAAMC,EAAU,MAAM,QAAQF,CAAK,EAC7BG,EAASD,EAAUF,EAAQ,CAACA,CAAK,EAEvC,MAAM,KAAK,WAAU,EAErB,IAAMI,EAAY,YAAY,IAAG,EAC3BC,EAAuC,CAAA,EAE7C,QAAWC,KAASH,EAAQ,CAE1B,IAAMI,EAAe,MAAM,KAAK,WAAWD,CAAK,EAG1CE,EAAU,MAAM,KAAK,aAAaD,CAAY,EAG9CE,EAAS,MAAM,KAAK,YAAYD,EAASP,CAAO,EACtDI,EAAQ,KAAKI,CAAM,CACrB,CAEA,IAAMC,EAAiB,YAAY,IAAG,EAAKN,EAE3C,QAAWK,KAAUJ,EACnBI,EAAO,eAAiBC,EAAiBL,EAAQ,OAGnD,OAAOH,EAAUG,EAAUA,EAAQ,CAAC,CACtC,CAKmB,MAAM,WAAWL,EAAgC,CAClE,IAAMM,EAAQ,MAAM,QAAQN,CAAK,EAAIA,EAAM,CAAC,EAAKA,EAG3CW,EAAS,MAAM,KAAK,aAAc,QAAQL,CAAK,EAGrD,OAAIK,EAAO,MAAM,SAAW,EACnB,CAACA,EAAO,QAAQ,CAAC,EAAG,GAAGA,EAAO,KAAK,CAAC,CAAC,EAGvC,CAACA,CAAM,CAChB,CAKQ,MAAM,aAAaR,EAAwB,CAGjD,IAAMS,EAAS,IAAI,aAAa,KAAK,UAAU,EAGzCC,EAAYV,EAAO,CAAC,GAAG,eAAc,GAAM,IAAI,aAAa,CAAC,EAC/DW,EAAM,EACV,QAAS,EAAI,EAAG,EAAI,KAAK,IAAI,IAAMD,EAAU,MAAM,EAAG,IACpDC,GAAOD,EAAU,CAAC,GAAK,EAGzB,QAAS,EAAI,EAAG,EAAI,KAAK,WAAY,IACnCD,EAAO,CAAC,EAAI,KAAK,IAAIE,GAAO,EAAI,GAAK,EAAG,EAAI,EAG9C,MAAO,CAAC,IAAIC,EAAeH,EAAQ,CAAC,EAAG,KAAK,UAAU,EAAG,SAAS,CAAC,CACrE,CAKmB,MAAM,YACvBJ,EACAP,EAAoC,CAEpC,IAAMW,EAASJ,EAAQ,CAAC,EACxB,GAAI,CAACI,EACH,MAAO,CAAE,MAAO,UAAW,MAAO,CAAC,EAKrC,IAAMI,EADQC,EAAQL,EAAQ,EAAE,EACP,eAAc,GAE1BX,GAAS,MAAQ,GAEnB,GAAKA,GAAS,gBAKzB,IAAIiB,EAAS,EACTC,EAAWH,EAAW,CAAC,GAAK,EAEhC,QAASI,EAAI,EAAGA,EAAIJ,EAAW,OAAQI,KAChCJ,EAAWI,CAAC,GAAK,GAAKD,IACzBA,EAAWH,EAAWI,CAAC,GAAK,EAC5BF,EAASE,GAMb,MAAO,CACL,MAHYnB,GAAS,SAASiB,CAAM,GAAK,KAAK,OAAOA,CAAM,GAAK,SAASA,CAAM,GAI/E,MAAOC,EAEX,GAUI,SAAUE,GACd3B,EAAkC,CAAA,EAClCC,EAAiB,CAEjB,OAAO,IAAIH,EACT,CACE,KAAM,uBACN,MAAOE,EAAO,OAAS,UACvB,QAASA,EAAO,QAChB,MAAOA,EAAO,OAAS,GACvB,aAAcA,EAAO,cAEvBC,CAAM,CAEV,CAGA2B,EAAiB,uBAAyB5B,GAAW,IAAIF,EAA4BE,CAAM,CAAC,EClItF,IAAO6B,GAAP,cAAsCC,CAA8E,CAIxH,YAAYC,EAAuB,CACjC,MAAMA,GAAU,CACd,KAAM,kBACN,MAAO,UACR,EAPKC,EAAA,iBAA8B,MAC9BA,EAAA,kBAAqB,MAO7B,CAKA,aAAaC,EAAoB,CAC/B,KAAK,UAAYA,EACjB,IAAMC,EAAaD,EAAU,mBAAkB,EAC/C,KAAK,WAAaC,EAAW,YAAcA,EAAW,YAAc,KACtE,CAKU,MAAM,WAAWC,EAAwB,CAEjD,IAAMC,EAAO,MAAM,QAAQD,CAAK,EAAIA,EAAM,CAAC,GAAK,GAAKA,EACrD,GAAI,CAAC,KAAK,UAER,MAAO,CAAC,IAAIE,EAAe,IAAI,aAAa,CAAC,CAAC,CAAC,EAAG,CAAC,CAAC,EAAG,SAAS,CAAC,EAEnE,IAAMC,EAAU,KAAK,UAAU,OAAOF,EAAM,CAC1C,iBAAkB,GAClB,QAAS,aACV,EACD,MAAO,CAAC,IAAIC,EACV,cAAc,KAAKC,EAAQ,SAAS,IAAIC,GAAM,OAAOA,CAAE,CAAC,CAAC,EACzD,CAAC,EAAGD,EAAQ,SAAS,MAAM,EAC3B,OAAO,CACR,CACH,CAKU,MAAM,YACdE,EACAC,EAA0B,CAG1B,MAAO,CACL,cAAe,GACf,SAAU,CAAA,EACV,UAAW,EACX,eAAgB,EAEpB,CAKS,MAAM,IACbC,EACAC,EAAiD,CAEjD,MAAM,KAAK,WAAU,EAErB,IAAMC,EAAU,MAAM,QAAQF,CAAM,EAAIA,EAAS,CAACA,CAAM,EAClDG,EAAU,MAAM,QAAQ,IAC5BD,EAAQ,IAAIE,GAAK,KAAK,eAAeA,EAAGH,GAAW,CAAA,CAAE,CAAC,CAAC,EAEzD,OAAO,MAAM,QAAQD,CAAM,EAAIG,EAAUA,EAAQ,CAAC,CACpD,CAKA,MAAO,OACLH,EACAC,EAAiC,CAAA,EAAE,CAEnC,IAAMI,EAAY,YAAY,IAAG,EAEjC,GAAI,CAAC,KAAK,UACR,MAAM,IAAI,MAAM,+CAA+C,EAGjE,GAAM,CACJ,aAAAC,EAAe,GACf,UAAAC,EAAY,IACZ,YAAAC,EAAc,EACd,KAAAC,EAAO,EACP,KAAAC,EAAO,EACP,kBAAAC,EAAoB,EACpB,cAAAC,EAAgB,CAAA,EAChB,SAAAC,EAAW,EAAI,EACbZ,EASAa,EAAW,CAAC,GANA,KAAK,UAAU,OAAOd,EAAQ,CAC5C,iBAAkB,GAClB,QAAS,aACT,WAAY,GACb,EAE0B,QAAQ,EAC7Be,EAAyB,CAAA,EAC3BC,EAAgB,GAGpB,QAASC,EAAI,EAAGA,EAAIX,GAEd,EAAAQ,EAAS,QAAUP,GAFSU,IAAK,CAKrC,IAAMC,EAAc,MAAM,KAAK,kBAC7BJ,EACAN,EACAC,EACAC,EACAC,EACAE,CAAQ,EAIV,GAAIK,IAAgB,KAAK,WAAY,CACnC,KAAM,CACJ,MAAO,GACP,QAASA,EACT,cAAAF,EACA,KAAM,IAER,KACF,CAGA,IAAMG,EAAQ,KAAK,UAAU,OAAO,CAACD,CAAW,EAAG,EAAI,EACvDH,EAAa,KAAKG,CAAW,EAC7BJ,EAAS,KAAKI,CAAW,EACzBF,GAAiBG,EAGblB,EAAQ,SACVA,EAAQ,QAAQkB,EAAOD,CAAW,EAIpC,IAAIE,EAAa,GACjB,QAAWC,KAAWT,EACpB,GAAII,EAAc,SAASK,CAAO,EAAG,CACnCL,EAAgBA,EAAc,MAAM,EAAG,CAACK,EAAQ,MAAM,EACtDD,EAAa,GACb,KACF,CAUF,GAPA,KAAM,CACJ,MAAAD,EACA,QAASD,EACT,cAAAF,EACA,KAAMI,GAGJA,EAAY,KAClB,CAGA,IAAME,EAAU,YAAY,IAAG,EAC/B,QAAQ,IAAI,4BAA4BA,EAAUjB,GAAW,QAAQ,CAAC,CAAC,IAAI,CAC7E,CAKQ,MAAM,eACZL,EACAC,EAA8B,CAE9B,IAAMI,EAAY,YAAY,IAAG,EAEjC,GAAI,CAAC,KAAK,UACR,MAAM,IAAI,MAAM,+CAA+C,EAGjE,GAAM,CACJ,aAAAC,EAAe,GACf,UAAAC,EAAY,IACZ,YAAAC,EAAc,EACd,KAAAC,EAAO,EACP,KAAAC,EAAO,EACP,kBAAAC,EAAoB,EACpB,cAAAC,EAAgB,CAAA,EAChB,SAAAC,EAAW,GACX,eAAAU,EAAiB,EAAK,EACpBtB,EASAa,EAAW,CAAC,GANA,KAAK,UAAU,OAAOd,EAAQ,CAC5C,iBAAkB,GAClB,QAAS,aACT,WAAY,GACb,EAE0B,QAAQ,EAC7Be,EAAyB,CAAA,EAG/B,QAASE,EAAI,EAAGA,EAAIX,GAEd,EAAAQ,EAAS,QAAUP,GAFSU,IAAK,CAKrC,IAAMC,EAAc,MAAM,KAAK,kBAC7BJ,EACAN,EACAC,EACAC,EACAC,EACAE,CAAQ,EAIV,GAAIK,IAAgB,KAAK,WAAY,MAOrC,GAJAH,EAAa,KAAKG,CAAW,EAC7BJ,EAAS,KAAKI,CAAW,EAGrBjB,EAAQ,QAAS,CACnB,IAAMkB,EAAQ,KAAK,UAAU,OAAO,CAACD,CAAW,EAAG,EAAI,EACvDjB,EAAQ,QAAQkB,EAAOD,CAAW,CACpC,CAGA,IAAMM,EAAc,KAAK,UAAU,OAAOT,EAAc,EAAI,EACxDK,EAAa,GACjB,QAAWC,KAAWT,EACpB,GAAIY,EAAY,SAASH,CAAO,EAAG,CACjCD,EAAa,GACb,KACF,CAEF,GAAIA,EAAY,KAClB,CAGA,IAAMJ,EAAgB,KAAK,UAAU,OAAOD,EAAc,EAAI,EACxDO,EAAU,YAAY,IAAG,EAE/B,MAAO,CACL,cAAAN,EACA,SAAUO,EAAiBvB,EAASgB,EAAgB,OACpD,SAAUD,EACV,UAAWA,EAAa,OACxB,eAAgBO,EAAUjB,EAE9B,CAKQ,MAAM,kBACZS,EACAN,EACAC,EACAC,EACAC,EACAE,EAAiB,CAEjB,GAAI,CAAC,KAAK,MACR,MAAM,IAAI,MAAM,kBAAkB,EAIpC,IAAMY,EAAc,IAAI9B,EACtB,cAAc,KAAKmB,EAAS,IAAIjB,GAAM,OAAOA,CAAE,CAAC,CAAC,EACjD,CAAC,EAAGiB,EAAS,MAAM,EACnB,OAAO,EAIHY,EAAgB,IAAI/B,EACxB,cAAc,KAAKmB,EAAS,IAAI,IAAM,OAAO,CAAC,CAAC,CAAC,EAChD,CAAC,EAAGA,EAAS,MAAM,EACnB,OAAO,EAIHa,EAAU,MAAMC,GAAa,KAAK,MAAO,CAACH,EAAaC,CAAa,CAAC,EAE3E,GAAI,CAACC,GAAWA,EAAQ,SAAW,EACjC,MAAM,IAAI,MAAM,2BAA2B,EAI7C,IAAME,EAASF,EAAQ,CAAC,EAClBG,EAAaD,EAAO,eAAc,EAClCE,EAAYF,EAAO,MAAMA,EAAO,MAAM,OAAS,CAAC,GAAK,MAGrDG,EAAqB,IAAI,aAAaD,CAAS,EAC/CE,GAAUnB,EAAS,OAAS,GAAKiB,EAEvC,QAASd,EAAI,EAAGA,EAAIc,EAAWd,IAC7Be,EAAmBf,CAAC,EAAIa,EAAWG,EAAShB,CAAC,GAAK,EAIpD,GAAIN,IAAsB,GACxB,QAAWuB,KAAUpB,EACnB,GAAIoB,EAASH,EAAW,CACtB,IAAMI,EAAQH,EAAmBE,CAAM,GAAK,EAC5CF,EAAmBE,CAAM,EAAIC,EAAQ,EACjCA,EAAQxB,EACRwB,EAAQxB,CACd,EAKJ,GAAIH,IAAgB,EAClB,QAASS,EAAI,EAAGA,EAAIc,EAAWd,IAC7Be,EAAmBf,CAAC,GAAKe,EAAmBf,CAAC,GAAK,GAAKT,EAK3D,IAAM4B,EAAe,IAAIzC,EAAeqC,EAAoB,CAACD,CAAS,EAAG,SAAS,EAC5EM,EAAQC,EAAQF,CAAY,EAAE,eAAc,EAGlD,OAAIvB,EACK,KAAK,OAAOwB,EAAO5B,EAAMC,CAAI,EAE7B,KAAK,OAAO2B,CAAK,CAE5B,CAKQ,OAAOA,EAAmB,CAChC,IAAIE,EAAS,EACTC,EAAUH,EAAM,CAAC,GAAK,EAE1B,QAASpB,EAAI,EAAGA,EAAIoB,EAAM,OAAQpB,KAC3BoB,EAAMpB,CAAC,GAAK,GAAKuB,IACpBA,EAAUH,EAAMpB,CAAC,GAAK,EACtBsB,EAAStB,GAIb,OAAOsB,CACT,CAKQ,OAAOF,EAAqB5B,EAAcC,EAAY,CAE5D,IAAM+B,EAAU,MAAM,KAAK,CAAE,OAAQJ,EAAM,MAAM,EAAI,CAACK,EAAGzB,IAAMA,CAAC,EAChEwB,EAAQ,KAAK,CAACE,EAAGC,KAAOP,EAAMO,CAAC,GAAK,IAAMP,EAAMM,CAAC,GAAK,EAAE,EAGxD,IAAIE,EAAmBJ,EAMvB,GALIhC,EAAO,GAAKA,EAAO4B,EAAM,SAC3BQ,EAAmBJ,EAAQ,MAAM,EAAGhC,CAAI,GAItCC,EAAO,EAAK,CACd,IAAIoC,EAAiB,EACfC,EAAqB,CAAA,EAE3B,QAAWC,KAAOH,EAGhB,GAFAE,EAAS,KAAKC,CAAG,EACjBF,GAAkBT,EAAMW,CAAG,GAAK,EAC5BF,GAAkBpC,EAAM,MAG9BmC,EAAmBE,CACrB,CAGA,IAAIE,EAAY,EAChB,QAAWD,KAAOH,EAChBI,GAAaZ,EAAMW,CAAG,GAAK,EAI7B,IAAME,EAAI,KAAK,OAAM,EAAKD,EACtBE,EAAa,EAEjB,QAAWH,KAAOH,EAEhB,GADAM,GAAcd,EAAMW,CAAG,GAAK,EACxBG,GAAcD,EAChB,OAAOF,EAKX,OAAOH,EAAiB,CAAC,GAAK,CAChC,GC7bK,IAAMO,GAAc,CACzB,SAAU,UAAW,MAAO,aAAc,WAAY,MAAO,QAAS,QACtE,OAAQ,gBAAiB,eAAgB,YAAa,gBAAiB,QACvE,OAAQ,MAAO,MAAO,QAAS,QAAS,MAAO,WAAY,OAAQ,QACnE,UAAW,WAAY,WAAY,UAAW,MAAO,WAAY,UACjE,OAAQ,YAAa,cAAe,OAAQ,eAAgB,iBAC5D,aAAc,YAAa,gBAAiB,SAAU,aAAc,MACpE,OAAQ,QAAS,QAAS,OAAQ,SAAU,QAAS,WAAY,SACjE,WAAY,SAAU,UAAW,QAAS,QAAS,OAAQ,QAAS,QACpE,eAAgB,MAAO,eAAgB,SAAU,KAAM,SAAU,QACjE,SAAU,WAAY,aAAc,YAAa,OAAQ,UAAW,OACpE,eAAgB,OAAQ,QAAS,OAAQ,WAAY,aAAc,aACnE,cAoBWC,GAAP,cAAuCC,CAAoD,CAI/F,YAAYC,EAAyBC,EAAiB,CACpD,MAAMD,GAAU,CACd,KAAM,mBACN,MAAO,UACR,EAPKE,EAAA,qBACAA,EAAA,eAQN,KAAK,OAASD,GAAUJ,GACxB,KAAK,aAAe,IAAIM,EAAkB,CACxC,MAAO,IACP,OAAQ,IACR,KAAM,CAAC,KAAO,KAAO,IAAK,EAC1B,IAAK,CAAC,KAAO,KAAO,IAAK,EACzB,cAAe,MAChB,CACH,CAKA,UAAUF,EAAgB,CACxB,KAAK,OAASA,CAChB,CAKU,MAAM,WAAWG,EAAgC,CACzD,IAAMC,EAAS,MAAM,QAAQD,CAAK,EAAIA,EAAQ,CAACA,CAAK,EAEpD,GAAIC,EAAO,SAAW,EAAG,CACvB,IAAMC,EAAS,MAAM,KAAK,aAAa,QAAQD,EAAO,CAAC,CAAE,EAEzD,MAAO,CAAC,IAAIE,EACVD,EAAO,eAAc,EACrB,CAAC,EAAG,GAAGA,EAAO,KAAK,EACnB,SAAS,CACV,CACH,CAEA,MAAO,CAAC,MAAM,KAAK,aAAa,aAAaD,CAAM,CAAC,CACtD,CAKU,MAAM,YACdG,EACAC,EAAyB,CAEzB,IAAMC,EAAOD,GAAqC,CAAA,EAC5CE,EAAYD,EAAK,WAAa,GAC9BE,EAAOF,EAAK,MAAQ,IACpBG,EAAMH,EAAK,KAAO,GAClBI,EAAeJ,EAAK,cAAgB,GAM1C,GAAI,CAACF,EAAQ,CAAC,EACZ,MAAO,CAAA,EAGT,IAAMO,EAAaP,EAAQ,CAAC,EAAE,eAAc,EACtCQ,EAAQ,CAAC,GAAGR,EAAQ,CAAC,EAAE,KAAK,EAG5BS,EAAa,KAAK,gBAAgBF,EAAYC,EAAOL,CAAS,EAGhEO,EAAWL,EAAM,KAAK,kBAAkBI,EAAYH,CAAY,EAAIG,EAGxE,OAAAC,EAAS,KAAK,CAACC,EAAGC,IAAMA,EAAE,MAAQD,EAAE,KAAK,EACzCD,EAAWA,EAAS,MAAM,EAAGN,CAAI,EAE1BM,CACT,CAKQ,gBACNG,EACAL,EACAL,EAAiB,CAEjB,IAAMM,EAA0B,CAAA,EAM1BK,EAAWN,EAAM,CAAC,GAAK,EACvBO,EAAUP,EAAM,CAAC,GAAK,EAE5B,GAAIO,GAAW,EAAG,CAEhB,IAAMC,EAAaD,EAAU,EAE7B,QAASE,EAAI,EAAGA,EAAIH,EAAUG,IAAK,CACjC,IAAMC,EAASD,EAAIF,EACbI,EAAaN,EAAKK,EAAS,CAAC,GAAK,EAEvC,GAAIC,EAAahB,EAAW,SAG5B,IAAIiB,EAAgB,EAChBC,EAAc,EAElB,QAASC,EAAI,EAAGA,EAAIN,EAAYM,IAAK,CACnC,IAAMC,EAAQV,EAAKK,EAAS,EAAII,CAAC,GAAK,EAClCC,EAAQH,IACVA,EAAgBG,EAChBF,EAAcC,EAElB,CAEA,IAAME,EAAaL,EAAaC,EAChC,GAAII,EAAarB,EAAW,SAG5B,IAAMsB,EAAIZ,EAAKK,CAAM,GAAK,EACpB,EAAIL,EAAKK,EAAS,CAAC,GAAK,EACxBQ,EAAIb,EAAKK,EAAS,CAAC,GAAK,EACxBS,EAAId,EAAKK,EAAS,CAAC,GAAK,EAE9BT,EAAW,KAAK,CACd,MAAO,KAAK,OAAOY,CAAW,GAAK,SAASA,CAAW,GACvD,MAAOG,EACP,QAASH,EACT,IAAK,CACH,EAAG,KAAK,IAAI,EAAGI,EAAIC,EAAI,CAAC,EACxB,EAAG,KAAK,IAAI,EAAG,EAAIC,EAAI,CAAC,EACxB,MAAOD,EACP,OAAQC,GAEV,cAAe,CACb,EAAG,KAAK,IAAI,EAAGF,EAAIC,EAAI,CAAC,EACxB,EAAG,KAAK,IAAI,EAAG,EAAIC,EAAI,CAAC,EACxB,MAAOD,EACP,OAAQC,GAEX,CACH,CACF,SAAWZ,IAAY,EAGrB,QAASE,EAAI,EAAGA,EAAIH,EAAUG,IAAK,CACjC,IAAMC,EAASD,EAAIF,EACba,EAAKf,EAAKK,CAAM,GAAK,EACrBW,EAAKhB,EAAKK,EAAS,CAAC,GAAK,EACzBY,EAAKjB,EAAKK,EAAS,CAAC,GAAK,EACzBa,EAAKlB,EAAKK,EAAS,CAAC,GAAK,EAE/BT,EAAW,KAAK,CACd,MAAO,KAAK,OAAO,CAAC,GAAK,SACzB,MAAO,EACP,QAAS,EACT,IAAK,CACH,EAAGmB,EACH,EAAGC,EACH,MAAOC,EAAKF,EACZ,OAAQG,EAAKF,GAEf,cAAe,CACb,EAAGD,EACH,EAAGC,EACH,MAAOC,EAAKF,EACZ,OAAQG,EAAKF,GAEhB,CACH,CAGF,OAAOpB,CACT,CAKQ,kBACNA,EACAH,EAAoB,CAEpB,GAAIG,EAAW,SAAW,EAAG,MAAO,CAAA,EAGpC,IAAMuB,EAAS,CAAC,GAAGvB,CAAU,EAAE,KAAK,CAAC,EAAGG,IAAMA,EAAE,MAAQ,EAAE,KAAK,EACzDqB,EAAwB,CAAA,EACxBC,EAAS,IAAI,MAAMF,EAAO,MAAM,EAAE,KAAK,EAAI,EAEjD,QAASf,EAAI,EAAGA,EAAIe,EAAO,OAAQf,IAAK,CACtC,GAAI,CAACiB,EAAOjB,CAAC,EAAG,SAEhB,IAAMkB,EAAUH,EAAOf,CAAC,EACxBgB,EAAS,KAAKE,CAAO,EAGrB,QAASC,EAAInB,EAAI,EAAGmB,EAAIJ,EAAO,OAAQI,IAAK,CAC1C,GAAI,CAACF,EAAOE,CAAC,EAAG,SAEhB,IAAMC,EAAQL,EAAOI,CAAC,EACtB,GAAID,EAAQ,UAAYE,EAAM,QAAS,SAE3B,KAAK,WAAWF,EAAQ,IAAKE,EAAM,GAAG,EACxC/B,IACR4B,EAAOE,CAAC,EAAI,GAEhB,CACF,CAEA,OAAOH,CACT,CAKQ,WAAWtB,EAAgBC,EAAc,CAC/C,IAAM0B,EAAW,KAAK,IAAI,EACxB,KAAK,IAAI3B,EAAE,EAAIA,EAAE,MAAOC,EAAE,EAAIA,EAAE,KAAK,EAAI,KAAK,IAAID,EAAE,EAAGC,EAAE,CAAC,CAAC,EAEvD2B,EAAW,KAAK,IAAI,EACxB,KAAK,IAAI5B,EAAE,EAAIA,EAAE,OAAQC,EAAE,EAAIA,EAAE,MAAM,EAAI,KAAK,IAAID,EAAE,EAAGC,EAAE,CAAC,CAAC,EAGzD4B,EAAeF,EAAWC,EAC1BE,EAAQ9B,EAAE,MAAQA,EAAE,OACpB+B,EAAQ9B,EAAE,MAAQA,EAAE,OACpB+B,EAAQF,EAAQC,EAAQF,EAE9B,OAAOG,EAAQ,EAAIH,EAAeG,EAAQ,CAC5C,GCxOI,IAAOC,GAAP,cAAkDC,CAAgE,CAItH,YAAYC,EAAuB,CACjC,MAAMA,GAAU,CACd,KAAM,+BACN,MAAO,UACR,EAPKC,EAAA,0BACAA,EAAA,iBAA8B,MASpC,KAAK,kBAAoB,IAAIC,EAAkB,CAC7C,WAAY,KACZ,MAAO,GACP,KAAM,IACN,UAAW,IACX,YAAa,GACd,CACH,CAKA,aAAaC,EAAoB,CAC/B,KAAK,UAAYA,CACnB,CAKU,MAAM,WAAWC,EAAgC,CACzD,IAAMC,EAAS,MAAM,QAAQD,CAAK,EAAIA,EAAQ,CAACA,CAAK,EAE9CE,EAAU,MAAM,QAAQ,IAC5BD,EAAO,IAAIE,GAAS,KAAK,kBAAkB,QAAQA,CAAK,CAAC,CAAC,EAI5D,GAAID,EAAQ,SAAW,EAAG,CACxB,IAAME,EAAIF,EAAQ,CAAC,EACnB,MAAO,CAAC,IAAIG,EACVD,EAAE,eAAc,EAChB,CAAC,EAAG,GAAGA,EAAE,KAAK,EACd,SAAS,CACV,CACH,CAGA,OAAOF,CACT,CAKU,MAAM,YACdI,EACAC,EAAyB,CAGzB,IAAMC,GADOD,GAAyB,CAAA,GACR,kBAAoB,GAElD,GAAI,CAACD,EAAQ,CAAC,EACZ,MAAO,CAAE,KAAM,EAAE,EAGnB,IAAMG,EAAaH,EAAQ,CAAC,EAAE,eAAc,EACtCI,EAAQJ,EAAQ,CAAC,EAAE,MAGnBK,EAAO,KAAK,aAAaF,EAAYC,CAAK,EAE1CE,EAAoB,CAAE,KAAAD,CAAI,EAGhC,OAAIH,IACFI,EAAO,OAAS,KAAK,kBAAkBH,EAAYC,EAAOC,CAAI,GAGzDC,CACT,CAKQ,aAAaC,EAAoBH,EAAwB,CAE/D,IAAMI,EAASJ,EAAM,CAAC,GAAKG,EAAK,OAC1BE,EAAYL,EAAM,CAAC,GAAK,EAExBM,EAAqB,CAAA,EAE3B,GAAID,EAAY,EAEd,QAASE,EAAI,EAAGA,EAAIH,EAAQG,IAAK,CAC/B,IAAMC,EAASD,EAAIF,EACfI,EAAS,EACTC,EAASP,EAAKK,CAAM,GAAK,KAE7B,QAASG,EAAI,EAAGA,EAAIN,EAAWM,KACxBR,EAAKK,EAASG,CAAC,GAAK,MAAaD,IACpCA,EAASP,EAAKK,EAASG,CAAC,GAAK,KAC7BF,EAASE,GAGbL,EAAS,KAAKG,CAAM,CACtB,KAGA,SAASF,EAAI,EAAGA,EAAIJ,EAAK,OAAQI,IAC/BD,EAAS,KAAK,KAAK,MAAMH,EAAKI,CAAC,GAAK,CAAC,CAAC,EAK1C,OAAI,KAAK,UACA,KAAK,UAAU,OAAOD,EAAU,EAAI,EAItCA,EAAS,KAAK,GAAG,CAC1B,CAKQ,kBACNM,EACAC,EACAZ,EAAY,CAIZ,IAAMa,EAAQb,EAAK,MAAM,KAAK,EAAE,OAAOc,GAAKA,EAAE,OAAS,CAAC,EAClDC,EAA2B,CAAA,EAE3BC,EAAiB,IACnBC,EAAY,GACZC,EAAa,EAEjB,QAASZ,EAAI,EAAGA,EAAIO,EAAM,OAAQP,IAIhC,GAHAW,IAAcA,EAAY,IAAM,IAAMJ,EAAMP,CAAC,GAGxCA,EAAI,GAAK,IAAM,GAAKA,IAAMO,EAAM,OAAS,EAAG,CAC/C,IAAMM,EAAWF,EAAU,MAAM,KAAK,EAAE,OAASD,EACjDD,EAAO,KAAK,CACV,KAAME,EACN,MAAOC,EACP,IAAKA,EAAaC,EACnB,EACDD,EAAaA,EAAaC,EAC1BF,EAAY,EACd,CAGF,OAAOF,CACT,CAKA,MAAM,iBACJvB,EACAI,EAAsB,CAAA,EAAE,CAExB,IAAMwB,EAAgBxB,EAAQ,eAAiB,GACzCyB,EAAezB,EAAQ,cAAgB,EAIvC0B,GADY,MAAM,KAAK,kBAAkB,WAAW9B,CAAK,GACnC,eAAc,EACpC+B,EAAa,KAEbC,EAAeJ,EAAgBG,EAC/BE,EAAiBJ,EAAeE,EAChCG,EAAcF,EAAeC,EAE7BV,EAAsB,CAAA,EAE5B,QAASY,EAAQ,EAAGA,EAAQL,EAAU,OAAQK,GAASD,EAAa,CAClE,IAAME,EAAM,KAAK,IAAID,EAAQH,EAAcF,EAAU,MAAM,EACrDO,EAAaP,EAAU,MAAMK,EAAOC,CAAG,EAEvCE,EAAc,MAAM,KAAK,IAC7B,IAAI,aAAaD,CAAU,EAC3BjC,CAAO,EAIT,GAAIkC,EAAY,OAAQ,CACtB,IAAMC,EAAaJ,EAAQJ,EAC3BO,EAAY,OAASA,EAAY,OAAO,IAAIE,IAAM,CAChD,GAAGA,EACH,MAAOA,EAAE,MAAQD,EACjB,IAAKC,EAAE,IAAMD,GACb,CACJ,CAEAhB,EAAO,KAAKe,CAAW,CACzB,CAGA,IAAMG,EAAalB,EAAO,IAAIiB,GAAKA,EAAE,IAAI,EAAE,KAAK,GAAG,EAC7CE,EAAenB,EAAO,QAAQiB,GAAKA,EAAE,QAAU,CAAA,CAAE,EAEvD,MAAO,CACL,KAAMC,EACN,OAAQC,EAEZ,GCvOI,IAAOC,GAAP,cAA8CC,CAGnD,CAIC,YAAYC,EAAuB,CACjC,MAAMA,GAAU,CACd,KAAM,2BACN,MAAO,UACR,EAPKC,EAAA,iBAA8B,MAC9BA,EAAA,0BAA6B,8BAOrC,CAKA,aAAaC,EAAoB,CAC/B,KAAK,UAAYA,CACnB,CAKA,MAAM,SACJC,EACAC,EACAC,EAAuC,CAEvC,OAAO,KAAK,IAAI,CAAE,KAAAF,EAAM,gBAAAC,CAAe,EAAIC,CAAO,CACpD,CAKS,MAAM,IACbC,EACAD,EAAyB,CAEzB,MAAM,KAAK,WAAU,EAErB,GAAM,CAAE,KAAAF,EAAM,gBAAAC,CAAe,EAAKE,EAC5BC,EAAOF,GAA4C,CAAA,EACnDG,EAAQ,MAAM,QAAQL,CAAI,EAAIA,EAAO,CAACA,CAAI,EAC1CM,EAAWF,EAAK,oBAAsB,KAAK,mBAC3CG,EAAaH,EAAK,YAAc,GAEhCI,EAAU,MAAM,QAAQ,IAC5BH,EAAM,IAAII,GAAK,KAAK,eAAeA,EAAGR,EAAiBK,EAAUC,CAAU,CAAC,CAAC,EAG/E,OAAO,MAAM,QAAQP,CAAI,EAAIQ,EAAUA,EAAQ,CAAC,CAClD,CAKQ,MAAM,eACZR,EACAC,EACAK,EACAC,EAAmB,CAEnB,IAAMG,EAAY,YAAY,IAAG,EAG3BC,EAAaV,EAAgB,IAAIW,GACrCN,EAAS,QAAQ,UAAWM,CAAK,CAAC,EAI9BC,EAAmB,CAAA,EAEzB,QAAWC,KAAcH,EAAY,CACnC,IAAMI,EAAQ,MAAM,KAAK,gBAAgBf,EAAMc,CAAU,EACzDD,EAAO,KAAKE,CAAK,CACnB,CAGA,IAAIC,EAEJ,GAAIT,EAEFS,EAAmBH,EAAO,IAAII,GAAK,GAAK,EAAI,KAAK,IAAI,CAACA,CAAC,EAAE,MACpD,CAEL,IAAMC,EAAS,IAAIC,EAAe,IAAI,aAAaN,CAAM,EAAG,CAACA,EAAO,MAAM,EAAG,SAAS,EACtFG,EAAmB,MAAM,KAAKI,EAAQF,CAAM,EAAE,eAAc,CAAE,CAChE,CAGA,IAAMG,EAAUpB,EAAgB,IAAI,CAACW,EAAOU,KAAO,CACjD,MAAAV,EACA,MAAOI,EAAiBM,CAAC,GAAK,GAC9B,EACF,OAAAD,EAAQ,KAAK,CAACE,EAAGC,IAAMA,EAAE,MAAQD,EAAE,KAAK,EAEjC,CACL,SAAUvB,EACV,OAAQqB,EAAQ,IAAIC,GAAKA,EAAE,KAAK,EAChC,OAAQD,EAAQ,IAAIC,GAAKA,EAAE,KAAK,EAChC,eAAgB,YAAY,IAAG,EAAKZ,EAExC,CAKQ,MAAM,gBAAgBe,EAAiBX,EAAkB,CAC/D,GAAI,CAAC,KAAK,UACR,MAAM,IAAI,MAAM,+CAA+C,EAIjE,YAAK,UAAU,OAAOW,EAAS,CAC7B,SAAUX,EACV,iBAAkB,GAClB,UAAW,IACX,WAAY,GACZ,oBAAqB,GACrB,mBAAoB,GACrB,EAMM,KAAK,OAAM,CACpB,CAKU,MAAM,WACdX,EAAoB,CAEpB,GAAM,CAAE,KAAAH,EAAM,gBAAAC,CAAe,EAAKE,EAG5BuB,EAAY,MAAM,QAAQ1B,CAAI,EAAIA,EAAK,CAAC,GAAK,GAAKA,EAClD2B,EAAa1B,EAAgB,CAAC,GAAK,GAEzC,GAAI,CAAC,KAAK,UACR,MAAO,CAAC,IAAIkB,EAAe,IAAI,aAAa,CAAC,CAAC,CAAC,EAAG,CAAC,CAAC,EAAG,SAAS,CAAC,EAGnE,IAAMS,EAAU,KAAK,UAAU,OAAOF,EAAW,CAC/C,SAAU,KAAK,mBAAmB,QAAQ,UAAWC,CAAU,EAC/D,iBAAkB,GAClB,UAAW,IACZ,EAED,MAAO,CAAC,IAAIR,EACV,cAAc,KAAKS,EAAQ,SAAS,IAAIC,GAAM,OAAOA,CAAE,CAAC,CAAC,EACzD,CAAC,EAAGD,EAAQ,SAAS,MAAM,EAC3B,OAAO,CACR,CACH,CAKU,MAAM,YACdE,EACAC,EAA0B,CAE1B,MAAO,CACL,SAAU,GACV,OAAQ,CAAA,EACR,OAAQ,CAAA,EAEZ,GCtKI,IAAOC,GAAP,cAAyCC,CAG9C,CAGC,YAAYC,EAAuB,CACjC,MAAMA,GAAU,CACd,KAAM,qBACN,MAAO,UACR,EANKC,EAAA,iBAA8B,KAOtC,CAKA,aAAaC,EAAoB,CAC/B,KAAK,UAAYA,CACnB,CAKS,MAAM,IACbC,EACAC,EAAkC,CAElC,MAAM,KAAK,WAAU,EAErB,IAAMC,EAAS,MAAM,QAAQF,CAAK,EAAIA,EAAQ,CAACA,CAAK,EAC9CG,EAAU,MAAM,QAAQ,IAC5BD,EAAO,IAAI,GAAK,KAAK,eAAe,EAAGD,GAAW,CAAA,CAAE,CAAC,CAAC,EAGxD,OAAO,MAAM,QAAQD,CAAK,EAAIG,EAAUA,EAAQ,CAAC,CACnD,CAKQ,MAAM,eACZH,EACAC,EAAiC,CAEjC,IAAMG,EAAY,YAAY,IAAG,EAEjC,GAAI,CAAC,KAAK,UACR,MAAM,IAAI,MAAM,+CAA+C,EAGjE,GAAM,CAAE,SAAAC,EAAU,QAAAC,CAAO,EAAKN,EACxB,CACJ,gBAAAO,EAAkB,EAAE,EAClBN,EAGEO,EAAU,KAAK,UAAU,OAAOH,EAAU,CAC9C,SAAUC,EACV,iBAAkB,GAClB,UAAW,IACX,WAAY,GACZ,oBAAqB,GACrB,mBAAoB,GACrB,EAGKG,EAAS,KAAK,eAClBH,EACAD,EACAG,EAAQ,SACRD,CAAe,EAGjB,MAAO,CACL,OAAQE,EAAO,KACf,MAAOA,EAAO,MACd,MAAOA,EAAO,MACd,IAAKA,EAAO,IACZ,eAAgB,YAAY,IAAG,EAAKL,EAExC,CAKQ,eACNE,EACAD,EACAK,EACAC,EAAiB,CAMjB,IAAMC,EAAgBP,EAAS,YAAW,EAAG,MAAM,KAAK,EAClDQ,EAAmBP,EAAQ,MAAM,QAAQ,EAAE,OAAOQ,GAAKA,EAAE,KAAI,CAAE,EAEjEC,EAAe,GACfC,EAAY,EACZC,EAAY,EAEhB,QAAWC,KAAYL,EAAkB,CACvC,IAAMM,EAAQD,EAAS,YAAW,EAAG,MAAM,KAAK,EAC5CE,EAAQ,EAEZ,QAAWC,KAAST,EACdO,EAAM,KAAKG,GAAKA,EAAE,SAASD,CAAK,GAAKA,EAAM,SAASC,CAAC,CAAC,IACxDF,GAAS,GAITA,EAAQJ,IACVA,EAAYI,EACZL,EAAeG,EAAS,KAAI,EAC5BD,EAAYX,EAAQ,QAAQY,EAAS,KAAI,CAAE,EAE/C,CAGA,IAAMC,EAAQJ,EAAa,MAAM,KAAK,EAClCI,EAAM,OAASR,IACjBI,EAAeI,EAAM,MAAM,EAAGR,CAAS,EAAE,KAAK,GAAG,GAGnD,IAAMY,EAAkBX,EAAc,OAAS,EAC3CI,EAAYJ,EAAc,OAC1B,EAEJ,MAAO,CACL,KAAMG,GAAgB,kBACtB,MAAO,KAAK,IAAIQ,EAAiB,CAAG,EACpC,MAAON,GAAa,EAAIA,EAAY,EACpC,IAAKA,GAAa,EAAIA,EAAYF,EAAa,OAAS,EAE5D,CAKU,MAAM,WAAWf,EAA0B,CACnD,GAAI,CAAC,KAAK,UACR,MAAO,CAAC,IAAIwB,EAAe,IAAI,aAAa,CAAC,CAAC,CAAC,EAAG,CAAC,CAAC,EAAG,SAAS,CAAC,EAGnE,IAAMC,EAAU,MAAM,QAAQzB,CAAK,EAAIA,EAAM,CAAC,EAAKA,EAE7CQ,EAAU,KAAK,UAAU,OAAOiB,EAAQ,SAAU,CACtD,SAAUA,EAAQ,QAClB,iBAAkB,GAClB,UAAW,IACX,WAAY,GACZ,oBAAqB,GACrB,mBAAoB,GACrB,EAED,MAAO,CACL,IAAID,EACF,cAAc,KAAKhB,EAAQ,SAAS,IAAIkB,GAAM,OAAOA,CAAE,CAAC,CAAC,EACzD,CAAC,EAAGlB,EAAQ,SAAS,MAAM,EAC3B,OAAO,EAET,IAAIgB,EACF,cAAc,KAAKhB,EAAQ,cAAc,IAAImB,GAAK,OAAOA,CAAC,CAAC,CAAC,EAC5D,CAAC,EAAGnB,EAAQ,cAAc,MAAM,EAChC,OAAO,EAGb,CAKU,MAAM,YACdoB,EACAC,EAA0B,CAG1B,GAAID,EAAQ,OAAS,EACnB,MAAO,CAAE,OAAQ,GAAI,MAAO,EAAG,MAAO,EAAG,IAAK,CAAC,EAGjD,IAAME,EAAcF,EAAQ,CAAC,EAAG,eAAc,EACxCG,EAAYH,EAAQ,CAAC,EAAG,eAAc,EACtCI,EAASF,EAAY,OAGrBG,EAAaC,EAAQ,IAAIV,EAAeM,EAAa,CAACE,CAAM,EAAG,SAAS,CAAC,EAAE,eAAc,EACzFG,EAAWD,EAAQ,IAAIV,EAAeO,EAAW,CAACC,CAAM,EAAG,SAAS,CAAC,EAAE,eAAc,EAGvFf,EAAY,EACZmB,EAAU,EACVpB,EAAY,EAEhB,QAASqB,EAAQ,EAAGA,EAAQL,EAAQK,IAClC,QAASC,EAAMD,EAAOC,EAAM,KAAK,IAAID,EAAQ,GAAIL,CAAM,EAAGM,IAAO,CAC/D,IAAMlB,GAASa,EAAWI,CAAK,GAAK,IAAMF,EAASG,CAAG,GAAK,GACvDlB,EAAQJ,IACVA,EAAYI,EACZH,EAAYoB,EACZD,EAAUE,EAEd,CAGF,MAAO,CACL,OAAQ,GACR,MAAOtB,EACP,MAAOC,EACP,IAAKmB,EAET,GChIF,eAAsBG,GACpBC,EACAC,EAAgC,CAEhC,IAAMC,EAAyB,CAC7B,KAAMF,EACN,MAAOC,GAAS,OAAS,UACzB,QAASA,GAAS,QAClB,MAAOA,GAAS,OAAS,GACzB,aAAcA,GAAS,cAKrBE,EAEJ,OAAQH,EAAM,CACZ,IAAK,sBACHG,EAAmB,IAAIC,EAA2BF,EAAQD,GAAS,MAAM,EACzE,MACF,IAAK,qBACHE,EAAmB,IAAIE,EAA0BH,CAAM,EACvD,MACF,IAAK,qBACHC,EAAmB,IAAIG,EAA0BJ,CAAM,EACvD,MACF,IAAK,uBACHC,EAAmB,IAAII,EAA4BL,EAAQD,GAAS,MAAM,EAC1E,MACF,IAAK,kBACHE,EAAmB,IAAIK,GAAuBN,CAAM,EACpD,MACF,IAAK,mBACHC,EAAmB,IAAIM,GAAwBP,EAAQD,GAAS,MAAM,EACtE,MACF,IAAK,+BACHE,EAAmB,IAAIO,GAAmCR,CAAM,EAChE,MACF,IAAK,2BACHC,EAAmB,IAAIQ,GAA+BT,CAAM,EAC5D,MACF,IAAK,qBACHC,EAAmB,IAAIS,GAA0BV,CAAM,EACvD,MACF,QACE,MAAM,IAAI,MAAM,0BAA0BF,CAAI,EAAE,CACpD,CAGA,aAAMG,EAAiB,WAAU,EAE1BA,CACT,CAKA,eAAsBU,GACpBC,EACAb,EAAgC,CAEhC,IAAMc,EAAY,MAAM,QAAQ,IAC9BD,EAAM,IAAId,GAAQD,GAASC,EAAMC,CAAO,CAAC,CAAC,EAGtCe,EAA4D,CAAA,EAElE,QAASC,EAAI,EAAGA,EAAIH,EAAM,OAAQG,IAAK,CACrC,IAAMjB,EAAOc,EAAMG,CAAC,EACpBD,EAAOhB,CAAiB,EAAIe,EAAUE,CAAC,CACzC,CAEA,OAAOD,CACT,CC/LAE,KChCAC,KAqFA,IAAMC,GAAmB,yBACnBC,GAAmB,OAKnBC,GAAmB,CACvB,aACA,uBACA,kBACA,mBACA,kBACA,kBACA,6BAUF,SAASC,GACPC,EACAC,EACAC,EAAsB,CAAA,EAAE,CAExB,IAAMC,EAAWD,EAAQ,UAAYN,GAC/BQ,EAAWF,EAAQ,UAAYL,GAC/BQ,EAAYH,EAAQ,UAAY,GAAGA,EAAQ,SAAS,IAAM,GAEhE,MAAO,GAAGC,CAAQ,IAAIH,CAAO,YAAYI,CAAQ,IAAIC,CAAS,GAAGJ,CAAQ,EAC3E,CAKA,eAAeK,GAAcC,EAAaC,EAAc,CACtD,IAAMC,EAAuB,CAAA,EAC7B,OAAID,IACFC,EAAQ,cAAmB,UAAUD,CAAK,IAG3B,MAAM,MAAMD,EAAK,CAAE,QAAAE,CAAO,CAAE,CAE/C,CAKA,eAAeC,GACbV,EACAC,EACAC,EAAsB,CAAA,EAAE,CAExB,IAAMK,EAAMR,GAAaC,EAASC,EAAUC,CAAO,EAEnD,GAAI,CACF,IAAMS,EAAW,MAAML,GAAcC,EAAKL,EAAQ,KAAK,EAEvD,OAAOS,EAAS,IAAMA,EAAS,SAAW,GAC5C,MAAQ,CACN,MAAO,EACT,CACF,CAKA,eAAeC,GACbZ,EACAE,EAAsB,CAAA,EAAE,CAGxB,QAAWD,KAAYH,GACrB,GAAI,MAAMY,GAAWV,EAASC,EAAUC,CAAO,EAC7C,OAAOD,EAIX,OAAO,IACT,CAKA,eAAsBY,GACpBb,EACAC,EACAC,EAAsB,CAAA,EAAE,CAExB,IAAMK,EAAMR,GAAaC,EAASC,EAAUC,CAAO,EAGnD,OAAOY,EAAcP,EAAK,CACxB,MAAOL,EAAQ,OAAS,GACxB,cAAeA,EAAQ,eAAiB,GACxC,WAAYA,EAAQ,WAAca,GAAY,CAC5Cb,EAAQ,WAAY,CAClB,KAAMD,EACN,UAAW,EACX,WAAY,EACZ,aAAcc,EACd,gBAAiBA,EAAS,QAC3B,CACH,EAAI,OACL,CACH,CAKA,eAAsBC,GACpBhB,EACAC,EACAC,EAAsB,CAAA,EAAE,CAExB,IAAMK,EAAMR,GAAaC,EAASC,EAAUC,CAAO,EAGnD,GAAIA,EAAQ,QAAU,IAAS,CAACA,EAAQ,eACvB,MAAMe,GAAcV,CAAG,EAC1B,CACV,IAAMW,EAAO,MAAMJ,EAAcP,EAAK,CAAE,MAAO,EAAI,CAAE,EAC/CY,EAAO,IAAI,YAAW,EAAG,OAAOD,CAAI,EAC1C,OAAO,KAAK,MAAMC,CAAI,CACxB,CAIF,IAAMR,EAAW,MAAML,GAAcC,EAAKL,EAAQ,KAAK,EAEvD,GAAI,CAACS,EAAS,GACZ,MAAM,IAAIS,EACR,sBAAsBnB,CAAQ,SAASD,CAAO,KAAKW,EAAS,MAAM,GAClEU,EAAW,eAAe,EAI9B,OAAOV,EAAS,KAAI,CACtB,CAKA,eAAsBW,GACpBtB,EACAE,EAAsB,CAAA,EAAE,CAExB,IAAMK,EAAMR,GAAaC,EAAS,iBAAkBE,CAAO,EAC3D,OAAOqB,EAAU,QAAQhB,CAAG,CAC9B,CAKA,eAAsBiB,GACpBxB,EACAE,EAAsB,CAAA,EAAE,CAExB,OAAOc,GAA0BhB,EAAS,cAAeE,CAAO,CAClE,CAKA,eAAsBuB,GACpBzB,EACAE,EAAsB,CAAA,EAAE,CAExB,IAAMwB,EAA8B,CAAA,EAEhCC,EAAc,EAEZC,EAAiB,CACrBC,EACAd,IACE,CACF,GAAIb,EAAQ,WAAY,CACtB,IAAM4B,EAAgBH,EAAc,EAAc,IAC5CI,EAAgBhB,EAAS,QAAU,EAEzCb,EAAQ,WAAW,CACjB,KAAA2B,EACA,UAAWF,EAAc,EACzB,WAAY,EACZ,aAAcZ,EACd,gBAAiBe,EAAeC,EACjC,CACH,CACF,EAGA,QAAQ,IAAI,mCAA4B/B,CAAO,KAAK,EACpD,IAAMgC,EAAY,MAAMpB,GAAcZ,EAASE,CAAO,EAEtD,GAAI,CAAC8B,EACH,MAAM,IAAIZ,EACR,0BAA0BpB,CAAO,8CACjCqB,EAAW,gBACX,CAAE,QAAArB,EAAS,WAAYF,EAAgB,CAAE,EAI7C4B,EAAM,MAAQM,EACd,QAAQ,IAAI,gCAAyBA,CAAS,EAAE,EAEhD,IAAMC,EAAY,MAAMpB,GAAab,EAASgC,EAAW,CACvD,GAAG9B,EACH,WAAagC,GAAMN,EAAeI,EAAWE,EAAE,YAAY,EAC5D,EAEDP,EAAc,EAGd,IAAIQ,EACJ,GAAI,CACF,QAAQ,IAAI,oCAA6B,EACzCT,EAAM,UAAY,iBAClBS,EAAY,MAAMb,GAAkBtB,EAASE,CAAO,EACpD,QAAQ,IAAI,yBAAoB,CAClC,MAAgB,CACd,QAAQ,KAAK,uCAA6BF,CAAO,EAAE,CACrD,CAEA2B,EAAc,EAGd,IAAIS,EACJ,GAAI,CACF,QAAQ,IAAI,oCAA0B,EACtCV,EAAM,OAAS,cACfU,EAAS,MAAMZ,GAAexB,EAASE,CAAO,EAC9C,QAAQ,IAAI,sBAAiB,CAC/B,MAAgB,CACd,QAAQ,KAAK,oCAA0BF,CAAO,EAAE,CAClD,CAEA,OAAA2B,EAAc,EAEVzB,EAAQ,YACVA,EAAQ,WAAW,CACjB,KAAM,WACN,UAAW,EACX,WAAY,EACZ,aAAc,CAAE,OAAQ,EAAG,MAAO,EAAG,QAAS,IAAK,MAAO,EAAG,IAAK,CAAC,EACnE,gBAAiB,IAClB,EAGH,QAAQ,IAAI,mCAA8BF,CAAO,EAAE,EAE5C,CACL,QAAAA,EACA,UAAAiC,EACA,UAAAE,EACA,OAAAC,EACA,MAAAV,EAEJ,CAmBA,eAAsBW,GACpBrC,EACAE,EAAsB,CAAA,EAAE,CAExB,OAAOuB,GAAczB,EAASE,CAAO,CACvC,CAKA,eAAsBoC,GACpBtC,EACAE,EAAsB,CAAA,EAAE,CAExB,GAAI,CAGF,OADkB,MAAMU,GAAcZ,EAASE,CAAO,IACjC,IACvB,MAAQ,CACN,MAAO,EACT,CACF,CAKA,eAAsBqC,GACpBvC,EACAE,EAAsB,CAAA,EAAE,CAQxB,GAAM,CAACsC,EAAUC,EAAcL,CAAM,EAAI,MAAM,QAAQ,IAAI,CACzDxB,GAAcZ,EAASE,CAAO,EAC9BQ,GAAWV,EAAS,iBAAkBE,CAAO,EAC7CsB,GAAexB,EAASE,CAAO,EAAE,MAAM,IAAG,EAAY,EACvD,EAED,MAAO,CACL,QAASsC,IAAa,KACtB,SAAUA,GAAY,OACtB,aAAAC,EACA,UAAWL,IAAW,OACtB,OAAAA,EAEJ,CASO,IAAMM,GAAiB,CAE5B,qBAAsB,yDACtB,sBAAuB,yDAGvB,qBAAsB,0BACtB,sBAAuB,0BAGvB,qBAAsB,+CAGtB,IAAO,uBACP,uBAAwB,uBAGxB,kBAAmB,cAGnB,oBAAqB,kBACrB,oBAAqB,kBAGrB,cAAiB,4BAGjB,YAAa,2BAGb,uBAAwB,8BAGxB,mBAAoB,wBAGpB,qBAAsB,4CAGtB,2BAA4B,iCAG5B,+BAAgC,yBAGhC,iBAAkB,uBAQd,SAAUC,GAAgBC,EAAsB,CACpD,OAAOF,GAAeE,CAAI,CAC5B,CAUA,eAAsBC,GACpBD,EACA1C,EAAsB,CAAA,EAAE,CAExB,IAAMF,EAAU2C,GAAgBC,CAAI,EACpC,OAAOnB,GAAczB,EAASE,CAAO,CACvC,CCxbA,eAAsB4C,GACpBC,EACAC,EAA4B,CAG5B,IAAMC,EAAYF,aAAiB,YAC/BA,EACA,MAAMG,GAAaH,CAAK,EAEtBI,EAAeF,EAAU,WAG3BG,EACAC,EAAkB,EAClBC,EAAgB,EAEpB,OAAQN,EAAQ,OAAQ,CACtB,IAAK,QACF,CAAE,KAAMI,EAAe,gBAAAC,EAAiB,cAAAC,CAAa,EACpDC,GAAaN,EAAWD,CAAO,GACjC,MACF,IAAK,SACF,CAAE,KAAMI,EAAe,gBAAAC,EAAiB,cAAAC,CAAa,EACpDE,GAAcP,EAAWD,CAAO,GAClC,MACF,IAAK,WACF,CAAE,KAAMI,EAAe,gBAAAC,EAAiB,cAAAC,CAAa,EACpDG,GAAgBR,EAAWD,CAAO,GACpC,MACF,IAAK,QACF,CAAE,KAAMI,EAAe,gBAAAC,EAAiB,cAAAC,CAAa,EACpDI,GAAaT,EAAWD,CAAO,GACjC,MACF,QACEI,EAAgBH,CACpB,CAEA,MAAO,CACL,UAAWG,EACX,aAAAD,EACA,cAAeC,EAAc,WAC7B,iBAAkBD,EAAeC,EAAc,WAC/C,MAAO,CACL,gBAAAC,EACA,cAAAC,GAGN,CAKA,eAAeJ,GAAaS,EAAmB,CAE7C,OAAO,IAAI,YAAY,CAAC,CAC1B,CAKA,SAASJ,GACPK,EACAC,EAA6B,CAG7B,IAAMC,EAAQ,IAAI,aAAaF,CAAI,EAC7BG,EAAS,IAAI,UAAUD,EAAM,MAAM,EAGrCE,EAAM,EACV,QAAS,EAAI,EAAG,EAAIF,EAAM,OAAQ,IAAK,CACrC,IAAMG,EAAM,KAAK,IAAIH,EAAM,CAAC,GAAK,CAAC,EAC9BG,EAAMD,IAAKA,EAAMC,EACvB,CACA,IAAMC,EAAQF,EAAM,IAGpB,QAAS,EAAI,EAAG,EAAIF,EAAM,OAAQ,IAChCC,EAAO,CAAC,EAAI,KAAK,OAAOD,EAAM,CAAC,GAAK,GAAKI,CAAK,EAGhD,MAAO,CACL,KAAMH,EAAO,OACb,gBAAiB,EACjB,cAAe,EAEnB,CAKA,SAASP,GACPI,EACAC,EAA6B,CAE7B,IAAMC,EAAQ,IAAI,aAAaF,CAAI,EAC7BG,EAAS,IAAI,WAAWD,EAAM,MAAM,EAGtCK,EAAM,IAAUH,EAAM,KAC1B,QAASI,EAAI,EAAGA,EAAIN,EAAM,OAAQM,IAAK,CACrC,IAAMC,EAAMP,EAAMM,CAAC,GAAK,EACpBC,EAAMF,IAAKA,EAAME,GACjBA,EAAML,IAAKA,EAAMK,EACvB,CACA,IAAMH,GAASF,EAAMG,GAAO,IAG5B,QAASC,EAAI,EAAGA,EAAIN,EAAM,OAAQM,IAChCL,EAAOK,CAAC,EAAI,KAAK,QAAQN,EAAMM,CAAC,GAAK,GAAKD,GAAOD,CAAK,EAGxD,MAAO,CACL,KAAMH,EAAO,OACb,gBAAiB,EACjB,cAAe,EAEnB,CAKA,SAASN,GACPG,EACAC,EAA6B,CAE7B,IAAMC,EAAQ,IAAI,aAAaF,CAAI,EAC7BG,EAAS,IAAI,YAAYD,EAAM,MAAM,EAG3C,QAASM,EAAI,EAAGA,EAAIN,EAAM,OAAQM,IAChCL,EAAOK,CAAC,EAAIE,GAAiBR,EAAMM,CAAC,GAAK,CAAC,EAG5C,MAAO,CACL,KAAML,EAAO,OACb,gBAAiB,EACjB,cAAe,EAEnB,CAKA,SAASL,GACPE,EACAC,EAA6B,CAE7B,IAAMC,EAAQ,IAAI,aAAaF,CAAI,EAE7BG,EAAS,IAAI,WAAW,KAAK,KAAKD,EAAM,OAAS,CAAC,CAAC,EAGrDE,EAAM,EACV,QAAS,EAAI,EAAG,EAAIF,EAAM,OAAQ,IAAK,CACrC,IAAMG,EAAM,KAAK,IAAIH,EAAM,CAAC,GAAK,CAAC,EAC9BG,EAAMD,IAAKA,EAAMC,EACvB,CACA,IAAMC,EAAQF,EAAM,EAGpB,QAAS,EAAI,EAAG,EAAIF,EAAM,OAAQ,GAAK,EAAG,CACxC,IAAMS,EAAO,KAAK,OAAOT,EAAM,CAAC,GAAK,GAAKI,CAAK,EAAI,EAC7CM,EAAO,KAAK,OAAOV,EAAM,EAAI,CAAC,GAAK,GAAKI,CAAK,EAAI,EACvDH,EAAO,EAAI,CAAC,GAAMQ,EAAO,KAAQ,EAAMC,EAAO,EAChD,CAEA,MAAO,CACL,KAAMT,EAAO,OACb,gBAAiB,EACjB,cAAe,EAEnB,CAKA,SAASO,GAAiBG,EAAa,CACrC,IAAMC,EAAY,IAAI,aAAa,CAAC,EAC9BC,EAAY,IAAI,WAAWD,EAAU,MAAM,EAEjDA,EAAU,CAAC,EAAID,EACf,IAAMG,EAAID,EAAU,CAAC,GAAK,EAEtBE,EAAQD,GAAK,GAAM,MACnBE,EAAKF,GAAK,GAAM,KACdG,EAAKH,GAAK,GAAM,IAEtB,OAAIG,EAAI,IAECF,EAGLE,EAAI,KAENF,GAAQ,MACRA,IAAUE,IAAM,IAAO,EAAI,IAAOH,EAAI,QAC/BC,GAGLE,EAAI,KAEND,GAAK,KACLD,IAASC,GAAM,IAAMC,IAAQD,GAAM,IAAMC,EAAM,GACxCF,IAGTA,GAAUE,EAAI,KAAQ,GAAOD,GAAK,EAClCD,GAAQC,EAAI,EACLD,EACT,CAmCA,eAAsBG,GACpBjC,EACAC,EAAuB,CAEvB,IAAMC,EAAYF,aAAiB,YAC/BA,EACA,MAAMG,GAAaH,CAAK,EAEtBkC,EAAU,IAAI,aAAahC,CAAS,EACpCiC,EAAQD,EAAQ,OAIhBE,EAAS,CAAC,GADGF,EAAQ,IAAI,KAAK,GAAG,CACV,EAAE,KAAK,CAACG,EAAGC,IAAMD,EAAIC,CAAC,EAC7CC,EAAe,KAAK,MAAMtC,EAAQ,SAAWmC,EAAO,MAAM,EAC1DI,EAAYJ,EAAOG,CAAY,GAAK,EAGtCE,EAAS,EACb,QAASpB,EAAI,EAAGA,EAAIa,EAAQ,OAAQb,IAC9B,KAAK,IAAIa,EAAQb,CAAC,GAAK,CAAC,EAAImB,IAC9BN,EAAQb,CAAC,EAAI,EACboB,KAIJ,MAAO,CACL,UAAWP,EAAQ,OACnB,eAAgBO,EAASN,EACzB,iBAAkBM,EAClB,gBAAiBN,EAErB,CAmCA,eAAsBO,GACpB1C,EAAgC,CAGhC,IAAM2C,EAAO3C,aAAiB,YAC1BA,EAAM,WACNA,EAAM,SAAS,UAEb4C,EAAkB,KAAK,MAAMD,EAAO,CAAC,EAE3C,MAAO,CACL,gBAAiBC,EACjB,UAAWD,EACX,OAAQ,CAAA,EACR,eAAgBC,EAAkB,EAClC,mBAAoB,CAClB,QAASD,EACT,YAAaA,EAAO,GACpB,MAAOA,EAAO,KAGpB,CAuCA,eAAsBE,GACpBC,EACA7C,EAA4B,CAAA,EAAE,CAE9B,GAAM,CACJ,WAAA8C,EAAa,EACb,KAAAC,EAAO,EAAE,EACP/C,EAGJ,QAASoB,EAAI,EAAGA,EAAI0B,EAAY1B,IAC9B,MAAMyB,EAAK,EAIb,IAAMG,EAAkB,CAAA,EACxB,QAAS5B,EAAI,EAAGA,EAAI2B,EAAM3B,IAAK,CAC7B,IAAM6B,EAAQ,YAAY,IAAG,EAC7B,MAAMJ,EAAK,EACXG,EAAM,KAAK,YAAY,IAAG,EAAKC,CAAK,CACtC,CAIA,IAAMC,EADMF,EAAM,OAAO,CAACZ,EAAGC,IAAMD,EAAIC,EAAG,CAAC,EACrBW,EAAM,OACtBG,EAAU,KAAK,IAAI,GAAGH,CAAK,EAC3BI,EAAU,KAAK,IAAI,GAAGJ,CAAK,EAG3BK,EADeL,EAAM,IAAIM,GAAK,KAAK,IAAIA,EAAIJ,EAAS,CAAC,CAAC,EACxB,OAAO,CAACd,EAAGC,IAAMD,EAAIC,EAAG,CAAC,EAAIW,EAAM,OACjEO,EAAS,KAAK,KAAKF,CAAc,EAEvC,MAAO,CACL,QAAAH,EACA,QAAAC,EACA,QAAAC,EACA,OAAAG,EACA,WAAY,IAAOL,EACnB,MAAAF,EAEJ,CASA,eAAsBQ,GACpBzD,EACA0D,EAAkC,CAElC,IAAMxD,EAAYF,aAAiB,YAC/BA,EACA,MAAMG,GAAaH,CAAK,EAE5B,OAAQ0D,EAAQ,CACd,IAAK,OAEH,IAAMC,EAAQ,IAAI,aAAazD,CAAS,EACxC,OAAO,KAAK,UAAU,MAAM,KAAKyD,CAAK,CAAC,EACzC,IAAK,SACL,IAAK,OACL,QACE,OAAOzD,CACX,CACF,CCxLA,eAAsB0D,IAAW,CAC/B,IAAMC,EAAW,MAAMC,GAAoB,EAC3C,OAAO,MAAM,KAAKD,EAAS,OAAM,CAAE,EAAE,KAAKE,GAAKA,CAAC,CAClD,CAKA,eAAsBC,IAAkB,CACtC,IAAMH,EAAW,MAAMC,GAAoB,EAE3C,OAAID,EAAS,IAAI,QAAQ,EAAU,SAC/BA,EAAS,IAAI,OAAO,EAAU,QAC9BA,EAAS,IAAI,MAAM,EAAU,OAE1B,IACT,CAKA,eAAsBI,GACpBC,EAAgB,CAEhB,IAAMC,EAAQ,IAAIC,EAElB,MAAM,QAAQ,IAAIF,EAAO,IAAI,MAAOG,GAAO,CACzC,GAAI,CAAE,MAAMF,EAAM,IAAIE,CAAG,EAAI,CAC3B,IAAMC,EAAW,MAAM,MAAMD,CAAG,EAC5BC,EAAS,IACX,MAAMH,EAAM,IAAIE,EAAKC,CAAQ,CAEjC,CACF,CAAC,CAAC,CACJ,CASO,IAAMC,GAAU,QAKvB,eAAsBC,IAAO,CAK3B,IAAMX,EAAW,MAAMC,GAAoB,EAE3C,MAAO,CACL,QAASS,GACT,SAAU,CACR,OAAQV,EAAS,IAAI,QAAQ,GAAK,GAClC,MAAOA,EAAS,IAAI,OAAO,GAAK,GAChC,KAAMA,EAAS,IAAI,MAAM,GAAK,GAC9B,KAAM,IAER,SAAU,CACR,uBACA,mBACA,oBACA,gBACA,gBAGN",
  "names": ["model_loader_exports", "__export", "cancelPreload", "clearModelCache", "deleteCachedModel", "getCachedModel", "getModelCacheStats", "getPreloadStatus", "getPreloadedModel", "isModelCached", "loadModelData", "preloadModel", "preloadModels", "supportsRangeRequests", "url", "response", "acceptRanges", "contentLength", "etag", "downloadChunk", "start", "end", "timeout", "controller", "timeoutId", "downloadWithResume", "options", "chunkSize", "parallelConnections", "onProgress", "supportsRange", "totalSize", "downloadSimple", "state", "modelCache", "numChunks", "chunks", "i", "pendingChunks", "c", "downloadedSize", "lastProgressTime", "lastDownloadedSize", "reportProgress", "now", "elapsed", "bytesDownloaded", "speed", "remaining", "eta", "downloadQueue", "inProgress", "chunk", "downloadPromise", "data", "result", "offset", "total", "reader", "loaded", "startTime", "done", "value", "cache", "forceDownload", "resumable", "cached", "preloadManager", "urls", "priority", "DB_NAME", "STORE_META", "STORE_CHUNKS", "STORE_STATE", "ModelCache", "PreloadManager", "init_model_loader", "__esmMin", "__publicField", "resolve", "reject", "request", "event", "db", "meta", "tx", "index", "results", "a", "b", "r", "sum", "cursor", "stores", "storeName", "metas", "m", "existing", "promise", "res", "rej", "task", "insertIndex", "u", "t", "error", "EdgeFlowError", "message", "code", "details", "__publicField", "ErrorCodes", "tensorIdCounter", "generateTensorId", "getTypedArrayConstructor", "dtype", "EdgeFlowError", "ErrorCodes", "calculateSize", "shape", "acc", "dim", "validateShape", "i", "EdgeFlowTensor", "_EdgeFlowTensor", "data", "__publicField", "expectedSize", "TypedArrayCtor", "bigIntData", "result", "clonedData", "indices", "flatIndex", "stride", "idx", "value", "newShape", "newSize", "rows", "cols", "j", "tensor", "flatData", "row", "inferredShape", "zeros", "size", "ones", "full", "random", "randn", "u1", "u2", "r", "theta", "arange", "start", "stop", "step", "linspace", "num", "eye", "n", "add", "a", "b", "aData", "bData", "sub", "mul", "div", "matmul", "m", "k1", "k2", "sum", "k", "softmax", "t", "axis", "actualAxis", "max", "relu", "sigmoid", "tanh", "total", "mean", "axisSize", "argmax", "maxIdx", "maxVal", "concat", "tensors", "first", "totalAxisSize", "offset", "Task", "id", "modelId", "priority", "executor", "__publicField", "cancelError", "EdgeFlowError", "ErrorCodes", "reject", "resolve", "err", "PRIORITY_ORDER", "PriorityQueue", "item", "inserted", "i", "currentItem", "index", "removed", "taskIdCounter", "generateTaskId", "DEFAULT_OPTIONS", "InferenceScheduler", "options", "__publicField", "modelId", "queue", "PriorityQueue", "running", "tasksToStart", "task", "error", "hasPending", "executor", "priority", "EdgeFlowError", "ErrorCodes", "Task", "timeout", "timeoutExecutor", "resolve", "reject", "timer", "result", "tasks", "scheduledTasks", "taskId", "cancelled", "stats", "event", "listener", "listeners", "type", "data", "batcher", "globalScheduler", "getScheduler", "setScheduler", "scheduler", "configureScheduler", "DEFAULT_POOL_CONFIG", "_MemoryManager", "config", "__publicField", "tensor", "disposer", "size", "model", "id", "resource", "resourceOrId", "error", "bytesPerElement", "dtype", "obj", "usage", "now", "oldResources", "tensorCount", "modelCount", "maxAge", "potentialLeaks", "event", "listener", "listeners", "type", "data", "MemoryManager", "MemoryScope", "_MemoryScope", "parent", "index", "child", "i", "withMemoryScope", "fn", "scope", "withMemoryScopeSync", "ModelCache", "options", "key", "entry", "oldestKey", "oldestTime", "getMemoryManager", "getMemoryStats", "release", "gc", "runtimeFactories", "runtimeInstances", "RUNTIME_PRIORITY", "_RuntimeManager", "__publicField", "type", "factory", "runtime", "EdgeFlowError", "ErrorCodes", "error", "existing", "results", "event", "listener", "listeners", "data", "RuntimeManager", "modelIdCounter", "generateModelId", "LoadedModelImpl", "metadata", "dispose", "getMemoryManager", "loadModel", "url", "options", "loadModelData", "modelData", "progress", "loadModelFromBuffer", "runInference", "model", "inputs", "getScheduler", "runBatchInference", "batches", "scheduler", "tasks", "task", "getRuntimeManager", "registerRuntime", "getBestRuntime", "getAvailableRuntimes", "GPUBufferUsage", "GPUShaderStage", "WebGPURuntime", "__publicField", "EdgeFlowError", "ErrorCodes", "info", "modelData", "options", "config", "webgpuData", "modelId", "metadata", "i", "o", "model", "LoadedModelImpl", "getMemoryManager", "inputs", "device", "outputs", "outputSpec", "outputSize", "a", "b", "outputBuffer", "stagingBuffer", "outputData", "inputData", "EdgeFlowTensor", "data", "decoder", "text", "jsonEnd", "jsonStr", "_data", "weightsBuffer", "shaderModule", "bindGroupLayout", "pipelineLayout", "pipeline", "buffer", "createWebGPURuntime", "WebNNRuntime", "__publicField", "EdgeFlowError", "ErrorCodes", "error", "modelData", "options", "config", "modelId", "metadata", "i", "o", "model", "LoadedModelImpl", "getMemoryManager", "inputs", "outputs", "outputSpec", "outputSize", "b", "outputData", "inputData", "EdgeFlowTensor", "data", "decoder", "text", "jsonEnd", "jsonStr", "createWebNNRuntime", "WASMRuntime", "__publicField", "bytes", "memory", "simdTest", "nextPtr", "allocations", "size", "ptr", "aPtr", "aRows", "aCols", "bPtr", "_bRows", "bCols", "outPtr", "view", "aOffset", "bOffset", "outOffset", "i", "j", "sum", "inputPtr", "outputPtr", "inOffset", "max", "modelData", "options", "config", "wasmData", "l", "modelId", "metadata", "o", "model", "LoadedModelImpl", "getMemoryManager", "inputs", "outputs", "outputSpec", "outputSize", "b", "outputTensor", "inputTensor", "softmax", "relu", "sigmoid", "outputData", "inputData", "EdgeFlowTensor", "data", "decoder", "text", "jsonEnd", "jsonStr", "_modelData", "_wasmData", "weight", "EdgeFlowError", "ErrorCodes", "createWASMRuntime", "ONNX_VERSION", "ONNX_CDN_BASE", "ONNX_SCRIPT_URL", "ort", "ortLoadPromise", "loadONNXRuntime", "resolve", "reject", "script", "getOrt", "sessionStore", "ONNXRuntime", "__publicField", "ortInstance", "modelData", "options", "sessionOptions", "modelBytes", "session", "inputNames", "outputNames", "modelId", "metadata", "name", "model", "LoadedModelImpl", "getMemoryManager", "error", "EdgeFlowError", "ErrorCodes", "inputs", "sessionData", "feeds", "i", "inputName", "inputTensor", "dtype", "ortTensor", "data", "results", "outputs", "outputName", "shape", "d", "EdgeFlowTensor", "createONNXRuntime", "registerAllBackends", "registerRuntime", "createWebGPURuntime", "createWebNNRuntime", "createONNXRuntime", "Cache", "options", "__publicField", "key", "entry", "value", "size", "ttl", "entryTtl", "total", "keyToEvict", "oldest", "oldestTime", "lfu", "minCount", "now", "request", "resolve", "reject", "entries", "tx", "store", "db", "InferenceCache", "modelId", "input", "inputArray", "hash", "arr", "sample", "_", "i", "ModelDownloadCache", "cacheName", "url", "response", "r", "createCache", "preset", "presets", "BasePipeline", "config", "__publicField", "ModelCache", "ModelDownloadCache", "cachedModel", "modelPath", "cachedResponse", "response", "loadModel", "input", "options", "startTime", "preprocessed", "outputs", "runInference", "result", "inputs", "pipelineFactories", "registerPipeline", "task", "factory", "getPipelineFactory", "SENTIMENT_LABELS", "EMOTION_LABELS", "IMAGENET_LABELS", "Tokenizer", "_Tokenizer", "__publicField", "bytes", "i", "chars", "n", "byte", "char", "json", "tokenizer", "data", "token", "id", "content", "url", "response", "EdgeFlowError", "ErrorCodes", "modelId", "options", "revision", "text", "result", "pattern", "b", "c", "word", "pairs", "minPair", "minRank", "pair", "rank", "parts", "first", "second", "newWord", "j", "tokens", "start", "end", "curSubstr", "substr", "byteStr", "normalized", "remaining", "sortedAddedTokens", "a", "addedToken", "newRemaining", "words", "wordTokens", "addedId", "vocabId", "ids", "pairIds", "typeIds", "template", "item", "specialToken", "seqIds", "addSpecialTokens", "maxLength", "padding", "truncation", "returnAttentionMask", "returnTokenTypeIds", "textPair", "inputIds", "pairTokens", "tokenTypeIds", "processed", "attentionMask", "padLength", "texts", "encodings", "t", "maxLen", "e", "skipSpecialTokens", "batchIds", "createBasicTokenizer", "loadTokenizer", "loadTokenizerFromHub", "TextClassificationPipeline", "BasePipeline", "config", "labels", "__publicField", "SENTIMENT_LABELS", "createBasicTokenizer", "input", "options", "isBatch", "inputs", "startTime", "results", "text", "tensorInputs", "outputs", "result", "processingTime", "encoded", "inputIds", "EdgeFlowTensor", "attentionMask", "numClasses", "logits", "sum", "b", "i", "probsArray", "softmax", "topK", "maxIdx", "maxScore", "SentimentAnalysisPipeline", "createTextClassificationPipeline", "createSentimentAnalysisPipeline", "registerPipeline", "FeatureExtractionPipeline", "BasePipeline", "config", "embeddingDim", "__publicField", "createBasicTokenizer", "input", "options", "isBatch", "inputs", "startTime", "results", "text", "tensorInputs", "outputs", "result", "processingTime", "encoded", "inputIds", "EdgeFlowTensor", "attentionMask", "seqLen", "embeddings", "inputData", "j", "inputVal", "hiddenStates", "pooling", "normalize", "data", "i", "val", "vec", "norm", "v", "createFeatureExtractionPipeline", "registerPipeline", "DEFAULT_IMAGE_OPTIONS", "ImagePreprocessor", "_ImagePreprocessor", "options", "__publicField", "size", "width", "height", "config", "sizeObj", "cropSize", "cropObj", "imageMean", "imageStd", "rescaleFactor", "doResize", "doRescale", "doNormalize", "doCenterCrop", "url", "response", "modelId", "revision", "input", "imageData", "processed", "inputs", "tensors", "batchSize", "firstTensor", "EdgeFlowTensor", "channels", "batchData", "i", "t", "resolve", "reject", "img", "blob", "cropWidth", "cropHeight", "srcX", "srcY", "srcCanvas", "source", "resizeMode", "srcW", "srcH", "dstX", "dstY", "dstW", "dstH", "scale", "mean", "std", "grayscale", "channelFormat", "dtype", "data", "pixels", "x", "pixelIdx", "gray", "idx", "c", "value", "shape", "DEFAULT_AUDIO_OPTIONS", "AudioPreprocessor", "_AudioPreprocessor", "samplingRate", "featureSize", "nFft", "hopLength", "audioData", "maxSamples", "arrayBuffer", "audioBuffer", "buffer", "channelData", "max", "abs", "result", "audio", "nMels", "numFrames", "melSpec", "frame", "start", "mel", "energy", "freqStart", "freqEnd", "sample", "preprocessText", "text", "lowercase", "removePunctuation", "normalizeWhitespace", "maxLength", "createImagePreprocessor", "preset", "presets", "createAudioPreprocessor", "ImageClassificationPipeline", "BasePipeline", "config", "labels", "numClasses", "__publicField", "IMAGENET_LABELS", "createImagePreprocessor", "input", "options", "isBatch", "inputs", "startTime", "results", "image", "tensorInputs", "outputs", "result", "processingTime", "tensor", "logits", "inputData", "sum", "EdgeFlowTensor", "probsArray", "softmax", "maxIdx", "maxScore", "i", "createImageClassificationPipeline", "registerPipeline", "TextGenerationPipeline", "BasePipeline", "config", "__publicField", "tokenizer", "specialIds", "input", "text", "EdgeFlowTensor", "encoded", "id", "_outputs", "_options", "prompt", "options", "prompts", "results", "p", "startTime", "maxNewTokens", "maxLength", "temperature", "topK", "topP", "repetitionPenalty", "stopSequences", "doSample", "inputIds", "generatedIds", "generatedText", "i", "nextTokenId", "token", "shouldStop", "stopSeq", "endTime", "returnFullText", "currentText", "inputTensor", "attentionMask", "outputs", "runInference", "logits", "logitsData", "vocabSize", "lastPositionLogits", "offset", "prevId", "score", "logitsTensor", "probs", "softmax", "maxIdx", "maxProb", "indices", "_", "a", "b", "candidateIndices", "cumulativeProb", "filtered", "idx", "totalProb", "r", "cumulative", "COCO_LABELS", "ObjectDetectionPipeline", "BasePipeline", "config", "labels", "__publicField", "ImagePreprocessor", "input", "inputs", "tensor", "EdgeFlowTensor", "outputs", "options", "opts", "threshold", "topK", "nms", "iouThreshold", "outputData", "shape", "detections", "filtered", "a", "b", "data", "numBoxes", "boxSize", "numClasses", "i", "offset", "objectness", "maxClassScore", "maxClassIdx", "c", "score", "confidence", "x", "w", "h", "x1", "y1", "x2", "y2", "sorted", "selected", "active", "current", "j", "other", "xOverlap", "yOverlap", "intersection", "aArea", "bArea", "union", "AutomaticSpeechRecognitionPipeline", "BasePipeline", "config", "__publicField", "AudioPreprocessor", "tokenizer", "input", "inputs", "tensors", "audio", "t", "EdgeFlowTensor", "outputs", "options", "returnTimestamps", "outputData", "shape", "text", "result", "data", "seqLen", "vocabSize", "tokenIds", "i", "offset", "maxIdx", "maxVal", "j", "_data", "_shape", "words", "w", "chunks", "wordsPerSecond", "chunkText", "chunkStart", "duration", "chunkDuration", "chunkOverlap", "audioData", "sampleRate", "chunkSamples", "overlapSamples", "stepSamples", "start", "end", "chunkAudio", "chunkResult", "timeOffset", "c", "mergedText", "mergedChunks", "ZeroShotClassificationPipeline", "BasePipeline", "config", "__publicField", "tokenizer", "text", "candidateLabels", "options", "input", "opts", "texts", "template", "multiLabel", "results", "t", "startTime", "hypotheses", "label", "scores", "hypothesis", "score", "normalizedScores", "s", "tensor", "EdgeFlowTensor", "softmax", "indexed", "i", "a", "b", "premise", "firstText", "firstLabel", "encoded", "id", "_outputs", "_options", "QuestionAnsweringPipeline", "BasePipeline", "config", "__publicField", "tokenizer", "input", "options", "inputs", "results", "startTime", "question", "context", "maxAnswerLength", "encoded", "answer", "_tokenIds", "maxLength", "questionWords", "contextSentences", "s", "bestSentence", "bestScore", "bestStart", "sentence", "words", "score", "qWord", "w", "normalizedScore", "EdgeFlowTensor", "qaInput", "id", "m", "outputs", "_options", "startLogits", "endLogits", "seqLen", "startProbs", "softmax", "endProbs", "bestEnd", "start", "end", "pipeline", "task", "options", "config", "pipelineInstance", "TextClassificationPipeline", "SentimentAnalysisPipeline", "FeatureExtractionPipeline", "ImageClassificationPipeline", "TextGenerationPipeline", "ObjectDetectionPipeline", "AutomaticSpeechRecognitionPipeline", "ZeroShotClassificationPipeline", "QuestionAnsweringPipeline", "createPipelines", "tasks", "pipelines", "result", "i", "init_model_loader", "init_model_loader", "DEFAULT_ENDPOINT", "DEFAULT_REVISION", "ONNX_MODEL_FILES", "buildFileUrl", "modelId", "filename", "options", "endpoint", "revision", "subfolder", "fetchWithAuth", "url", "token", "headers", "fileExists", "response", "findOnnxModel", "downloadFile", "loadModelData", "progress", "downloadJson", "isModelCached", "data", "text", "EdgeFlowError", "ErrorCodes", "downloadTokenizer", "Tokenizer", "downloadConfig", "downloadModel", "files", "currentStep", "reportProgress", "file", "baseProgress", "stepProgress", "modelFile", "modelData", "p", "tokenizer", "config", "fromHub", "modelExists", "getModelInfo", "onnxFile", "hasTokenizer", "POPULAR_MODELS", "getDefaultModel", "task", "fromTask", "quantize", "model", "options", "modelData", "getModelData", "originalSize", "quantizedData", "layersQuantized", "layersSkipped", "quantizeInt8", "quantizeUint8", "quantizeFloat16", "quantizeInt4", "_model", "data", "_options", "input", "output", "max", "abs", "scale", "min", "i", "val", "float32ToFloat16", "val1", "val2", "value", "floatView", "int32View", "x", "bits", "m", "e", "prune", "weights", "total", "sorted", "a", "b", "thresholdIdx", "threshold", "pruned", "analyzeModel", "size", "estimatedParams", "benchmark", "runFn", "warmupRuns", "runs", "times", "start", "avgTime", "minTime", "maxTime", "avgSquaredDiff", "t", "stdDev", "exportModel", "format", "array", "isSupported", "runtimes", "getAvailableRuntimes", "v", "getBestRuntimeType", "preload", "models", "cache", "ModelDownloadCache", "url", "response", "VERSION", "getInfo"]
}
